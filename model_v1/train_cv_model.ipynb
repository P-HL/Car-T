{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e2ae3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a94668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         # å¯¼å…¥Pandasåº“ï¼Œç”¨äºæ•°æ®å¤„ç†\n",
    "import numpy as np          # å¯¼å…¥NumPyåº“ï¼Œç”¨äºæ•°å€¼è®¡ç®—\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# å¯¼å…¥æ¨¡å‹é€‰æ‹©æ¨¡å—ä¸­çš„train_test_splitï¼Œç”¨äºæ‹†åˆ†æ•°æ®é›†\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# å¯¼å…¥é¢„å¤„ç†æ¨¡å—ä¸­çš„StandardScaler å’ŒOneHotEncoderï¼Œç”¨äºæ•°æ®æ ‡å‡†åŒ–å’Œç‹¬çƒ­ç¼–ç \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# å¯¼å…¥é€»è¾‘å›å½’æ¨¡å‹\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# å¯¼å…¥éšæœºæ£®æ—åˆ†ç±»å™¨\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# å¯¼å…¥å¤šå±‚æ„ŸçŸ¥å™¨åˆ†ç±»å™¨\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# å¯¼å…¥æ”¯æŒå‘é‡æœºåˆ†ç±»å™¨\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# å¯¼å…¥XGBooståº“\n",
    "# !pip install xgboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "import xgboost as xgb\n",
    "\n",
    "#å¯¼å…¥LightGBMåº“\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble  import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from catboost import CatBoostClassifier     # pip install catboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import statsmodels.api as sm                # pip install statsmodels\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "\n",
    "import shap\n",
    "# plt.rcParams['font.sans-serif'] = ['SimSun'] #å®‹ä½“\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['font.family'] = ['sans-serif']\n",
    "plt.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2874ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ¨æ€åŸºçº¿ç‰¹å¾æèµ·å–å’Œç¼ºå¤±å€¼è¿‡æ»¤ç›¸å…³å‡½æ•°\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm  # ç”¨äºæ˜¾ç¤ºè¿›åº¦æ¡\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ==============================================================\n",
    "# 0. æ•°æ®ç›‘æ§ç±»ï¼ˆåŠ¨æ€åŸºçº¿ç‰¹å¾æå–å‡½æ•°å®šä¹‰â€”â€”è¡¥å……åŠŸèƒ½ï¼‰\n",
    "# ==============================================================\n",
    "class VariableMonitor:\n",
    "    \"\"\"\n",
    "    ç›‘æ§ç‰¹å®šå˜é‡åœ¨æ•°æ®æå–è¿‡ç¨‹ä¸­çš„ç©ºæ•°æ®æƒ…å†µã€‚\n",
    "    \n",
    "    åŠŸèƒ½ï¼š\n",
    "    1. è¿½è¸ªæŒ‡å®šå˜é‡ä½•æ—¶ä¸ºç©º\n",
    "    2. è®°å½•åœ¨æŒ‡å®šæ—¶é—´åŒºé—´å†…æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®çš„æ‚£è€…ID\n",
    "    3. ä¿å­˜æ‚£è€…IDä»¥ä¾›åç»­æ‰‹åŠ¨æ£€æŸ¥\n",
    "    \n",
    "    ç”¨æ³•ç¤ºä¾‹ï¼š\n",
    "        monitor = VariableMonitor(variables_to_monitor=['CBC004', 'CBC001'])\n",
    "        # åœ¨æ•°æ®æå–è¿‡ç¨‹ä¸­è°ƒç”¨ record_patient_data æ–¹æ³•\n",
    "        # æœ€åè°ƒç”¨ get_empty_patient_ids() æˆ– save_report() è·å–ç»“æœ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, variables_to_monitor=None, time_window=(-15, 0)):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–ç›‘æ§å™¨\n",
    "        \n",
    "        å‚æ•°:\n",
    "            variables_to_monitor: è¦ç›‘æ§çš„å˜é‡åˆ—è¡¨ï¼Œä¾‹å¦‚ ['CBC004', 'CBC001']\n",
    "            time_window: ç›‘æ§çš„æ—¶é—´çª—å£ï¼Œé»˜è®¤ (-15, 0)\n",
    "        \"\"\"\n",
    "        self.variables_to_monitor = variables_to_monitor or []\n",
    "        self.time_window = time_window\n",
    "        \n",
    "        # è®°å½•æ¯ä¸ªå˜é‡çš„ç©ºæ•°æ®æ‚£è€…ID\n",
    "        # ç»“æ„: {variable_name: set(patient_ids)}\n",
    "        self.empty_data_patients = {var: set() for var in self.variables_to_monitor}\n",
    "        \n",
    "        # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰\n",
    "        # ç»“æ„: {variable_name: {patient_id: {'reason': str, 'time_range': tuple}}}\n",
    "        self.detailed_info = {var: {} for var in self.variables_to_monitor}\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        self.total_patients_processed = 0\n",
    "        \n",
    "    def record_patient_data(self, patient_id, variable_name, has_data, time_range=None, reason=None):\n",
    "        \"\"\"\n",
    "        è®°å½•æ‚£è€…çš„æ•°æ®çŠ¶æ€\n",
    "        \n",
    "        å‚æ•°:\n",
    "            patient_id: æ‚£è€…ID\n",
    "            variable_name: å˜é‡åç§°\n",
    "            has_data: æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ® (bool)\n",
    "            time_range: å®é™…æ•°æ®çš„æ—¶é—´èŒƒå›´ (å¯é€‰)\n",
    "            reason: ä¸ºç©ºçš„åŸå›  (å¯é€‰)ï¼Œä¾‹å¦‚ 'no_file', 'no_data_in_window', 'variable_missing'\n",
    "        \"\"\"\n",
    "        if variable_name not in self.variables_to_monitor:\n",
    "            return\n",
    "        \n",
    "        if not has_data:\n",
    "            self.empty_data_patients[variable_name].add(patient_id)\n",
    "            self.detailed_info[variable_name][patient_id] = {\n",
    "                'reason': reason or 'unknown',\n",
    "                'time_range': time_range,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }\n",
    "    \n",
    "    def get_empty_patient_ids(self, variable_name=None):\n",
    "        \"\"\"\n",
    "        è·å–æŒ‡å®šå˜é‡ï¼ˆæˆ–æ‰€æœ‰å˜é‡ï¼‰çš„ç©ºæ•°æ®æ‚£è€…IDé›†åˆ\n",
    "        \n",
    "        å‚æ•°:\n",
    "            variable_name: å˜é‡åç§°ï¼Œå¦‚æœä¸º Noneï¼Œè¿”å›æ‰€æœ‰å˜é‡çš„ç»“æœ\n",
    "            \n",
    "        è¿”å›:\n",
    "            å¦‚æœæŒ‡å®šå˜é‡åï¼Œè¿”å›è¯¥å˜é‡çš„æ‚£è€…IDé›†åˆ\n",
    "            å¦‚æœæœªæŒ‡å®šï¼Œè¿”å›å­—å…¸ {variable_name: set(patient_ids)}\n",
    "        \"\"\"\n",
    "        if variable_name:\n",
    "            return self.empty_data_patients.get(variable_name, set())\n",
    "        return self.empty_data_patients\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"\n",
    "        è·å–ç›‘æ§æ‘˜è¦ä¿¡æ¯\n",
    "        \n",
    "        è¿”å›:\n",
    "            dict: åŒ…å«å„å˜é‡çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        for var in self.variables_to_monitor:\n",
    "            empty_count = len(self.empty_data_patients[var])\n",
    "            summary[var] = {\n",
    "                'empty_patient_count': empty_count,\n",
    "                'empty_patient_ids': sorted(list(self.empty_data_patients[var])),\n",
    "                'time_window': self.time_window\n",
    "            }\n",
    "        \n",
    "        summary['total_patients_processed'] = self.total_patients_processed\n",
    "        return summary\n",
    "\n",
    "    def save_report(self, output_path='monitor_report.json', include_details=True):\n",
    "        \"\"\"\n",
    "        ä¿å­˜ç›‘æ§æŠ¥å‘Šåˆ°æ–‡ä»¶\n",
    "        \n",
    "        å‚æ•°:\n",
    "            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "            include_details: æ˜¯å¦åŒ…å«è¯¦ç»†ä¿¡æ¯\n",
    "        \"\"\"\n",
    "        report = {\n",
    "            'monitoring_config': {\n",
    "                'variables': self.variables_to_monitor,\n",
    "                'time_window': self.time_window,\n",
    "                'total_patients': int(self.total_patients_processed),  # è½¬æ¢ä¸º int\n",
    "                'generated_at': datetime.now().isoformat()\n",
    "            },\n",
    "            'summary': self._convert_to_json_serializable(self.get_summary())  # é€’å½’è½¬æ¢\n",
    "        }\n",
    "        \n",
    "        if include_details:\n",
    "            report['detailed_info'] = self._convert_to_json_serializable({\n",
    "                var: {pid: info for pid, info in patient_info.items()}\n",
    "                for var, patient_info in self.detailed_info.items()\n",
    "            })\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nç›‘æ§æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "    def _convert_to_json_serializable(self, obj):\n",
    "            \"\"\"\n",
    "            é€’å½’è½¬æ¢ NumPy/Pandas ç±»å‹ä¸º JSON å¯åºåˆ—åŒ–çš„ Python åŸç”Ÿç±»å‹\n",
    "            é€’å½’å¤„ç†å­—å…¸çš„ Keys å’Œ Values\n",
    "            \"\"\"\n",
    "            import numpy as np\n",
    "            \n",
    "            if isinstance(obj, dict):\n",
    "                # éå†å­—å…¸æ—¶ï¼ŒåŒæ—¶é€šè¿‡ int() è½¬æ¢ Keyï¼Œé˜²æ­¢ Keys æ˜¯ int64 ç±»å‹\n",
    "                new_dict = {}\n",
    "                for key, value in obj.items():\n",
    "                    # å¦‚æœ Key æ˜¯ numpy æ•´æ•°ç±»å‹ï¼Œå¼ºè½¬ä¸º Python int\n",
    "                    if isinstance(key, (np.integer, np.int64, np.int32)):\n",
    "                        new_key = int(key)\n",
    "                    else:\n",
    "                        new_key = key\n",
    "                    new_dict[new_key] = self._convert_to_json_serializable(value)\n",
    "                return new_dict\n",
    "                \n",
    "            elif isinstance(obj, list):\n",
    "                return [self._convert_to_json_serializable(item) for item in obj]\n",
    "            elif isinstance(obj, set):\n",
    "                return sorted([self._convert_to_json_serializable(item) for item in obj])\n",
    "            elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "                return int(obj)\n",
    "            elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "                return float(obj)\n",
    "            elif isinstance(obj, np.ndarray):\n",
    "                return obj.tolist()\n",
    "            else:\n",
    "                return obj\n",
    "        \n",
    "    def print_summary(self):\n",
    "        \"\"\"æ‰“å°ç›‘æ§æ‘˜è¦åˆ°æ§åˆ¶å°\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"å˜é‡ç›‘æ§æ‘˜è¦æŠ¥å‘Š\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ç›‘æ§æ—¶é—´çª—å£: {self.time_window}\")\n",
    "        print(f\"å¤„ç†æ‚£è€…æ€»æ•°: {self.total_patients_processed}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for var in self.variables_to_monitor:\n",
    "            empty_count = len(self.empty_data_patients[var])\n",
    "            print(f\"\\nå˜é‡: {var}\")\n",
    "            print(f\"  ç©ºæ•°æ®æ‚£è€…æ•°: {empty_count}\")\n",
    "            if empty_count > 0:\n",
    "                print(f\"  æ‚£è€…IDåˆ—è¡¨: {sorted(list(self.empty_data_patients[var]))[:10]}\" + \n",
    "                      (f\" ... (å…±{empty_count}ä¸ª)\" if empty_count > 10 else \"\"))\n",
    "        \n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 1.åŠ¨æ€åŸºçº¿ç‰¹å¾æå–å‡½æ•°å®šä¹‰ï¼ˆä¿®å¤å¢å¼ºç‰ˆï¼‰\n",
    "# ==============================================================\n",
    "def extract_baseline_features(patient_ids, dynamic_dir, time_col='day', window_start=-15, cutoff_day=0, monitor=None):\n",
    "    \"\"\"\n",
    "    ä»åŠ¨æ€æ–‡ä»¶ä¸­æå– [window_start, cutoff_day] åŒºé—´å†…çš„æœ€åä¸€æ¬¡æœ‰æ•ˆè§‚æµ‹å€¼ä½œä¸ºåŸºçº¿ç‰¹å¾ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        patient_ids: æ‚£è€… ID åˆ—è¡¨\n",
    "        dynamic_dir: åŠ¨æ€æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„\n",
    "        time_col: æ—¶é—´åˆ—åï¼Œæ”¯æŒå¤§å°å†™æ¨¡ç³ŠåŒ¹é…ï¼Œé»˜è®¤ä¸º 'day'\n",
    "        window_start: æ—¶é—´çª—å£èµ·å§‹å¤©æ•° (é»˜è®¤ -15)\n",
    "        cutoff_day: æ—¶é—´çª—å£ç»“æŸå¤©æ•° (é»˜è®¤ 0)\n",
    "        monitor: ç›‘æ§å™¨å¯¹è±¡ (å¯é€‰)\n",
    "        é€»è¾‘è¯´æ˜ï¼šSort -> Ffill -> Take Last\n",
    "    è¿”å›:\n",
    "        df_features: æå–åçš„ç‰¹å¾ DataFrame\n",
    "        missing_pids: åœ¨æŒ‡å®šçª—å£å†…å®Œå…¨æ— æ•°æ®çš„æ‚£è€…IDåˆ—è¡¨\n",
    "    \"\"\"\n",
    "    extracted_features = []\n",
    "    missing_pids = [] # ç”¨äºå­˜å‚¨ç¼ºå¤±æ•°æ®çš„æ‚£è€…ID\n",
    "    \n",
    "    print(f\"æ­£åœ¨ä» {len(patient_ids)} ä¸ªåŠ¨æ€æ–‡ä»¶ä¸­æå–åŸºçº¿ç‰¹å¾ (Day [{window_start}, {cutoff_day}])...\")\n",
    "    \n",
    "    # æ›´æ–°ç›‘æ§å™¨çš„æ‚£è€…æ€»æ•°\n",
    "    if monitor:\n",
    "        monitor.total_patients_processed = len(patient_ids)\n",
    "    \n",
    "    for pid in tqdm(patient_ids):\n",
    "        file_path = os.path.join(dynamic_dir, f\"{pid}.csv\")\n",
    "        \n",
    "        # åˆå§‹åŒ–å­—å…¸ï¼ŒåŒ…å« ID\n",
    "        patient_data = {'ID': pid}\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # è¯»å–åŠ¨æ€æ•°æ®\n",
    "                df_dyn = pd.read_csv(file_path)\n",
    "                \n",
    "                # 1. å¥å£®æ€§ä¿®å¤ï¼šå¤„ç†æ—¶é—´åˆ—åå¤§å°å†™ä¸ä¸€è‡´é—®é¢˜\n",
    "                actual_time_col = None\n",
    "                if time_col in df_dyn.columns:\n",
    "                    actual_time_col = time_col\n",
    "                else:\n",
    "                    # å°è¯•æŸ¥æ‰¾å¿½ç•¥å¤§å°å†™çš„åŒ¹é…\n",
    "                    col_map = {c.lower(): c for c in df_dyn.columns}\n",
    "                    if time_col.lower() in col_map:\n",
    "                        actual_time_col = col_map[time_col.lower()]\n",
    "                \n",
    "                if actual_time_col:\n",
    "                    # 2. å¥å£®æ€§ä¿®å¤ï¼šå¼ºåˆ¶è½¬æ¢æ—¶é—´åˆ—ä¸ºæ•°å€¼å‹ï¼Œå¤„ç†è„æ•°æ®\n",
    "                    df_dyn[actual_time_col] = pd.to_numeric(df_dyn[actual_time_col], errors='coerce')\n",
    "                    df_dyn = df_dyn.dropna(subset=[actual_time_col]) # åˆ é™¤æ—¶é—´è§£æå¤±è´¥çš„è¡Œ\n",
    "\n",
    "                    # 3. ç­›é€‰ï¼šä¸¥æ ¼ä¿ç•™ [window_start, cutoff_day] åŒºé—´çš„æ•°æ®\n",
    "                    mask = (df_dyn[actual_time_col] >= window_start) & (df_dyn[actual_time_col] <= cutoff_day)\n",
    "                    df_baseline_window = df_dyn[mask].copy()\n",
    "                    \n",
    "                    if not df_baseline_window.empty:\n",
    "                        # 4. æ’åºï¼šæŒ‰æ—¶é—´å‡åºæ’åˆ— (Day -15 -> Day 0)\n",
    "                        df_sorted = df_baseline_window.sort_values(by=actual_time_col, ascending=True)\n",
    "                        \n",
    "                        # ==========================================================\n",
    "                        # æ ¸å¿ƒä¿®å¤é€»è¾‘\n",
    "                        # ==========================================================\n",
    "                        # è§£å†³\"å­˜åœ¨æ•°æ®ä½†æå–ä¸ºç©º\"çš„é—®é¢˜,åŸä»£ç ä»…å–æœ€åä¸€è¡Œ(iloc[-1])ï¼Œä¼šå¯¼è‡´ä¹‹å‰è¡Œå­˜åœ¨çš„æœ‰æ•ˆæ•°æ®ï¼ˆç¨€ç–æ•°æ®ï¼‰è¢«é—æ¼.\n",
    "                        # é—®é¢˜ï¼šå¦‚æœ Day -5 æœ‰å€¼ï¼ŒDay -1 æ— å€¼(NaN)ï¼Œåªå–æœ€åä¸€è¡Œ(Day -1)ä¼šå¯¼è‡´ NaNã€‚\n",
    "                        # ä¿®å¤ï¼šffill() ä¼šå°† Day -5 çš„æœ‰æ•ˆå€¼\"æºå¸¦\"åˆ° Day -1ã€‚\n",
    "                        # è¿™æ ·æœ€åä¸€è¡Œå°±åŒ…å«äº†æˆªè‡³è¯¥æ—¶åˆ»æ‰€æœ‰å˜é‡çš„æœ€æ–°å·²çŸ¥å€¼ã€‚\n",
    "                        df_filled = df_sorted.ffill() \n",
    "                        last_observation = df_filled.iloc[-1]\n",
    "                        \n",
    "                        # 5. è½¬æ¢ï¼šæå–ç‰¹å¾\n",
    "                        for col in last_observation.index:\n",
    "                            # æ’é™¤ ID, æ—¶é—´åˆ— å’Œ å¯èƒ½çš„ç´¢å¼•åˆ—\n",
    "                            if col != actual_time_col and col != 'ID' and not col.startswith('Unnamed'): \n",
    "                                patient_data[f\"baseline_{col}\"] = last_observation[col]\n",
    "                        \n",
    "                        # è®°å½•Gap Days (è¯„ä¼°æ•°æ®æ–°é²œåº¦)\n",
    "                        patient_data['baseline_gap_days'] = cutoff_day - last_observation[actual_time_col]\n",
    "                        \n",
    "                        # ã€ç›‘æ§åŠŸèƒ½ã€‘æ£€æŸ¥ç›‘æ§çš„å˜é‡æ˜¯å¦æå–åˆ°äº†æ•°æ®\n",
    "                        if monitor:\n",
    "                            time_range = (df_baseline_window[actual_time_col].min(), \n",
    "                                        df_baseline_window[actual_time_col].max())\n",
    "                            for var in monitor.variables_to_monitor:\n",
    "                                # æ£€æŸ¥è¯¥å˜é‡æ˜¯å¦å­˜åœ¨ä¸”æœ‰æœ‰æ•ˆå€¼\n",
    "                                baseline_var_name = f\"baseline_{var}\"\n",
    "                                has_data = False\n",
    "                                if baseline_var_name in patient_data:\n",
    "                                    has_data = pd.notna(patient_data[baseline_var_name])\n",
    "                          \n",
    "                                monitor.record_patient_data(\n",
    "                                    patient_id=pid,\n",
    "                                    variable_name=var,\n",
    "                                    has_data=has_data,\n",
    "                                    time_range=time_range,\n",
    "                                    reason='value_is_nan' if not has_data else None\n",
    "                                )\n",
    "\n",
    "                    else:\n",
    "                        # ä¿®å¤è¦æ±‚ 1: æ•°æ®ä¸ºç©ºæ—¶çš„å¼‚å¸¸å¤„ç†\n",
    "                        # çª—å£å†…æ²¡æœ‰æ•°æ®\n",
    "                        print(f\"âš ï¸ è­¦å‘Š: æ‚£è€… {pid} åœ¨çª—å£ [{window_start}, {cutoff_day}] å†…æ— æœ‰æ•ˆæ•°æ®\")\n",
    "                        missing_pids.append(pid)\n",
    "                        \n",
    "                        # ã€ç›‘æ§åŠŸèƒ½ã€‘æ—¶é—´çª—å£å†…æ²¡æœ‰æ•°æ®\n",
    "                        if monitor:\n",
    "                            for var in monitor.variables_to_monitor:\n",
    "                                monitor.record_patient_data(\n",
    "                                    patient_id=pid,\n",
    "                                    variable_name=var,\n",
    "                                    has_data=False,\n",
    "                                    time_range=None,\n",
    "                                    reason='no_data_in_time_window'\n",
    "                                )\n",
    "                else:\n",
    "                    print(f\"âŒ é”™è¯¯: æ–‡ä»¶ {pid}.csv ä¸­æœªæ‰¾åˆ°æ—¶é—´åˆ— '{time_col}'\")\n",
    "                    missing_pids.append(pid)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ å¤„ç† ID {pid} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}\")\n",
    "                missing_pids.append(pid)\n",
    "        else:\n",
    "            # æ–‡ä»¶ä¸å­˜åœ¨\n",
    "            # print(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\") # å¯é€‰ï¼šå‡å°‘åˆ·å±\n",
    "            if monitor:\n",
    "                for var in monitor.variables_to_monitor:\n",
    "                    monitor.record_patient_data(\n",
    "                        patient_id=pid,\n",
    "                        variable_name=var,\n",
    "                        has_data=False,\n",
    "                        time_range=None,\n",
    "                        reason='patient_file_not_found'\n",
    "                    )\n",
    "        \n",
    "        extracted_features.append(patient_data)\n",
    "    \n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    df_features = pd.DataFrame(extracted_features)\n",
    "    print(f\"\\næå–å®Œæˆã€‚å…± {len(missing_pids)} ä½æ‚£è€…åŸºçº¿æ•°æ®å®Œå…¨ç¼ºå¤±ã€‚\")\n",
    "    # if len(missing_pids) > 0:     # å¯é€‰ï¼šå‡å°‘åˆ·å±\n",
    "    #     print(f\"ç¼ºå¤±æ•°æ®æ‚£è€…IDç¤ºä¾‹: {missing_pids[:10]}\")\n",
    "    return df_features\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# 2. åŠ¨æ€ç‰¹å¾ç¼ºå¤±å€¼è¿‡æ»¤å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "# è¯†åˆ«åŠ¨æ€ç‰¹å¾ï¼šè‡ªåŠ¨è¯†åˆ«ä»¥ baseline_ å¼€å¤´çš„æ‰€æœ‰åˆ—\n",
    "# ç¼ºå¤±å€¼ç»Ÿè®¡ï¼šè®¡ç®—æ¯ä¸ªåŠ¨æ€ç‰¹å¾çš„ç¼ºå¤±æ•°é‡å’Œç™¾åˆ†æ¯”\n",
    "# é˜ˆå€¼è¿‡æ»¤ï¼šåˆ é™¤ç¼ºå¤±å€¼ > 20% çš„åˆ—\n",
    "# 1.ç”ŸæˆæŠ¥å‘Šï¼š\n",
    "# - ç¼ºå¤±å€¼ç»Ÿè®¡ CSV æ–‡ä»¶\n",
    "# - å¯è§†åŒ–ç¼ºå¤±å€¼åˆ†å¸ƒå›¾\n",
    "# 2.æ–°å¢ drop_high_missing å‚æ•°ï¼š\n",
    "# - Trueï¼ˆé»˜è®¤ï¼‰ï¼šåˆ é™¤è¶…è¿‡é˜ˆå€¼çš„åˆ—ï¼ˆåŸæœ‰è¡Œä¸ºï¼‰\n",
    "# - Falseï¼šä»…åˆ†æå¹¶æŠ¥å‘Šï¼Œä¸åˆ é™¤ä»»ä½•åˆ—\n",
    "# 3.å¢å¼ºçš„ç»Ÿè®¡ä¿¡æ¯ï¼š\n",
    "# - cols_above_thresholdï¼šè®°å½•è¶…è¿‡é˜ˆå€¼çš„åˆ—æ•°\n",
    "# - suggested_drop_colsï¼šå»ºè®®åˆ é™¤çš„åˆ—ï¼ˆæ— è®ºæ˜¯å¦å®é™…åˆ é™¤ï¼‰\n",
    "# - drop_mode_enabledï¼šè®°å½•å½“å‰æ¨¡å¼\n",
    "def filter_baseline_features_by_missing(df, missing_threshold=20.0, \n",
    "                                        output_dir='/home/phl/PHL/Car-T/model_v1/output/dynamic_feature_missing',\n",
    "                                        verbose=True,\n",
    "                                        drop_high_missing=True\n",
    "                                        ):\n",
    "    \"\"\"\n",
    "    è¿‡æ»¤åŠ¨æ€ç‰¹å¾ä¸­ç¼ºå¤±å€¼è¶…è¿‡é˜ˆå€¼çš„åˆ—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df: DataFrame, åŒ…å«åŠ¨æ€ç‰¹å¾çš„æ•°æ®æ¡†\n",
    "        missing_threshold: float, ç¼ºå¤±å€¼ç™¾åˆ†æ¯”é˜ˆå€¼ï¼ˆé»˜è®¤20%ï¼‰\n",
    "        output_dir: str, è¾“å‡ºç›®å½•è·¯å¾„\n",
    "        verbose: bool, æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "        drop_high_missing: bool, æ˜¯å¦åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—ï¼ˆTrue=åˆ é™¤ï¼ŒFalse=ä»…åˆ†æï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        DataFrame: è¿‡æ»¤åçš„æ•°æ®æ¡†ï¼ˆå¦‚æœdrop_high_missing=Falseï¼Œåˆ™è¿”å›åŸå§‹æ•°æ®æ¡†ï¼‰\n",
    "        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"åŠ¨æ€ç‰¹å¾ç¼ºå¤±å€¼{'è¿‡æ»¤' if drop_high_missing else 'åˆ†æ'}ï¼ˆé˜ˆå€¼: {missing_threshold}%ï¼‰\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    # 1. è‡ªåŠ¨è¯†åˆ«ä»¥ baseline_ å¼€å¤´çš„æ‰€æœ‰åŠ¨æ€ç‰¹å¾åˆ—\n",
    "    baseline_cols = [col for col in df.columns if col.startswith('baseline_')]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\næå–åˆ°çš„åŠ¨æ€ç‰¹å¾åˆ—æ•°é‡: {len(baseline_cols)}\")\n",
    "    \n",
    "    if len(baseline_cols) == 0:\n",
    "        print(\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•åŠ¨æ€ç‰¹å¾åˆ—ï¼ˆbaseline_*ï¼‰\")\n",
    "        return df, {'baseline_cols': 0, 'cols_dropped': 0, 'cols_kept': 0}\n",
    "    \n",
    "    # 2. è®¡ç®—æ¯ä¸ªåŠ¨æ€ç‰¹å¾çš„ç¼ºå¤±æ•°é‡å’Œç™¾åˆ†æ¯”\n",
    "    missing_stats = {}\n",
    "    for col in baseline_cols:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        missing_stats[col] = {\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct\n",
    "        }\n",
    "    \n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    df_missing_stats = pd.DataFrame(missing_stats).T\n",
    "    df_missing_stats = df_missing_stats.sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nåŠ¨æ€ç‰¹å¾ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰:\")\n",
    "        print(df_missing_stats.head(10).to_string())\n",
    "    \n",
    "    # 3. åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼Œåˆ é™¤ç¼ºå¤±å€¼ > 20% çš„åˆ—\n",
    "    cols_to_drop = [col for col, stats in missing_stats.items() \n",
    "                    if stats['missing_pct'] > missing_threshold]\n",
    "    cols_to_keep = [col for col, stats in missing_stats.items() \n",
    "                    if stats['missing_pct'] <= missing_threshold]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nç¼ºå¤±å€¼{'è¿‡æ»¤' if drop_high_missing else 'åˆ†æ'}ç»“æœ:\")\n",
    "        print(f\"  è¶…è¿‡é˜ˆå€¼çš„åˆ—æ•°: {len(cols_to_drop)}\")\n",
    "        print(f\"  ä½äºé˜ˆå€¼çš„åˆ—æ•°: {len(cols_to_keep)}\")\n",
    "    \n",
    "    # 4. æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ é™¤åˆ—\n",
    "    df_filtered = df.copy()\n",
    "    if drop_high_missing and cols_to_drop:\n",
    "        if verbose:\n",
    "            print(f\"\\næ­£åœ¨åˆ é™¤ {len(cols_to_drop)} ä¸ªé«˜ç¼ºå¤±ç‡åŠ¨æ€ç‰¹å¾...\")\n",
    "            print(f\"åˆ é™¤çš„åˆ—ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰: {cols_to_drop[:5]}\")\n",
    "        df_filtered = df_filtered.drop(columns=cols_to_drop)\n",
    "    elif not drop_high_missing:\n",
    "        if verbose:\n",
    "            print(f\"\\nâš ï¸ ä»…åˆ†ææ¨¡å¼ï¼šæœªåˆ é™¤ä»»ä½•åˆ—\")\n",
    "            if cols_to_drop:\n",
    "                print(f\"å»ºè®®åˆ é™¤çš„åˆ—ï¼ˆè¶…è¿‡{missing_threshold}%ç¼ºå¤±ç‡ï¼‰:\")\n",
    "                for col in cols_to_drop[:10]:  # æ˜¾ç¤ºå‰10ä¸ª\n",
    "                    pct = missing_stats[col]['missing_pct']\n",
    "                    print(f\"  - {col}: {pct:.2f}%\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"\\nâœ… æ‰€æœ‰åŠ¨æ€ç‰¹å¾çš„ç¼ºå¤±å€¼æ¯”ä¾‹å‡ä½äºé˜ˆå€¼ï¼Œæ— éœ€åˆ é™¤\")\n",
    "    \n",
    "    # 5. ç”ŸæˆæŠ¥å‘Šï¼šç¼ºå¤±å€¼ç»Ÿè®¡ CSV æ–‡ä»¶\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    report_path = os.path.join(output_dir, 'baseline_features_missing_report.csv')\n",
    "    df_missing_stats.to_csv(report_path)\n",
    "    if verbose:\n",
    "        print(f\"\\nç¼ºå¤±å€¼ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}\")\n",
    "    \n",
    "    # 6. å¯è§†åŒ–ï¼šç¼ºå¤±å€¼åˆ†å¸ƒå›¾ï¼ˆæ°´å¹³æ¡å½¢å›¾ï¼‰\n",
    "    if len(df_missing_stats) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        plot_data = df_missing_stats.head(30).sort_values('missing_pct', ascending=True)\n",
    "        colors = ['#E74C3C' if pct > missing_threshold else '#2ECC71' \n",
    "                  for pct in plot_data['missing_pct']]\n",
    "        \n",
    "        ax.barh(range(len(plot_data)), plot_data['missing_pct'], color=colors, alpha=0.7)\n",
    "        ax.set_yticks(range(len(plot_data)))\n",
    "        ax.set_yticklabels(plot_data.index, fontsize=8)\n",
    "        ax.axvline(x=missing_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'é˜ˆå€¼ ({missing_threshold}%)')\n",
    "        ax.set_xlabel('ç¼ºå¤±å€¼ç™¾åˆ†æ¯” (%)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # æ ¹æ®æ¨¡å¼è°ƒæ•´æ ‡é¢˜\n",
    "        title = f'åŠ¨æ€ç‰¹å¾ç¼ºå¤±å€¼åˆ†å¸ƒ (Top 30) - {\"åˆ é™¤æ¨¡å¼\" if drop_high_missing else \"ä»…åˆ†æ\"}'\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, 'baseline_features_missing_plot.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        if verbose:\n",
    "            print(f\"ç¼ºå¤±å€¼åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. å‡†å¤‡è¿”å›çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats_summary = {\n",
    "        'baseline_cols': len(baseline_cols),\n",
    "        'cols_dropped': len(cols_to_drop) if drop_high_missing else 0,\n",
    "        'cols_kept': len(cols_to_keep),\n",
    "        'cols_above_threshold': len(cols_to_drop),  # è®°å½•è¶…è¿‡é˜ˆå€¼çš„åˆ—æ•°\n",
    "        'original_shape': df.shape,\n",
    "        'filtered_shape': df_filtered.shape,\n",
    "        'missing_stats_df': df_missing_stats,\n",
    "        'dropped_cols': cols_to_drop if drop_high_missing else [],\n",
    "        'suggested_drop_cols': cols_to_drop,  # å»ºè®®åˆ é™¤çš„åˆ—ï¼ˆæ— è®ºæ˜¯å¦çœŸæ­£åˆ é™¤ï¼‰\n",
    "        'drop_mode_enabled': drop_high_missing  # è®°å½•æ˜¯å¦å¯ç”¨åˆ é™¤æ¨¡å¼\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nå¤„ç†åæ•°æ®å½¢çŠ¶: {df_filtered.shape}\")\n",
    "        remaining_baseline = sum(1 for col in df_filtered.columns if col.startswith('baseline_'))\n",
    "        print(f\"å‰©ä½™åŠ¨æ€ç‰¹å¾åˆ—æ•°: {remaining_baseline}\")\n",
    "        \n",
    "        if not drop_high_missing and cols_to_drop:\n",
    "            print(f\"\\nğŸ’¡ æç¤º: å½“å‰ä¸ºä»…åˆ†ææ¨¡å¼ï¼Œæœªåˆ é™¤ä»»ä½•åˆ—\")\n",
    "            print(f\"   å¦‚éœ€åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—ï¼Œè¯·è®¾ç½® drop_high_missing=True\")\n",
    "    \n",
    "    return df_filtered, stats_summary\n",
    "\n",
    "# ==============================================================\n",
    "# 3.é™æ€ç‰¹å¾ç¼ºå¤±å€¼è¿‡æ»¤å‡½æ•°å®šä¹‰\n",
    "# ==============================================================\n",
    "# ä¸“é—¨é’ˆå¯¹é™æ€ç‰¹å¾ï¼šè‡ªåŠ¨æ’é™¤ baseline_ å¼€å¤´çš„åŠ¨æ€ç‰¹å¾å’ŒæŒ‡å®šçš„æ’é™¤åˆ—\n",
    "# çµæ´»çš„æ’é™¤åˆ—è¡¨ï¼šé€šè¿‡ exclude_cols å‚æ•°è‡ªå®šä¹‰éœ€è¦æ’é™¤çš„åˆ—\n",
    "# ä¸åŠ¨æ€ç‰¹å¾ä¸€è‡´çš„é€»è¾‘ï¼šç›¸åŒçš„å‚æ•°ã€è¿”å›å€¼å’Œå¯è§†åŒ–é£æ ¼\n",
    "def filter_static_features_by_missing(df, missing_threshold=20.0, \n",
    "                                      output_dir='/home/phl/PHL/Car-T/model_v1/output/static_feature_missing',\n",
    "                                      verbose=True,\n",
    "                                      drop_high_missing=True,\n",
    "                                      exclude_cols=None):\n",
    "    \"\"\"\n",
    "    è¿‡æ»¤é™æ€ç‰¹å¾ä¸­ç¼ºå¤±å€¼è¶…è¿‡é˜ˆå€¼çš„åˆ—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        df: DataFrame, åŒ…å«é™æ€ç‰¹å¾çš„æ•°æ®æ¡†\n",
    "        missing_threshold: float, ç¼ºå¤±å€¼ç™¾åˆ†æ¯”é˜ˆå€¼ï¼ˆé»˜è®¤20%ï¼‰\n",
    "        output_dir: str, è¾“å‡ºç›®å½•è·¯å¾„\n",
    "        verbose: bool, æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯\n",
    "        drop_high_missing: bool, æ˜¯å¦åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—ï¼ˆTrue=åˆ é™¤ï¼ŒFalse=ä»…åˆ†æï¼‰\n",
    "        exclude_cols: list, éœ€è¦æ’é™¤åˆ†æçš„åˆ—ï¼ˆå¦‚IDã€æ ‡ç­¾åˆ—ç­‰ï¼‰\n",
    "    \n",
    "    è¿”å›:\n",
    "        DataFrame: è¿‡æ»¤åçš„æ•°æ®æ¡†ï¼ˆå¦‚æœdrop_high_missing=Falseï¼Œåˆ™è¿”å›åŸå§‹æ•°æ®æ¡†ï¼‰\n",
    "        dict: åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"é™æ€ç‰¹å¾ç¼ºå¤±å€¼{'è¿‡æ»¤' if drop_high_missing else 'åˆ†æ'}ï¼ˆé˜ˆå€¼: {missing_threshold}%ï¼‰\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    # 1. ç¡®å®šè¦åˆ†æçš„åˆ—\n",
    "    # é»˜è®¤æ’é™¤çš„åˆ—ï¼šIDã€ç›®æ ‡å˜é‡\n",
    "    default_exclude = ['ID', 'Infection', 'CRS', 'ICANS', 'E_ICAHT', 'L_ICAHT']\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = default_exclude\n",
    "    else:\n",
    "        exclude_cols = list(set(default_exclude + exclude_cols))\n",
    "    \n",
    "    # æå–é™æ€ç‰¹å¾åˆ—ï¼ˆæ’é™¤baseline_å¼€å¤´çš„åŠ¨æ€ç‰¹å¾å’Œéœ€è¦æ’é™¤çš„åˆ—ï¼‰\n",
    "    static_cols = [col for col in df.columns \n",
    "                   if not col.startswith('baseline_') and col not in exclude_cols]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\næå–åˆ°çš„é™æ€ç‰¹å¾åˆ—æ•°é‡: {len(static_cols)}\")\n",
    "        print(f\"æ’é™¤çš„åˆ—: {exclude_cols}\")\n",
    "    \n",
    "    if len(static_cols) == 0:\n",
    "        print(\"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ä»»ä½•é™æ€ç‰¹å¾åˆ—\")\n",
    "        return df, {'static_cols': 0, 'cols_dropped': 0, 'cols_kept': 0}\n",
    "    \n",
    "    # 2. è®¡ç®—ç¼ºå¤±å€¼ç»Ÿè®¡\n",
    "    missing_stats = {}\n",
    "    for col in static_cols:\n",
    "        missing_count = df[col].isna().sum()\n",
    "        missing_pct = (missing_count / len(df)) * 100\n",
    "        missing_stats[col] = {\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct\n",
    "        }\n",
    "    \n",
    "    # è½¬æ¢ä¸º DataFrame\n",
    "    df_missing_stats = pd.DataFrame(missing_stats).T\n",
    "    df_missing_stats = df_missing_stats.sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\né™æ€ç‰¹å¾ç¼ºå¤±å€¼ç»Ÿè®¡ï¼ˆå‰10ä¸ªï¼‰:\")\n",
    "        print(df_missing_stats.head(10).to_string())\n",
    "    \n",
    "    # 3. åº”ç”¨é˜ˆå€¼è¿‡æ»¤\n",
    "    cols_to_drop = [col for col, stats in missing_stats.items() \n",
    "                    if stats['missing_pct'] > missing_threshold]\n",
    "    cols_to_keep = [col for col, stats in missing_stats.items() \n",
    "                    if stats['missing_pct'] <= missing_threshold]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nç¼ºå¤±å€¼{'è¿‡æ»¤' if drop_high_missing else 'åˆ†æ'}ç»“æœ:\")\n",
    "        print(f\"  è¶…è¿‡é˜ˆå€¼çš„åˆ—æ•°: {len(cols_to_drop)}\")\n",
    "        print(f\"  ä½äºé˜ˆå€¼çš„åˆ—æ•°: {len(cols_to_keep)}\")\n",
    "    \n",
    "    # 4. æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ é™¤åˆ—\n",
    "    df_filtered = df.copy()\n",
    "    if drop_high_missing and cols_to_drop:\n",
    "        if verbose:\n",
    "            print(f\"\\næ­£åœ¨åˆ é™¤ {len(cols_to_drop)} ä¸ªé«˜ç¼ºå¤±ç‡é™æ€ç‰¹å¾...\")\n",
    "            print(f\"åˆ é™¤çš„åˆ—ç¤ºä¾‹ï¼ˆå‰5ä¸ªï¼‰: {cols_to_drop[:5]}\")\n",
    "        df_filtered = df_filtered.drop(columns=cols_to_drop)\n",
    "    elif not drop_high_missing:\n",
    "        if verbose:\n",
    "            print(f\"\\nâš ï¸ ä»…åˆ†ææ¨¡å¼ï¼šæœªåˆ é™¤ä»»ä½•åˆ—\")\n",
    "            if cols_to_drop:\n",
    "                print(f\"å»ºè®®åˆ é™¤çš„åˆ—ï¼ˆè¶…è¿‡{missing_threshold}%ç¼ºå¤±ç‡ï¼‰:\")\n",
    "                for col in cols_to_drop[:10]:\n",
    "                    pct = missing_stats[col]['missing_pct']\n",
    "                    print(f\"  - {col}: {pct:.2f}%\")\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(\"\\nâœ… æ‰€æœ‰é™æ€ç‰¹å¾çš„ç¼ºå¤±å€¼æ¯”ä¾‹å‡ä½äºé˜ˆå€¼ï¼Œæ— éœ€åˆ é™¤\")\n",
    "    \n",
    "    # 5. ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    report_path = os.path.join(output_dir, 'static_features_missing_report.csv')\n",
    "    df_missing_stats.to_csv(report_path)\n",
    "    if verbose:\n",
    "        print(f\"\\nç¼ºå¤±å€¼ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}\")\n",
    "    \n",
    "    # 6. å¯è§†åŒ–\n",
    "    if len(df_missing_stats) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # å–å‰30ä¸ªç‰¹å¾è¿›è¡Œå±•ç¤ºï¼ˆå¦‚æœé™æ€ç‰¹å¾å°‘äº30ä¸ªåˆ™å…¨éƒ¨å±•ç¤ºï¼‰\n",
    "        n_features_to_plot = min(30, len(df_missing_stats))\n",
    "        plot_data = df_missing_stats.head(n_features_to_plot).sort_values('missing_pct', ascending=True)\n",
    "        \n",
    "        colors = ['#E74C3C' if pct > missing_threshold else '#2ECC71' \n",
    "                  for pct in plot_data['missing_pct']]\n",
    "        \n",
    "        ax.barh(range(len(plot_data)), plot_data['missing_pct'], color=colors, alpha=0.7)\n",
    "        ax.set_yticks(range(len(plot_data)))\n",
    "        ax.set_yticklabels(plot_data.index, fontsize=8)\n",
    "        ax.axvline(x=missing_threshold, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'é˜ˆå€¼ ({missing_threshold}%)')\n",
    "        ax.set_xlabel('ç¼ºå¤±å€¼ç™¾åˆ†æ¯” (%)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # æ ¹æ®æ¨¡å¼è°ƒæ•´æ ‡é¢˜\n",
    "        title = f'é™æ€ç‰¹å¾ç¼ºå¤±å€¼åˆ†å¸ƒ (Top {n_features_to_plot}) - {\"åˆ é™¤æ¨¡å¼\" if drop_high_missing else \"ä»…åˆ†æ\"}'\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = os.path.join(output_dir, 'static_features_missing_plot.png')\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        if verbose:\n",
    "            print(f\"ç¼ºå¤±å€¼åˆ†å¸ƒå›¾å·²ä¿å­˜è‡³: {plot_path}\")\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. å‡†å¤‡è¿”å›çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats_summary = {\n",
    "        'static_cols': len(static_cols),\n",
    "        'cols_dropped': len(cols_to_drop) if drop_high_missing else 0,\n",
    "        'cols_kept': len(cols_to_keep),\n",
    "        'cols_above_threshold': len(cols_to_drop),\n",
    "        'original_shape': df.shape,\n",
    "        'filtered_shape': df_filtered.shape,\n",
    "        'missing_stats_df': df_missing_stats,\n",
    "        'dropped_cols': cols_to_drop if drop_high_missing else [],\n",
    "        'suggested_drop_cols': cols_to_drop,\n",
    "        'drop_mode_enabled': drop_high_missing\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nå¤„ç†åæ•°æ®å½¢çŠ¶: {df_filtered.shape}\")\n",
    "        remaining_static = sum(1 for col in df_filtered.columns \n",
    "                              if not col.startswith('baseline_') and col not in exclude_cols)\n",
    "        print(f\"å‰©ä½™é™æ€ç‰¹å¾åˆ—æ•°: {remaining_static}\")\n",
    "        \n",
    "        if not drop_high_missing and cols_to_drop:\n",
    "            print(f\"\\nğŸ’¡ æç¤º: å½“å‰ä¸ºä»…åˆ†ææ¨¡å¼ï¼Œæœªåˆ é™¤ä»»ä½•åˆ—\")\n",
    "            print(f\"   å¦‚éœ€åˆ é™¤é«˜ç¼ºå¤±ç‡åˆ—ï¼Œè¯·è®¾ç½® drop_high_missing=True\")\n",
    "    \n",
    "    return df_filtered, stats_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95aeba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# é€šç”¨ç‰¹å¾åˆ é™¤å‡½æ•°\n",
    "# =============================================================================\n",
    "def drop_features_from_datasets(X_train_df, X_test_df, columns_to_drop=None, \n",
    "                                 feature_names=None, verbose=True, features_to_keep=None):\n",
    "    \"\"\"\n",
    "    ä»è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åŒæ­¥åˆ é™¤æŒ‡å®šçš„ç‰¹å¾åˆ—\n",
    "    \n",
    "    å‚æ•°:\n",
    "        X_train_df: pd.DataFrame\n",
    "            è®­ç»ƒé›†DataFrame\n",
    "        X_test_df: pd.DataFrame\n",
    "            æµ‹è¯•é›†DataFrame\n",
    "        columns_to_drop: list, optional\n",
    "            éœ€è¦åˆ é™¤çš„ç‰¹å¾åˆ—ååˆ—è¡¨\n",
    "        feature_names: list, optional\n",
    "            å½“å‰çš„ç‰¹å¾ååˆ—è¡¨,å¦‚æœæä¾›åˆ™ä¼šåŒæ­¥æ›´æ–°\n",
    "        verbose: bool, default=True\n",
    "            æ˜¯å¦æ‰“å°è¯¦ç»†æ—¥å¿—\n",
    "        features_to_keep: list, optional\n",
    "            éœ€è¦ä¿ç•™çš„ç‰¹å¾åˆ—ååˆ—è¡¨(holdåˆ—è¡¨)ã€‚å¦‚æœæä¾›ï¼Œå°†ä¿ç•™è¿™äº›åˆ—å¹¶åˆ é™¤å…¶ä»–æ‰€æœ‰åˆ—ã€‚\n",
    "    \n",
    "    è¿”å›:\n",
    "        tuple: (X_train_filtered, X_test_filtered, feature_names_updated, stats_dict)\n",
    "            - X_train_filtered: åˆ é™¤ç‰¹å¾åçš„è®­ç»ƒé›†\n",
    "            - X_test_filtered: åˆ é™¤ç‰¹å¾åçš„æµ‹è¯•é›†\n",
    "            - feature_names_updated: æ›´æ–°åçš„ç‰¹å¾ååˆ—è¡¨\n",
    "            - stats_dict: åˆ é™¤æ“ä½œçš„ç»Ÿè®¡ä¿¡æ¯å­—å…¸\n",
    "    \n",
    "    å¼‚å¸¸:\n",
    "        ValueError: å½“è¾“å…¥æ•°æ®ä¸åˆæ³•æ—¶æŠ›å‡º\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # ===================== æ­¥éª¤1: è¾“å…¥éªŒè¯ =====================\n",
    "    if not isinstance(X_train_df, pd.DataFrame):\n",
    "        raise ValueError(\"X_train_df å¿…é¡»æ˜¯ pandas DataFrame\")\n",
    "    if not isinstance(X_test_df, pd.DataFrame):\n",
    "        raise ValueError(\"X_test_df å¿…é¡»æ˜¯ pandas DataFrame\")\n",
    "        \n",
    "    if columns_to_drop is None and features_to_keep is None:\n",
    "        raise ValueError(\"å¿…é¡»æä¾› columns_to_drop æˆ– features_to_keep å…¶ä¸­ä¹‹ä¸€\")\n",
    "\n",
    "    if columns_to_drop is not None and not isinstance(columns_to_drop, (list, tuple, np.ndarray)):\n",
    "        raise ValueError(\"columns_to_drop å¿…é¡»æ˜¯åˆ—è¡¨æˆ–æ•°ç»„\")\n",
    "    \n",
    "    if features_to_keep is not None and not isinstance(features_to_keep, (list, tuple, np.ndarray)):\n",
    "        raise ValueError(\"features_to_keep å¿…é¡»æ˜¯åˆ—è¡¨æˆ–æ•°ç»„\")\n",
    "    \n",
    "    # ===================== ç‰¹æ®Šé€»è¾‘: å¤„ç† features_to_keep =====================\n",
    "    if features_to_keep is not None:\n",
    "        features_to_keep = list(features_to_keep)\n",
    "        if verbose:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"ç‰¹å¾ä¿ç•™æ“ä½œ (Hold Mode)\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"\\nè¯·æ±‚ä¿ç•™çš„ç‰¹å¾æ•° (Hold List): {len(features_to_keep)}\")\n",
    "            \n",
    "        # 1. æ£€æŸ¥ä¿ç•™åˆ—è¡¨ä¸­çš„åˆ—æ˜¯å¦å­˜åœ¨\n",
    "        missing_in_train = [col for col in features_to_keep if col not in X_train_df.columns]\n",
    "        missing_in_test = [col for col in features_to_keep if col not in X_test_df.columns]\n",
    "        \n",
    "        if verbose and (missing_in_train or missing_in_test):\n",
    "            print(f\"\\nä¿ç•™åˆ—éªŒè¯è¯¦æƒ…:\")\n",
    "            if missing_in_train:\n",
    "                print(f\"  âš ï¸ {len(missing_in_train)} ä¸ªä¿ç•™åˆ—åœ¨è®­ç»ƒé›†ä¸­ç¼ºå¤±\")\n",
    "                for col in missing_in_train[:5]: print(f\"    - {col}\")\n",
    "            if missing_in_test:\n",
    "                print(f\"  âš ï¸ {len(missing_in_test)} ä¸ªä¿ç•™åˆ—åœ¨æµ‹è¯•é›†ä¸­ç¼ºå¤±\")\n",
    "        \n",
    "        # 2. è½¬æ¢é€»è¾‘ï¼šé™¤äº†ä¿ç•™åˆ—ä¹‹å¤–çš„æ‰€æœ‰åˆ—éƒ½éœ€è¦è¢«åˆ é™¤\n",
    "        # ä»¥è®­ç»ƒé›†ä¸ºåŸºå‡†è®¡ç®—è¦åˆ é™¤çš„åˆ—\n",
    "        all_train_cols = X_train_df.columns.tolist()\n",
    "        columns_to_drop = [col for col in all_train_cols if col not in features_to_keep]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nè®¡ç®—ç»“æœ:\")\n",
    "            print(f\"  åŸç‰¹å¾æ€»æ•°: {len(all_train_cols)}\")\n",
    "            print(f\"  å°†è¦ä¿ç•™çš„æœ‰æ•ˆç‰¹å¾æ•°: {len([c for c in all_train_cols if c in features_to_keep])}\")\n",
    "            print(f\"  å°†è¦åˆ é™¤çš„ç‰¹å¾æ•°: {len(columns_to_drop)}\")\n",
    "            \n",
    "    else:\n",
    "        # ===================== å¸¸è§„é€»è¾‘: ä½¿ç”¨ columns_to_drop =====================\n",
    "        # è½¬æ¢ä¸ºåˆ—è¡¨\n",
    "        columns_to_drop = list(columns_to_drop)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"ç‰¹å¾åˆ é™¤æ“ä½œ\")\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"\\nè¯·æ±‚åˆ é™¤çš„ç‰¹å¾æ•°: {len(columns_to_drop)}\")\n",
    "    \n",
    "    # ===================== æ­¥éª¤2: éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨ =====================\n",
    "    # æ£€æŸ¥å“ªäº›åˆ—å­˜åœ¨äºè®­ç»ƒé›†ä¸­\n",
    "    cols_in_train = [col for col in columns_to_drop if col in X_train_df.columns]\n",
    "    cols_not_in_train = [col for col in columns_to_drop if col not in X_train_df.columns]\n",
    "    \n",
    "    # æ£€æŸ¥å“ªäº›åˆ—å­˜åœ¨äºæµ‹è¯•é›†ä¸­\n",
    "    cols_in_test = [col for col in columns_to_drop if col in X_test_df.columns]\n",
    "    cols_not_in_test = [col for col in columns_to_drop if col not in X_test_df.columns]\n",
    "    \n",
    "    # æ‰¾å‡ºåœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­éƒ½å­˜åœ¨çš„åˆ—\n",
    "    cols_to_drop_valid = list(set(cols_in_train) & set(cols_in_test))\n",
    "    \n",
    "    # æ‰¾å‡ºä»…åœ¨ä¸€ä¸ªæ•°æ®é›†ä¸­å­˜åœ¨çš„åˆ—(å¯èƒ½çš„æ•°æ®ä¸ä¸€è‡´)\n",
    "    cols_only_in_train = list(set(cols_in_train) - set(cols_in_test))\n",
    "    cols_only_in_test = list(set(cols_in_test) - set(cols_in_train))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nåˆ—éªŒè¯ç»“æœ:\")\n",
    "        print(f\"  âœ“ åœ¨ä¸¤ä¸ªæ•°æ®é›†ä¸­éƒ½å­˜åœ¨: {len(cols_to_drop_valid)} åˆ—\")\n",
    "        if cols_not_in_train or cols_not_in_test:\n",
    "            print(f\"  âš  åœ¨è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨: {len(cols_not_in_train)} åˆ—\")\n",
    "            print(f\"  âš  åœ¨æµ‹è¯•é›†ä¸­ä¸å­˜åœ¨: {len(cols_not_in_test)} åˆ—\")\n",
    "        if cols_only_in_train:\n",
    "            print(f\"  âš  ä»…åœ¨è®­ç»ƒé›†ä¸­å­˜åœ¨: {len(cols_only_in_train)} åˆ—\")\n",
    "        if cols_only_in_test:\n",
    "            print(f\"  âš  ä»…åœ¨æµ‹è¯•é›†ä¸­å­˜åœ¨: {len(cols_only_in_test)} åˆ—\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæœªæ‰¾åˆ°çš„åˆ—ç¤ºä¾‹\n",
    "        if cols_not_in_train:\n",
    "            print(f\"\\n  æœªåœ¨è®­ç»ƒé›†ä¸­æ‰¾åˆ°çš„åˆ—ç¤ºä¾‹(å‰5ä¸ª):\")\n",
    "            for col in cols_not_in_train[:5]:\n",
    "                print(f\"    - {col}\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ•°æ®ä¸ä¸€è‡´çš„è­¦å‘Š\n",
    "        if cols_only_in_train or cols_only_in_test:\n",
    "            print(f\"\\n  âš ï¸ è­¦å‘Š: è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ—ç»“æ„ä¸ä¸€è‡´!\")\n",
    "            if cols_only_in_train:\n",
    "                print(f\"    ä»…åœ¨è®­ç»ƒé›†ä¸­å­˜åœ¨çš„åˆ—(å‰3ä¸ª): {cols_only_in_train[:3]}\")\n",
    "            if cols_only_in_test:\n",
    "                print(f\"    ä»…åœ¨æµ‹è¯•é›†ä¸­å­˜åœ¨çš„åˆ—(å‰3ä¸ª): {cols_only_in_test[:3]}\")\n",
    "    \n",
    "    # ===================== æ­¥éª¤3: è®°å½•åˆ é™¤å‰çš„çŠ¶æ€ =====================\n",
    "    original_train_shape = X_train_df.shape\n",
    "    original_test_shape = X_test_df.shape\n",
    "    original_n_features = X_train_df.shape[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nåˆ é™¤å‰:\")\n",
    "        print(f\"  è®­ç»ƒé›†å½¢çŠ¶: {original_train_shape}\")\n",
    "        print(f\"  æµ‹è¯•é›†å½¢çŠ¶: {original_test_shape}\")\n",
    "        print(f\"  ç‰¹å¾æ•°é‡: {original_n_features}\")\n",
    "    \n",
    "    # ===================== æ­¥éª¤4: æ‰§è¡Œåˆ é™¤æ“ä½œ =====================\n",
    "    if len(cols_to_drop_valid) == 0:\n",
    "        if verbose:\n",
    "            print(\"\\nâš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„åˆ—å¯ä»¥åˆ é™¤!\")\n",
    "        X_train_filtered = X_train_df.copy()\n",
    "        X_test_filtered = X_test_df.copy()\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"\\næ­£åœ¨åˆ é™¤ {len(cols_to_drop_valid)} ä¸ªç‰¹å¾...\")\n",
    "            print(f\"  åˆ é™¤çš„åˆ—ç¤ºä¾‹(å‰10ä¸ª):\")\n",
    "            for col in cols_to_drop_valid[:10]:\n",
    "                print(f\"    - {col}\")\n",
    "            if len(cols_to_drop_valid) > 10:\n",
    "                print(f\"    ... è¿˜æœ‰ {len(cols_to_drop_valid) - 10} åˆ—æœªæ˜¾ç¤º\")\n",
    "        \n",
    "        # ä»è®­ç»ƒé›†ä¸­åˆ é™¤\n",
    "        X_train_filtered = X_train_df.drop(columns=cols_to_drop_valid, errors='ignore')\n",
    "        \n",
    "        # ä»æµ‹è¯•é›†ä¸­åˆ é™¤\n",
    "        X_test_filtered = X_test_df.drop(columns=cols_to_drop_valid, errors='ignore')\n",
    "    \n",
    "    # ===================== æ­¥éª¤5: æ›´æ–°ç‰¹å¾ååˆ—è¡¨ =====================\n",
    "    if feature_names is not None:\n",
    "        # ä»ç‰¹å¾ååˆ—è¡¨ä¸­åˆ é™¤å·²åˆ é™¤çš„åˆ—\n",
    "        feature_names_updated = [fn for fn in feature_names if fn not in cols_to_drop_valid]\n",
    "    else:\n",
    "        # å¦‚æœæœªæä¾›feature_names,ä½¿ç”¨è®­ç»ƒé›†çš„åˆ—å\n",
    "        feature_names_updated = X_train_filtered.columns.tolist()\n",
    "    \n",
    "    # ===================== æ­¥éª¤6: è®°å½•åˆ é™¤åçš„çŠ¶æ€ =====================\n",
    "    filtered_train_shape = X_train_filtered.shape\n",
    "    filtered_test_shape = X_test_filtered.shape\n",
    "    filtered_n_features = X_train_filtered.shape[1]\n",
    "    \n",
    "    n_features_dropped = original_n_features - filtered_n_features\n",
    "    drop_rate = (n_features_dropped / original_n_features * 100) if original_n_features > 0 else 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nåˆ é™¤å:\")\n",
    "        print(f\"  è®­ç»ƒé›†å½¢çŠ¶: {filtered_train_shape}\")\n",
    "        print(f\"  æµ‹è¯•é›†å½¢çŠ¶: {filtered_test_shape}\")\n",
    "        print(f\"  ç‰¹å¾æ•°é‡: {filtered_n_features}\")\n",
    "        print(f\"\\nåˆ é™¤ç»Ÿè®¡:\")\n",
    "        print(f\"  å®é™…åˆ é™¤çš„ç‰¹å¾æ•°: {n_features_dropped}\")\n",
    "        print(f\"  åˆ é™¤ç‡: {drop_rate:.1f}%\")\n",
    "        print(f\"  å‰©ä½™ç‰¹å¾æ•°: {filtered_n_features}\")\n",
    "    \n",
    "    # ===================== æ­¥éª¤7: æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ =====================\n",
    "    # æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ—æ˜¯å¦ä¸€è‡´\n",
    "    train_cols = set(X_train_filtered.columns)\n",
    "    test_cols = set(X_test_filtered.columns)\n",
    "    \n",
    "    if train_cols != test_cols:\n",
    "        cols_only_train = train_cols - test_cols\n",
    "        cols_only_test = test_cols - train_cols\n",
    "        warning_msg = (\n",
    "            f\"\\nâš ï¸ è­¦å‘Š: åˆ é™¤åè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„åˆ—ä¸ä¸€è‡´!\\n\"\n",
    "            f\"  ä»…åœ¨è®­ç»ƒé›†: {len(cols_only_train)} åˆ—\\n\"\n",
    "            f\"  ä»…åœ¨æµ‹è¯•é›†: {len(cols_only_test)} åˆ—\"\n",
    "        )\n",
    "        if verbose:\n",
    "            print(warning_msg)\n",
    "    elif verbose:\n",
    "        print(f\"\\nâœ“ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡: è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ—å®Œå…¨ä¸€è‡´\")\n",
    "    \n",
    "    # ===================== æ­¥éª¤8: æ„å»ºç»Ÿè®¡ä¿¡æ¯å­—å…¸ =====================\n",
    "    stats_dict = {\n",
    "        'original_n_features': original_n_features,\n",
    "        'filtered_n_features': filtered_n_features,\n",
    "        'n_features_dropped': n_features_dropped,\n",
    "        'drop_rate_percent': drop_rate,\n",
    "        # 'cols_requested': len(columns_to_drop), # This might be misleading in keep mode if we consider requested to drop\n",
    "        'cols_dropped_valid': len(cols_to_drop_valid),\n",
    "        'cols_not_found': len(cols_not_in_train) + len(cols_not_in_test),\n",
    "        'original_train_shape': original_train_shape,\n",
    "        'filtered_train_shape': filtered_train_shape,\n",
    "        'original_test_shape': original_test_shape,\n",
    "        'filtered_test_shape': filtered_test_shape,\n",
    "        'columns_dropped': cols_to_drop_valid,\n",
    "        'data_consistent': train_cols == test_cols\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nâœ… ç‰¹å¾åˆ é™¤æ“ä½œå®Œæˆ!\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    return X_train_filtered, X_test_filtered, feature_names_updated, stats_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c699ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ¸…æ´—è„šæœ¬\n",
    "from pipeline.perfect_pipeline import ConstantColumnDropper, ToxicityBinarizer\n",
    "from pipeline.data_splitters import (\n",
    "    PatientLevelStratifiedSplitter,\n",
    "    PatientLevelStratifiedSplitterWithCV,\n",
    "    patient_level_train_test_split\n",
    ")\n",
    "\n",
    "# è¯»å…¥é™æ€æ•°æ®å’ŒåŠ¨æ€æ•°æ®\n",
    "df = pd.read_csv(\"/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/csv/B-NHL_static_data.csv\")\n",
    "dynamic_data_dir = \"/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/processed\"\n",
    "print(f\"åŸå§‹åˆ†å¸ƒ: {df.shape}\")\n",
    "# print (df)\n",
    "\n",
    "# =============================================================================\n",
    "#  1. è¡Œå¤„ç†: å‰”é™¤ Ann Arboråˆ†æœŸï¼ˆAnn Arbor stageå³AASï¼‰ ä¸ºç©ºçš„æ‚£è€…ï¼ˆå³ç™½è¡€ç—…æˆ–è€…åŸå‘ä¸­æ¢çš„æ‚£è€…ï¼‰ \n",
    "# åŸç†ï¼šåç»­è¯»å–åŠ¨æ€æ•°æ®æ—¶ï¼ˆå¦‚ build_features_from_df å‡½æ•°ï¼‰ï¼Œæ˜¯åŸºäº df['ID'] åˆ—è¡¨è¿›è¡Œéå†çš„ã€‚å› æ­¤åªéœ€è¦åœ¨å†…å­˜ä¸­çš„é™æ€æ•°æ®DataFrameå³ df é‡Œå‰”é™¤è¿™äº›æ‚£è€…å¯¹åº”çš„è¡Œã€‚\n",
    "# è¿™æ ·ï¼Œåç»­çš„ç¨‹åºé€»è¾‘è‡ªç„¶å°±ä¼šè·³è¿‡è¿™äº› IDï¼Œä»è€Œè¾¾åˆ°â€œé€»è¾‘åˆ é™¤æ•°æ®â€çš„æ•ˆæœï¼Œè€Œä¸ä¼šåœ¨ç‰©ç†ä¸Šåˆ é™¤ç£ç›˜ä¸Šçš„åŸå§‹åŠ¨æ€æ•°æ®æ–‡ä»¶ã€‚\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "# æ‰¾å‡º AASä¸º NaN çš„è¡Œ\n",
    "if 'AAS' in df.columns:\n",
    "    missing_AAS_mask = df['AAS'].isna()\n",
    "    missing_ids = df.loc[missing_AAS_mask, 'ID'].values\n",
    "    \n",
    "    if len(missing_ids) > 0:\n",
    "        print(f\"æ­¥éª¤1(è¡Œå¤„ç†): å‘ç° {len(missing_ids)} ä¾‹æ‚£è€… AAS ä¿¡æ¯ç¼ºå¤±(å³ç™½è¡€ç—…æˆ–åŸå‘ä¸­æ¢æ‚£è€…), æ‚£è€…ID: {missing_ids}, æ­£åœ¨æ‰§è¡Œå‰”é™¤æ“ä½œ...\")\n",
    "        \n",
    "        # ä»…ä»é™æ€æ•°æ® DataFrame ä¸­å‰”é™¤è¿™äº›è¡Œ, åªè¦è¿™é‡Œå‰”é™¤äº† IDï¼Œåç»­æ­¥éª¤è‡ªç„¶ä¼šè·³è¿‡è¯»å–å¯¹åº”çš„åŠ¨æ€æ•°æ®æ–‡ä»¶ã€‚\n",
    "        df_clean1 = df[~missing_AAS_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"å·²å‰”é™¤å˜é‡ AAS ä¸ºç©ºçš„æ‚£è€…\")\n",
    "        print(f\"å¤„ç†ååˆ†å¸ƒ: {df_clean1.shape}\")\n",
    "    else:\n",
    "        print(\"æ£€æŸ¥é€šè¿‡ï¼šæœªå‘ç° AAS ä¸ºç©ºçš„æ‚£è€…ã€‚\")\n",
    "else:\n",
    "    print(\"è­¦å‘Šï¼šé™æ€æ•°æ®ä¸­æœªæ‰¾åˆ° 'AAS' åˆ—ï¼Œè·³è¿‡æ­¤æ£€æŸ¥ã€‚\")\n",
    "# print (df_clean1)\n",
    "\n",
    "# =============================================================================\n",
    "#  2. è¡Œå¤„ç†: å‰”é™¤ç›®æ ‡å˜é‡ä¸ºç©ºçš„æ‚£è€…(ä»…é€»è¾‘å‰”é™¤é™æ€å˜é‡ä¸­çš„æ•°æ®ï¼Œä¸åˆ é™¤ç‰©ç†æ–‡ä»¶)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)   \n",
    "# æ­¥éª¤1: å¤„ç†ç›®æ ‡å˜é‡åˆ—ä¸­çš„ç¼ºå¤±å€¼\n",
    "if 'Infection' in df.columns:\n",
    "\n",
    "    # æ‰¾å‡ºç›®æ ‡å˜é‡ä¸ºç©ºçš„æ‚£è€…IDå¹¶æ‰“å°\n",
    "    missing_label_mask = df_clean1['Infection'].isna()\n",
    "    if missing_label_mask.sum() > 0:\n",
    "        missing_ids_infection = df_clean1.loc[missing_label_mask, 'ID'].values\n",
    "        print(f\"æ­¥éª¤2(è¡Œå¤„ç†): å‘ç° {df_clean1['Infection'].isna().sum()} ä¾‹æ‚£è€…ç›®æ ‡å˜é‡åˆ— 'Infection' ç¼ºå¤±, æ‚£è€…ID: {missing_ids_infection}, æ­£åœ¨æ‰§è¡Œå‰”é™¤æ“ä½œ...\")\n",
    "\n",
    "    # åˆ é™¤ç›®æ ‡å˜é‡åˆ—ä¸­åŒ…å« NaN çš„è¡Œï¼Œä¼šå¯¼è‡´ç´¢å¼•IDä¸è¿ç»­ï¼Œä½†æ˜¯ä¸å½±å“åç»­æ•°æ®åˆ’åˆ†\n",
    "    # åˆ†å‰²å™¨è¿”å›çš„æ˜¯ç´¢å¼•ä½ç½®(ilocï¼‰ï¼Œä¸ä¾èµ–åŸå§‹ç´¢å¼•å€¼\n",
    "    df_clean2 = df_clean1[df_clean1['Infection'].notna()].copy()\n",
    "    print(f\"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {df_clean2['Infection'].value_counts().to_dict()}\")\n",
    "    print(f\"å¤„ç†ååˆ†å¸ƒ: {df_clean2.shape}\")\n",
    "\n",
    "    # å¯é€‰ï¼šé‡ç½®ç´¢å¼•ä½¿å…¶è¿ç»­ï¼ˆæ¨èç”¨äºåç»­å¤„ç†ï¼‰ï¼Œè™½ç„¶ç´¢å¼•ä¸è¿ç»­ä¸å½±å“åˆ’åˆ†ï¼Œä½†å¦‚æœåç»­ä»£ç ä¾èµ–è¿ç»­ç´¢å¼•ï¼Œå¯èƒ½ä¼šå‡ºé—®é¢˜\n",
    "    df_clean2 = df_clean2.reset_index(drop=True)\n",
    "# print(df_clean2)\n",
    "# print(df_clean2.shape)\n",
    "\n",
    "# ============================================================================\n",
    "# æ­¥éª¤3.1: é™æ€ç‰¹å¾ç¼ºå¤±å€¼åˆ†æï¼ˆå…ˆåˆ†æé™æ€ç‰¹å¾ï¼‰\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤3.2(åˆ—å¤„ç†): é™æ€ç‰¹å¾ç¼ºå¤±å€¼åˆ†æ\")\n",
    "\n",
    "df_clean2, static_missing_stats = filter_static_features_by_missing(\n",
    "    df=df_clean2,\n",
    "    missing_threshold=20.0,\n",
    "    output_dir='/home/phl/PHL/Car-T/model_v1/output/static_feature_missing',\n",
    "    verbose=True,\n",
    "    drop_high_missing=False,  # å…ˆåˆ†æï¼ˆFalseï¼‰ï¼Œå†å†³å®šæ˜¯å¦åˆ é™¤ï¼ˆTrueï¼‰\n",
    "    exclude_cols=['ID', 'Infection', 'CRS', 'ICANS', 'E_ICAHT', 'L_ICAHT']\n",
    ")\n",
    "\n",
    "print(f\"\\næ­¥éª¤3.1å®Œæˆ: é™æ€ç‰¹å¾åˆ†æå®Œæˆ\")\n",
    "print(f\"\\nåŸå§‹å½¢çŠ¶: {static_missing_stats['original_shape']}\")\n",
    "print(f\"\\né™æ€ç‰¹å¾æ•°: {static_missing_stats['static_cols']}\")\n",
    "print(f\"å»ºè®®åˆ é™¤çš„é™æ€ç‰¹å¾åˆ—æ•°: {static_missing_stats['cols_above_threshold']}\")\n",
    "print(f\"å®é™…åˆ é™¤çš„é™æ€ç‰¹å¾åˆ—æ•°: {static_missing_stats['cols_dropped']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# æ­¥éª¤3.2: è¡Œå¤„ç†: æ‰§è¡ŒåŠ¨æ€ç‰¹å¾æå–å¹¶åˆå¹¶\n",
    "# ============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤3(è¡Œå¤„ç†): åŠ¨æ€ç‰¹å¾æå–ä¸åˆå¹¶\")\n",
    "# 1. æå–ç‰¹å¾, df_clean2 æ˜¯å½“å‰æ¸…æ´—å¥½çš„é™æ€æ•°æ®æ¡†\n",
    "\n",
    "# å¢å¼ºç‰ˆç‰¹å¾æå–â€”â€”â€”â€”åˆ›å»ºç›‘æ§å™¨ï¼ŒæŒ‡å®šè¦ç›‘æ§çš„å˜é‡\n",
    "monitor = VariableMonitor(\n",
    "    variables_to_monitor=['CBC001', 'CBC004'],  # ç›‘æ§å¤šä¸ªå˜é‡'baseline_CBC004'\n",
    "    time_window=(-15, 0)                                               # ç›‘æ§æ—¶é—´çª—å£\n",
    ")\n",
    "\n",
    "print(df_clean2)\n",
    "baseline_features_df = extract_baseline_features(\n",
    "    patient_ids=df_clean2['ID'].values,\n",
    "    dynamic_dir=dynamic_data_dir,   # ä½¿ç”¨åŠ¨æ€æ•°æ®å®šä¹‰è·¯å¾„\n",
    "    time_col='day',                 # è¯·ç¡®è®¤åŠ¨æ€CSVä¸­æ—¶é—´åˆ—åä¸º 'day' è¿˜æ˜¯ 'time'\n",
    "    cutoff_day=0,                   # æˆªæ­¢æ—¶é—´ç‚¹ä¸º0å¤©ï¼ˆå³æ²»ç–—å‰ï¼‰\n",
    "    monitor=monitor                 # ä¼ å…¥ç›‘æ§å™¨å‚æ•°åˆ—è¡¨\n",
    ")\n",
    "\n",
    "# è·å–æ‰€æœ‰å˜é‡çš„ç›‘æ§ç»“æœ\n",
    "all_empty_patients = monitor.get_empty_patient_ids()\n",
    "\n",
    "print(\"\\næ‰€æœ‰å˜é‡çš„ç©ºæ•°æ®ç»Ÿè®¡:\")\n",
    "for var_name, patient_set in all_empty_patients.items():\n",
    "    print(f\"  {var_name}: {len(patient_set)} ä¸ªæ‚£è€…\")\n",
    "    if patient_set:\n",
    "        print(f\"    æ‚£è€…ID: {sorted(list(patient_set))[:5]}\")  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "        \n",
    "# ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶\n",
    "# monitor.print_summary()     # æ‰“å°æ‘˜è¦æŠ¥å‘Š\n",
    "monitor.save_report('/home/phl/PHL/Car-T/model_v1/output/monitor_report.json')\n",
    "\n",
    "\n",
    "# 2. åˆå¹¶åˆ°ä¸»æ•°æ®æ¡†\n",
    "# ä½¿ç”¨å·¦è¿æ¥ (Left Join) ç¡®ä¿ä¸ä¼šä¸¢å¤± df_clean2 ä¸­çš„æ‚£è€…\n",
    "# å¦‚æœæŸä¸ªæ‚£è€…æ²¡æœ‰åŠ¨æ€æ•°æ®ï¼Œæ–°åˆ—å°†è‡ªåŠ¨å¡«å……ä¸º NaN\n",
    "print(f\"\\nåˆå¹¶å‰ df_clean2 å½¢çŠ¶: {df_clean2.shape}\")\n",
    "df_clean3 = pd.merge(df_clean2, baseline_features_df, on='ID', how='left')\n",
    "print(f\"åˆå¹¶å df_clean3 å½¢çŠ¶: {df_clean3.shape}\")\n",
    "\n",
    "# 3. ç®€å•çš„ç¼ºå¤±å€¼æ£€æŸ¥\n",
    "new_cols = [c for c in df_clean3.columns if c.startswith('baseline_')]\n",
    "missing_counts = df_clean3[new_cols].isna().sum()\n",
    "print(f\"\\næ–°ç‰¹å¾ç¼ºå¤±å€¼æƒ…å†µ (Top 5): \\n{missing_counts.sort_values(ascending=False).head()}\")\n",
    "print(f\"å¤„ç†ååˆ†å¸ƒ: {df_clean3.shape}\")\n",
    "# print (df_clean3)\n",
    "\n",
    "# ============================================================================\n",
    "# æ­¥éª¤3.3: åŠ¨æ€ç‰¹å¾ç¼ºå¤±å€¼åˆ†æ\n",
    "# ============================================================================\n",
    "df_clean3, dynamic_missing_stats = filter_baseline_features_by_missing(\n",
    "    df=df_clean3,\n",
    "    missing_threshold=20.0,\n",
    "    output_dir='/home/phl/PHL/Car-T/model_v1/output/dynamic_feature_missing',\n",
    "    verbose=True,\n",
    "    drop_high_missing=False  # å…ˆåˆ†æï¼ˆFalseï¼‰ï¼Œå†å†³å®šæ˜¯å¦åˆ é™¤ï¼ˆTrueï¼‰\n",
    ")\n",
    "\n",
    "print(f\"\\næ­¥éª¤3.3å®Œæˆ: åŠ¨æ€ç‰¹å¾åˆ†æå®Œæˆ\")\n",
    "print(f\"\\nåŸå§‹å½¢çŠ¶: {dynamic_missing_stats['original_shape']}\")\n",
    "print(f\"\\nåŠ¨æ€ç‰¹å¾æ•°: {dynamic_missing_stats['baseline_cols']}\")\n",
    "print(f\"å»ºè®®åˆ é™¤çš„åˆ—æ•°: {dynamic_missing_stats['cols_above_threshold']}\")\n",
    "print(f\"å®é™…åˆ é™¤çš„åˆ—æ•°: {dynamic_missing_stats['cols_dropped']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. å€¼å¤„ç†: äºŒå…ƒåŒ–æ¯’æ€§ç­‰çº§\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤4(å€¼å¤„ç†): äºŒå…ƒåŒ–æ¯’æ€§ç­‰çº§\")\n",
    "# åˆ é™¤å¸¸é‡åˆ— & ç¼ºå¤±å€¼å¤„ç†\n",
    "# dropper = ConstantColumnDropper()\n",
    "# df_clean4 = dropper.fit_transform(df_clean3)\n",
    "# æ£€æŸ¥ç¼ºå¤±å€¼å¹¶åˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ\n",
    "#df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# binarizer = ToxicityBinarizer(col=\"CRS\", threshold=2)   # å•ä¸ªæŒ‡æ ‡äºŒå…ƒåŒ–\n",
    "binarizer = ToxicityBinarizer(      # å¤šä¸ªæ¯’æ€§æŒ‡æ ‡ä½¿ç”¨ç›¸åŒé˜ˆå€¼\n",
    "    columns=[\"CRS\", \"ICANS\", \"E_ICAHT\", \"L_ICAHT\", \"Infection\"],\n",
    "    threshold=2)\n",
    "df_clean4 = binarizer.fit_transform(df_clean3)\n",
    "# df_clean4 = binarizer.fit_transform(df_clean2)\n",
    "\n",
    "# =============================================================================\n",
    "#  5. åˆ—å¤„ç†: æ‰‹åŠ¨åˆ é™¤ä¸éœ€è¦çš„å†—ä½™åˆ—ï¼ˆä»…åˆ é™¤é™æ€å˜é‡ä¸­çš„éƒ¨åˆ†åˆ—ï¼‰\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "# æ ¹æ®éœ€è¦ä¿®æ”¹å†—ä½™åˆ—åˆ—è¡¨\n",
    "cols_to_drop = ['CCID', 'Disease']\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"æ­¥éª¤5(åˆ—å¤„ç†): æ­£åœ¨åˆ é™¤å†—ä½™åˆ—: {cols_to_drop}\")\n",
    "    df_clean5 = df_clean4.drop(columns=cols_to_drop, errors='ignore') # errors='ignore' é˜²æ­¢åˆ—ä¸å­˜åœ¨æ—¶æŠ¥é”™\n",
    "    print(f\"å¤„ç†ååˆ†å¸ƒ: {df_clean5.shape}\")\n",
    "# print (df_clean5)\n",
    "df_final = df_clean5\n",
    "\n",
    "print(f\"\\næ•°æ®é¢„è§ˆ:{df_final.shape}\")\n",
    "print(f\"è¡Œæ•°:{df_final.shape[0]}ï¼Œåˆ—æ•°:{df_final.shape[1]}\")\n",
    "pd.set_option('display.max_columns', None)          # æ˜¾ç¤ºæ‰€æœ‰åˆ—\n",
    "pd.set_option('display.max_rows', None)             # æ˜¾ç¤ºæ‰€æœ‰è¡Œ\n",
    "pd.set_option('display.expand_frame_repr', False)   #ä¸æ¢è¡Œæ˜¾ç¤º\n",
    "# æ˜¾ç¤ºå¤„ç†åçš„æ•°æ®æ¡†ï¼Œhead() æ–¹æ³•æœ¬èº«å°±åªè¿”å›å‰5è¡Œæ•°æ®\n",
    "# print(df_final.head())\n",
    "# print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†åˆ’åˆ†\n",
    "# å‚è€ƒï¼šhttps://sklearn.apachecn.org/master/30/â€”â€”â€”â€”3.1.2.2.2. åˆ†å±‚éšæœº Splitï¼ˆStratifiedShuffleSplitï¼‰\n",
    "# æ–¹æ¡ˆ1:70/30 æ‚£è€…çº§åˆ†å±‚åˆ’åˆ†ï¼›\n",
    "# æ–¹æ¡ˆ2:70/30 æ‚£è€…çº§åˆ†å±‚åˆ’åˆ† + 5æŠ˜äº¤å‰éªŒè¯\n",
    "\n",
    "# =============================================================================\n",
    "# 4. æ•°æ®é›†åˆ’åˆ†ï¼Œåˆ’åˆ†ä¸º70%è®­ç»ƒé›†å’Œ30%æµ‹è¯•é›†ï¼Œä½¿ç”¨åŸºäºç±»çš„åˆ†å‰²å™¨è¿›è¡Œæ‚£è€…çº§åˆ†å±‚åˆ†å‰²\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤6(æ•°æ®é›†åˆ’åˆ†): åŸºç¡€æ‚£è€…çº§åˆ†å±‚åˆ’åˆ†\")\n",
    "\n",
    "# a.åŸºç¡€åˆ†å‰²å™¨\n",
    "# splitter = PatientLevelStratifiedSplitter(test_size=0.3, random_state=42)\n",
    "# train_df, test_df = splitter.split(\n",
    "#     df_final, \n",
    "#     label_col=\"Infection\", \n",
    "#     patient_id_col=\"ID\"  # ç¡®ä¿æ‚£è€…çº§ç‹¬ç«‹æ€§\n",
    "# )\n",
    "# print(f\"\\nè®­ç»ƒé›†å¤§å°: {len(train_df)}\")\n",
    "# print(f\"æµ‹è¯•é›†å¤§å°: {len(test_df)}\")\n",
    "\n",
    "# b.å¸¦äº¤å‰éªŒè¯çš„åˆ†å‰²å™¨ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰\n",
    "# #=====================================OLDç‰ˆæœ¬è¾“å‡ºå·®å¼‚ï¼ˆè¿”å›æ‚£è€…IDï¼‰=========================================\n",
    "# # cv_folds ç»“æ„:\n",
    "# cv_folds = [\n",
    "#     (train_patient_ids, val_patient_ids),  # Fold 1\n",
    "#     (train_patient_ids, val_patient_ids),  # Fold 2\n",
    "#     ...]\n",
    "# # ç±»å‹: List[Tuple[np.ndarray[str/int], np.ndarray[str/int]]]\n",
    "# # å†…å®¹: æ‚£è€…çš„IDå€¼ (å¦‚ [101, 205, 308, ...])\n",
    "# # ä½¿ç”¨æ–¹å¼:\n",
    "# for train_ids, val_ids in cv_folds:\n",
    "#     # å¿…é¡»æ‰‹åŠ¨è½¬æ¢: ID â†’ DataFrameè¡Œ\n",
    "#     fold_train = train_df[train_df[\"patient_id\"].isin(train_ids)]  # æ…¢!\n",
    "#     fold_val = train_df[train_df[\"patient_id\"].isin(val_ids)]\n",
    "    \n",
    "#     X_train = fold_train.drop(columns=[\"label\"])\n",
    "#     y_train = fold_train[\"label\"]\n",
    "# #=====================================NEWç‰ˆæœ¬ï¼ˆè¿”å›æ•´æ•°ç´¢å¼•ï¼‰===========================================\n",
    "# # ä¿®æ”¹å: cv_folds_indices ç›´æ¥æ˜¯æ•´æ•°ç´¢å¼•, å¯ç›´æ¥ç”¨äº X_train.iloc[],ä¸”PatientLevelStratifiedSplitterWithCV å·²é‡ç½®äº† train_df ç´¢å¼•ï¼Œ\n",
    "#    æ‰€ä»¥X_train, y_train å®é™…ä¸Šå·²ç»æ˜¯ 0,1,2...\n",
    "# Sklearnæ ‡å‡†ç”¨æ³•: è¿”å›æ•´æ•°ç´¢å¼•è€Œä¸æ˜¯æ‚£è€…ID\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# for train_idx, val_idx in skf.split(X, y):\n",
    "#     X_train, X_val = X[train_idx], X[val_idx]  # æ•´æ•°ç´¢å¼•!\n",
    "splitter = PatientLevelStratifiedSplitterWithCV(test_size=0.3, n_folds=3, random_state=42)    \n",
    "train_df, test_df, cv_folds_indices = splitter.split(\n",
    "    df_final, \n",
    "    label_col=\"Infection\", \n",
    "    patient_id_col=\"ID\"  # ç¡®ä¿æ‚£è€…çº§ç‹¬ç«‹æ€§\n",
    ")\n",
    "# æŸ¥çœ‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†å¸ƒ\n",
    "print(\"=\" * 70)\n",
    "print(f\"æ•°æ®åˆ’åˆ†å®Œæˆ:\")\n",
    "print(f\"\\nè®­ç»ƒé›†å¤§å°: {len(train_df)} æ ·æœ¬\")\n",
    "print(f\"\\nè®­ç»ƒé›†åˆ†å¸ƒ:\\n{train_df}\")\n",
    "print(f\"\\næµ‹è¯•é›†å¤§å°: {len(test_df)} æ ·æœ¬\")\n",
    "print(f\"\\næµ‹è¯•é›†åˆ†å¸ƒ:\\n{test_df}\")\n",
    "print(f\"\\näº¤å‰éªŒè¯æŠ˜æ•°: {len(cv_folds_indices)} æŠ˜\")\n",
    "print(f\"\\näº¤å‰éªŒè¯æŠ˜æ•°åŠç´¢å¼•:{cv_folds_indices}\")\n",
    "\n",
    "# å¯é€‰: éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§ (é¦–æ¬¡è¿è¡Œæ—¶å»ºè®®å¯ç”¨)\n",
    "from pipeline.data_splitters import validate_cv_indices\n",
    "validate_cv_indices(train_df, cv_folds_indices)\n",
    "\n",
    "# # # ======================è¾“å‡ºå·®å¼‚=====================\n",
    "for train_idx, val_idx in cv_folds_indices:\n",
    "    # ç›´æ¥ä½ç½®ç´¢å¼•, å¼€ç®±å³ç”¨!\n",
    "    X_train = train_df.iloc[train_idx].drop(columns=[\"Infection\"])\n",
    "    y_train = train_df.iloc[train_idx][\"Infection\"]\n",
    "    \n",
    "    X_val = train_df.iloc[val_idx].drop(columns=[\"Infection\"])\n",
    "    y_val = train_df.iloc[val_idx][\"Infection\"]\n",
    "    \n",
    "    print(f\"Fold è®­ç»ƒé›†å¤§å°: {len(X_train)}, éªŒè¯é›†å¤§å°: {len(X_val)}\")\n",
    "    print(f\"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {y_val.value_counts().to_dict()}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\nç‰¹å¾æ•°æ®é¢„è§ˆ:\\n{X_train}\")\n",
    "    print(X_train.index.tolist())\n",
    "    print(f\"\\nç›®æ ‡å˜é‡æ•°æ®é¢„è§ˆ:\\n{y_train}\")\n",
    "    print(y_train.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d0ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¿å­˜ä¸ºExcelæ–‡ä»¶\n",
    "\n",
    "import os\n",
    "\n",
    "# å®šä¹‰ä¿å­˜è·¯å¾„\n",
    "output_data_dir = '/home/phl/PHL/Car-T/model_v1/output/split_datasets'\n",
    "if not os.path.exists(output_data_dir):\n",
    "    os.makedirs(output_data_dir)\n",
    "    print(f\"åˆ›å»ºè¾“å‡ºç›®å½•: {output_data_dir}\")\n",
    "\n",
    "# # ä¿å­˜è®­ç»ƒé›†\n",
    "# train_file_path = os.path.join(output_data_dir, 'train_dataset.xlsx')\n",
    "# train_df.to_excel(train_file_path, index=False, engine='openpyxl')\n",
    "# print(f\"âœ… è®­ç»ƒé›†å·²ä¿å­˜è‡³: {train_file_path}\")\n",
    "# print(f\"   å½¢çŠ¶: {train_df.shape}\")\n",
    "\n",
    "# # ä¿å­˜æµ‹è¯•é›†\n",
    "# test_file_path = os.path.join(output_data_dir, 'test_dataset.xlsx')\n",
    "# test_df.to_excel(test_file_path, index=False, engine='openpyxl')\n",
    "# print(f\"âœ… æµ‹è¯•é›†å·²ä¿å­˜è‡³: {test_file_path}\")\n",
    "# print(f\"   å½¢çŠ¶: {test_df.shape}\")\n",
    "\n",
    "# è‹¥æ•°æ®é›†å¾ˆå¤§,Excelæ–‡ä»¶å¯èƒ½è¾ƒå¤§ã€‚å¯ä»¥è€ƒè™‘ä¿å­˜ä¸ºCSVæ ¼å¼(æ›´è½»é‡)\n",
    "train_df.to_csv(os.path.join(output_data_dir, 'train_dataset.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(output_data_dir, 'test_dataset.csv'), index=False)\n",
    "\n",
    "# # å¯é€‰: ä¿å­˜äº¤å‰éªŒè¯æŠ˜ä¿¡æ¯\n",
    "# cv_info = []\n",
    "# for i, (train_idx, val_idx) in enumerate(cv_folds_indices, 1):\n",
    "#     cv_info.append({\n",
    "#         'Fold': i,\n",
    "#         'Train_Size': len(train_idx),\n",
    "#         'Val_Size': len(val_idx),\n",
    "#         'Train_Indices': str(train_idx.tolist()),\n",
    "#         'Val_Indices': str(val_idx.tolist())\n",
    "#     })\n",
    "\n",
    "# cv_info_df = pd.DataFrame(cv_info)\n",
    "# cv_file_path = os.path.join(output_data_dir, 'cv_folds_info.xlsx')\n",
    "# cv_info_df.to_excel(cv_file_path, index=False, engine='openpyxl')\n",
    "# print(f\"âœ… äº¤å‰éªŒè¯æŠ˜ä¿¡æ¯å·²ä¿å­˜è‡³: {cv_file_path}\")\n",
    "\n",
    "print(f\"\\nğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜è‡³ç›®å½•: {output_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ea81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº”æŠ˜äº¤å‰åˆ’åˆ†æ•°æ®é¢„å¤„ç†â€”â€”â€”â€”å®šä¹‰ç‰¹å¾ç±»å‹å’Œæ¸…æ´—\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # å¯¼å…¥ç¼ºå¤±å€¼å¡«å……å™¨\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# è‡ªå®šä¹‰æ ‡ç­¾ç¼–ç å™¨ï¼Œç”¨äºå¤„ç†å¤šåˆ—\n",
    "# åœ¨å•å…ƒæ ¼ 13 ä¸­æ›¿æ¢ MultiColumnLabelEncoder\n",
    "\n",
    "class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    ç”¨äºå¯¹å¤šä¸ªç±»åˆ«åˆ—è¿›è¡Œæ ‡ç­¾ç¼–ç çš„è‡ªå®šä¹‰è½¬æ¢å™¨\n",
    "    æ”¯æŒ sklearn Pipeline çš„ get_feature_names_out æ–¹æ³•\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.label_encoders = {}\n",
    "        self.feature_names_in_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"æ‹Ÿåˆæ¯ä¸€åˆ—çš„æ ‡ç­¾ç¼–ç å™¨\"\"\"\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        self.feature_names_in_ = list(X_df.columns) if hasattr(X_df, 'columns') else [f\"col_{i}\" for i in range(X_df.shape[1])]\n",
    "        \n",
    "        for col_idx in range(X_df.shape[1]):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(X_df.iloc[:, col_idx].astype(str))\n",
    "            self.label_encoders[col_idx] = le\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"è½¬æ¢æ‰€æœ‰åˆ—\"\"\"\n",
    "        X_df = pd.DataFrame(X) if not isinstance(X, pd.DataFrame) else X\n",
    "        X_transformed = np.zeros((X_df.shape[0], X_df.shape[1]), dtype=np.float64)\n",
    "        \n",
    "        for col_idx in range(X_df.shape[1]):\n",
    "            le = self.label_encoders[col_idx]\n",
    "            col_data = X_df.iloc[:, col_idx].astype(str)\n",
    "            # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«ï¼šæ˜ å°„ä¸º -1\n",
    "            X_transformed[:, col_idx] = col_data.map(\n",
    "                lambda x: le.transform([x])[0] if x in le.classes_ else -1\n",
    "            )\n",
    "        \n",
    "        return X_transformed\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"è¿”å›è¾“å‡ºç‰¹å¾åç§° - å…¼å®¹ sklearn Pipeline\"\"\"\n",
    "        if input_features is not None:\n",
    "            return np.array([f\"cat__{name}\" for name in input_features])\n",
    "        elif self.feature_names_in_ is not None:\n",
    "            return np.array([f\"cat__{name}\" for name in self.feature_names_in_])\n",
    "        else:\n",
    "            return np.array([f\"cat__{i}\" for i in range(len(self.label_encoders))])\n",
    "\n",
    "\n",
    "# åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡\n",
    "X_train_raw = train_df.drop(columns=['Infection', 'ID',\n",
    "                                    'CRS', 'ICANS','E_ICAHT', 'L_ICAHT' \n",
    "                                     ])\n",
    "y_train = train_df['Infection']\n",
    "y_train = train_df['Infection'].astype(int) # <--- ä¿®æ”¹ï¼šå¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°ç±»å‹ï¼ŒXGBoost å› ä¸ºæ•°æ®ç±»å‹ï¼ˆæµ®ç‚¹æ ‡ç­¾ï¼‰æˆ–é…ç½®åŸå› æ²¡æœ‰æ­£ç¡®å¹¿æ’­è‡ªå·±æ˜¯ classifierï¼Œä¼šé»˜è®¤è‡ªå·±æ˜¯ regressorï¼Œä»è€Œå¯¼è‡´ AUC 0.5 çš„é—®é¢˜ã€‚\n",
    "\n",
    "X_test_raw = test_df.drop(columns=['Infection', 'ID',\n",
    "                                   'CRS', 'ICANS','E_ICAHT', 'L_ICAHT' \n",
    "                                    ])\n",
    "y_test = test_df['Infection']\n",
    "y_test = test_df['Infection'].astype(int) \n",
    "\n",
    "# =============================================================================\n",
    "# 1. ç²¾ç»†åŒ–å®šä¹‰ç‰¹å¾ç»„\n",
    "# =============================================================================\n",
    "\n",
    "# A. è¿ç»­æ•°å€¼ç‰¹å¾ -> StandardScaler\n",
    "numeric_features = ['Age', 'BMDB']\n",
    "\n",
    "# B. æ— åºåˆ†ç±»ç‰¹å¾ï¼ˆå°‘ç±»åˆ«ï¼‰-> One-Hot ç¼–ç _OneHotEncoderâ€”â€”â€”â€”â€”â€”å­—ç¬¦ä¸²ï¼Œæ— å¤§å°å…³ç³»\n",
    "nominal_features = ['CM', 'TYPE', 'PHSC']  # ç±»åˆ«æ•° â‰¤ 5\n",
    "\n",
    "# # C. æ— åºåˆ†ç±»ç‰¹å¾ï¼ˆå¤šç±»åˆ«ï¼‰-> æ ‡ç­¾ç¼–ç _LabelEncoderï¼ˆä»…ç”¨äºæ ‘æ¨¡å‹ï¼‰\n",
    "# nominal_features_many = ['PHSC']  # ç±»åˆ«æ•° > 5\n",
    "\n",
    "# D. æœ‰åºåˆ†ç±»ç‰¹å¾ -> OrdinalEncoderï¼ˆä¿ç•™é¡ºåºï¼‰\n",
    "# å·²ç»æ˜¯ 0,1,2 æ ¼å¼ -> ä¿æŒæ•°å€¼ç‰¹æ€§\n",
    "# è¿™æ · SVM/LR å¯ä»¥åˆ©ç”¨å…¶å¤§å°å…³ç³» (å¦‚ AAS åˆ†æœŸ 4 > 1)ï¼Œæ ‘æ¨¡å‹ä¹Ÿèƒ½æ›´å¥½åˆ‡åˆ†\n",
    "# æ³¨æ„ï¼šè¯·æ£€æŸ¥ PHSC, BMC ç­‰æ˜¯å¦çœŸçš„æ˜¯æ•°å€¼ 0,1,2ã€‚å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¯·ç§»åˆ° nominal_features\n",
    "ordinal_features = ['Sex', 'BMC', 'EI', 'EM', 'B_symptoms', 'AAS', 'NL', 'BT', 'CTFA', 'PCT'\n",
    "                    # 'CRS', 'ICANS','E_ICAHT', 'L_ICAHT'\n",
    "                ]  # æœ‰æ˜ç¡®é¡ºåºçš„\n",
    "\n",
    "# E. äºŒåˆ†ç±»ç‰¹å¾ -> ä¿æŒåŸå€¼ï¼ˆæ— éœ€ç¼–ç ï¼‰\n",
    "# binary_features = ['Sex', 'BMC', 'EI', 'EM', 'B_symptoms', 'BT', 'CTFA', 'PCT']\n",
    "\n",
    "# F. è‡ªåŠ¨è¯†åˆ«åŠ¨æ€ç‰¹å¾ (baseline_*)ï¼Œè¿™äº›é€šå¸¸æ˜¯æ•°å€¼å‹ä½†åŒ…å«è„æ•°æ®\n",
    "\n",
    "# æ’é™¤æ‰ä¸Šé¢å·²ç»å®šä¹‰è¿‡çš„æ‰€æœ‰åˆ—\n",
    "defined_features = numeric_features + nominal_features + ordinal_features\n",
    "other_features = [col for col in X_train_raw.columns if col not in defined_features]\n",
    "print(f\"è‡ªåŠ¨è¯†åˆ«å‡ºçš„åŠ¨æ€/å…¶ä»–ç‰¹å¾: {len(other_features)} ä¸ª\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. æ•°æ®æ¸…æ´—(éœ€å¯¹è¿ç»­æ•°å€¼ç‰¹å¾ã€åŠ¨æ€ç‰¹å¾ã€ç±»åˆ«ç‰¹å¾åˆ†ç±»å‹å¤„ç†)\n",
    "# =============================================================================\n",
    "X_train = X_train_raw.copy()\n",
    "X_test = X_test_raw.copy()\n",
    "\n",
    "# ç»„1: éœ€è¦ä¿æŒä¸ºæ•°å€¼çš„åˆ— (è¿ç»­æ•°å€¼ + æœ‰åº + åŠ¨æ€)\n",
    "cols_to_numeric = numeric_features + ordinal_features + other_features\n",
    "print(\"æ­£åœ¨æ¸…æ´—æ•°å€¼å‹å’Œæœ‰åºç±»åˆ«ç‰¹å¾...\")\n",
    "# å°† DataFrame ä¸­æ‰€æœ‰æ•°æ®å¼ºåˆ¶è½¬æ•°å€¼ï¼Œéæ•°å­—(è„æ•°æ®)å˜ NaN, åç»­å¯ä»¥é€šè¿‡ SimpleImputer ç»Ÿä¸€å¡«å……è¿™äº›ç¼ºå¤±å€¼\n",
    "X_train[cols_to_numeric] = X_train[cols_to_numeric].apply(pd.to_numeric, errors='coerce')\n",
    "X_test[cols_to_numeric] = X_test[cols_to_numeric].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# B. å¡«å……æ•°å€¼åˆ—çš„ç¼ºå¤±å€¼ (ç”¨ä¸­ä½æ•°æˆ–0, ä¿æŒæ•´æ•°ç‰¹æ€§çš„åˆ—é€šå¸¸ä¸­ä½æ•°ä¹Ÿæ˜¯æ•´æ•°)\n",
    "# è®¡ç®—è®­ç»ƒé›†ä¸­ä½æ•°ï¼Œå¦‚æœä¸­ä½æ•°ä¹Ÿæ˜¯NaN (è¯´æ˜æ•´åˆ—éƒ½æ˜¯NaN)ï¼Œåˆ™å¡« 0 (ä»£è¡¨æ— æ•°æ®)\n",
    "train_medians = X_train[cols_to_numeric].median()\n",
    "X_train[cols_to_numeric] = X_train[cols_to_numeric].fillna(train_medians).fillna(0)\n",
    "X_test[cols_to_numeric] = X_test[cols_to_numeric].fillna(train_medians).fillna(0)\n",
    "\n",
    "# ç»„2: æ— åºç±»åˆ«åˆ— (å­—ç¬¦ä¸²)(å¡«å……ç¼ºå¤±å€¼ä¸º 'missing'ï¼Œä¿ç•™å­—ç¬¦ä¸²æ ¼å¼)\n",
    "print(\"æ­£åœ¨æ¸…æ´—æ— åºç±»åˆ«ç‰¹å¾...\")\n",
    "X_train[nominal_features] = X_train[nominal_features].fillna('missing').astype(str)\n",
    "X_test[nominal_features] = X_test[nominal_features].fillna('missing').astype(str)\n",
    "\n",
    "print(f\"æ•°æ®æ¸…æ´—å®Œæˆã€‚\")\n",
    "print(f\"æœ‰åºç‰¹å¾ (å¦‚ AAS) ç¤ºä¾‹å€¼: {X_train['AAS'].unique()}\")\n",
    "print(f\"æ— åºç‰¹å¾ (å¦‚ TYPE) ç¤ºä¾‹å€¼: {X_train['TYPE'].unique()}\")\n",
    "print(f\"æ•°æ®æ¸…æ´—å®Œæˆã€‚X_train å½¢çŠ¶: {X_train.shape}\")\n",
    "print(f\"æ•°å€¼åˆ—æ— ç¼ºå¤±å€¼: {not X_train[cols_to_numeric].isna().any().any()}\")\n",
    "print(f\"ç±»åˆ«åˆ—ä¿ç•™åŸå§‹æ ¼å¼ (ç¤ºä¾‹): {X_train['Sex'].unique()}\")\n",
    "# print(f\"é¢„å¤„ç†åçš„è®­ç»ƒæ•°æ®ï¼š{X_train}\")\n",
    "# print(f\"é¢„å¤„ç†åçš„æµ‹è¯•æ•°æ®ï¼š{X_test}\")\n",
    "print(f\"é¢„å¤„ç†å X_train ç¼ºå¤±å€¼æ•°é‡: {X_train.isna().sum().sum()}\")\n",
    "print(f\"é¢„å¤„ç†å X_test ç¼ºå¤±å€¼æ•°é‡: {X_test.isna().sum().sum()}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. å®šä¹‰ Pipeline é¢„å¤„ç†å™¨\n",
    "# https://zhuanlan.zhihu.com/p/42368821\n",
    "# æ„å»ºä¸€ä¸ªåŒ…å«é¢„å¤„ç†å’Œæ¨¡å‹çš„ Pipelineï¼Œæ‰”ç»™ GridSearchCV\n",
    "# ä¸è¦åœ¨è¿™é‡Œåš preprocessor.fit_transformï¼Œåªåšåˆ—ç±»å‹çš„å®šä¹‰å’ŒåŸºç¡€æ¸…æ´—ï¼ˆè¡Œç‹¬ç«‹çš„æ“ä½œï¼‰\n",
    "# =============================================================================\n",
    "\n",
    "# A.æ•°å€¼/æœ‰åºç‰¹å¾é¢„å¤„ç†\n",
    "# å½’ä¸€åŒ–å¯¹ SVM/LR å¾ˆé‡è¦ï¼Œå¯¹æœ‰åºç±»åˆ«(0,1,2)åšæ ‡å‡†åŒ–ä¹Ÿæ˜¯å®‰å…¨çš„\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),      # ç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼      \n",
    "    ('scaler', StandardScaler())                        # æ•°å€¼ç‰¹å¾æ ‡å‡†åŒ–\n",
    "])\n",
    "\n",
    "# ============================================================================\n",
    "# B.æ— åºç±»åˆ«ç‰¹å¾é¢„å¤„ç†æ–¹æ¡ˆé€‰æ‹©\n",
    "# ============================================================================\n",
    "# æ–¹æ¡ˆ1: ç‹¬çƒ­ç¼–ç  (One-Hot Encoding) - å·²æ³¨é‡Šï¼Œä¿ç•™å¤‡ç”¨\n",
    "# ä¼˜ç‚¹: ä¸å¼•å…¥é¡ºåºå‡è®¾ï¼Œé€‚åˆçº¿æ€§æ¨¡å‹\n",
    "# ç¼ºç‚¹: å¯èƒ½äº§ç”Ÿé«˜ç»´ç¨€ç–ç‰¹å¾ï¼Œå¢åŠ ç‰¹å¾é—´å†—ä½™å’Œå¤šé‡å…±çº¿æ€§\n",
    "# ----------------------------------------------------------------------------\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # ç”¨å¸¸æ•°'missing'å¡«å……ç¼ºå¤±å€¼\n",
    "    # drop='first' é€‚åˆçº¿æ€§æ¨¡å‹(é€»è¾‘å›å½’)ï¼Œhandle_unknown=PCgnore' é€‚åˆæ ‘æ¨¡å‹\n",
    "    # å¦‚æœä¸»è¦ç”¨ XGBoost/RFï¼Œå»ºè®®å»æ‰ drop='first'ï¼Œä¿ç•™ handle_unknown='ignore'\n",
    "    # sparse_output=False é€‚ç”¨äº sklearn >= 1.2ï¼Œæ—§ç‰ˆæœ¬è¯·ç”¨ sparse=False\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))         # å¤„ç†æµ‹è¯•é›†ä¸­çš„æ–°ç±»åˆ«\n",
    "])\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# æ–¹æ¡ˆ2: æ ‡ç­¾ç¼–ç  (Label Encoding) - å½“å‰ä½¿ç”¨\n",
    "# ä¼˜ç‚¹: é™ä½ç‰¹å¾ç»´åº¦ï¼Œå‡å°‘å†—ä½™å’Œå¤šé‡å…±çº¿æ€§ï¼Œæé«˜è®¡ç®—æ•ˆç‡\n",
    "# ç¼ºç‚¹: å¼•å…¥äº†é¡ºåºå‡è®¾ï¼ˆä½†æ ‘æ¨¡å‹å¯ä»¥å¾ˆå¥½å¤„ç†ï¼‰\n",
    "# æ³¨æ„: é€‚åˆæ ‘æ¨¡å‹ï¼ˆå¦‚XGBoost/RandomForestï¼‰ï¼Œå¯¹äºçº¿æ€§æ¨¡å‹å»ºè®®ä½¿ç”¨ç‹¬çƒ­ç¼–ç \n",
    "# ----------------------------------------------------------------------------\n",
    "# å½“å‰ä½¿ç”¨: æ ‡ç­¾ç¼–ç  (Label Encoding) ç”¨äºæ— åºç±»åˆ«ç‰¹å¾\n",
    "# é€‚ç”¨äº: æ ‘æ¨¡å‹ (XGBoost, RandomForestç­‰)\n",
    "# è‹¥éœ€ä½¿ç”¨ç‹¬çƒ­ç¼–ç ï¼Œè¯·å–æ¶ˆæ³¨é‡Šä¸Šé¢çš„æ–¹æ¡ˆ1ä»£ç å¹¶æ³¨é‡Šæ–¹æ¡ˆ2\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('label_encoder', MultiColumnLabelEncoder())  # æ ‡ç­¾ç¼–ç \n",
    "# ])\n",
    "\n",
    "\n",
    "# # å°‘ç±»åˆ«æ— åºåˆ†ç±» -> OneHot\n",
    "# nominal_few_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "#     ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "# ])\n",
    "\n",
    "# # å¤šç±»åˆ«æ— åºåˆ†ç±» -> Labelï¼ˆä»…æ ‘æ¨¡å‹æ¨èï¼‰\n",
    "# nominal_many_transformer = Pipeline(steps=[\n",
    "#     ('label_encoder', MultiColumnLabelEncoder())\n",
    "# ])\n",
    "\n",
    "# # æœ‰åºåˆ†ç±» -> Ordinal\n",
    "# ordinal_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "# ])\n",
    "\n",
    "# # äºŒåˆ†ç±» -> ç›´æ¥æ ‡å‡†åŒ–ï¼ˆå·²ç»æ˜¯ 0/1ï¼‰\n",
    "# binary_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('scaler', StandardScaler())  # å¯é€‰ï¼šå¯¹äºŒåˆ†ç±»æ˜¯å¦æ ‡å‡†åŒ–å–å†³äºæ¨¡å‹\n",
    "# ])\n",
    "\n",
    "\n",
    "# ç»„åˆé¢„å¤„ç†å™¨ ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_numeric),              # å°† è¿ç»­æ•°å€¼ + æœ‰åºç±»åˆ« + åŠ¨æ€ç‰¹å¾ å…¨éƒ¨ä½œä¸ºæ•°å€¼å¤„ç†\n",
    "        ('cat', categorical_transformer, nominal_features)      # å¯¹ æ— åºå­—ç¬¦ä¸² åš One-Hot ç¼–ç æˆ–æ ‡ç­¾ç¼–ç \n",
    "        # ('nom_few', nominal_few_transformer, nominal_features_few),\n",
    "        # ('nom_many', nominal_many_transformer, nominal_features_many),\n",
    "        # ('ord', ordinal_transformer, ordinal_features),\n",
    "        # ('bin', binary_transformer, binary_features),\n",
    "    ],\n",
    "    # remainder='drop'            # å…¶ä»–æœªå®šä¹‰çš„åˆ—ä¸¢å¼ƒ\n",
    "    remainder='passthrough'     # é‡è¦ï¼šä¿ç•™æœªåœ¨ä¸Šé¢åˆ—å‡ºçš„å…¶ä»–åˆ—ï¼Œå¦åˆ™ä¼šè¢«ä¸¢å¼ƒï¼›è¿™é‡Œçš„ passthrough ä¼šè®©æ¸…æ´—å¥½çš„ baseline ç‰¹å¾ç›´æ¥é€šè¿‡\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967192d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ç‰¹å¾è¿‡æ»¤æ­¥éª¤1:æŸ¥çœ‹å¹¶ä¿å­˜ X_testã€y_trainã€y_test çš„ä¿¡æ¯\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"X_testã€y_trainã€y_test ä¿¡æ¯æŸ¥çœ‹ä¸ä¿å­˜\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output/feature_info'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# =============================================================================\n",
    "# 1. X_test åˆ—åä¿å­˜\n",
    "# =============================================================================\n",
    "print(\"\\nã€1ã€‘X_test åˆ—åä¿å­˜\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "X_test_column_names = X_test.columns.tolist()\n",
    "\n",
    "print(f\"X_test çš„å½¢çŠ¶: {X_test.shape}\")\n",
    "print(f\"æ€»åˆ—æ•°: {len(X_test_column_names)}\")\n",
    "\n",
    "# åœ¨æ§åˆ¶å°æ‰“å°æ‰€æœ‰åˆ—å\n",
    "print(f\"\\nX_test æ‰€æœ‰åˆ—å:\")\n",
    "for i, col in enumerate(X_test_column_names, 1):\n",
    "    print(f\"  {i}. {col}\")\n",
    "\n",
    "# æŒ‰ç‰¹å¾ç±»å‹åˆ†ç»„\n",
    "baseline_cols_test = [col for col in X_test_column_names if col.startswith('baseline_')]\n",
    "other_cols_test = [col for col in X_test_column_names if not col.startswith('baseline_')]\n",
    "\n",
    "print(f\"\\nX_test ç‰¹å¾åˆ†å¸ƒ:\")\n",
    "print(f\"  é™æ€ç‰¹å¾: {len(other_cols_test)} ä¸ª\")\n",
    "print(f\"  åŠ¨æ€ç‰¹å¾ (baseline_*): {len(baseline_cols_test)} ä¸ª\")\n",
    "\n",
    "# ä¿å­˜åˆ°txtæ–‡ä»¶\n",
    "output_file_test = os.path.join(output_dir, 'X_test_column_names.txt')\n",
    "\n",
    "with open(output_file_test, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# X_test åˆ—åæ¸…å•\\n\")\n",
    "    f.write(f\"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"# æ•°æ®å½¢çŠ¶: {X_test.shape}\\n\")\n",
    "    f.write(f\"# æ€»åˆ—æ•°: {len(X_test_column_names)}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"æ‰€æœ‰åˆ—ååˆ—è¡¨:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, col in enumerate(X_test_column_names, 1):\n",
    "        f.write(f\"{i}. {col}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(\"ç‰¹å¾ç±»å‹åˆ†ç»„:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    f.write(f\"\\né™æ€ç‰¹å¾ ({len(other_cols_test)} ä¸ª):\\n\")\n",
    "    for i, col in enumerate(other_cols_test, 1):\n",
    "        f.write(f\"  {i}. {col}\\n\")\n",
    "    \n",
    "    f.write(f\"\\nåŠ¨æ€ç‰¹å¾ ({len(baseline_cols_test)} ä¸ª):\\n\")\n",
    "    for i, col in enumerate(baseline_cols_test, 1):\n",
    "        f.write(f\"  {i}. {col}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… X_test åˆ—åå·²ä¿å­˜è‡³: {output_file_test}\")\n",
    "\n",
    "# ä¿å­˜ä¸ºCSVæ ¼å¼\n",
    "csv_file_test = os.path.join(output_dir, 'X_test_column_names.csv')\n",
    "pd.DataFrame({\n",
    "    'åºå·': range(1, len(X_test_column_names) + 1),\n",
    "    'åˆ—å': X_test_column_names,\n",
    "    'ç‰¹å¾ç±»å‹': ['åŠ¨æ€ç‰¹å¾' if col.startswith('baseline_') else 'é™æ€ç‰¹å¾' \n",
    "               for col in X_test_column_names]\n",
    "}).to_csv(csv_file_test, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… X_test åˆ—åCSVå·²ä¿å­˜è‡³: {csv_file_test}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. y_train ä¿¡æ¯ä¿å­˜\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€2ã€‘y_train ä¿¡æ¯ä¿å­˜\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"y_train çš„å½¢çŠ¶: {y_train.shape}\")\n",
    "print(f\"y_train æ•°æ®ç±»å‹: {y_train.dtype}\")\n",
    "print(f\"y_train æ ‡ç­¾åˆ†å¸ƒ:\\n{y_train.value_counts().to_dict()}\")\n",
    "\n",
    "# ä¿å­˜åˆ°txtæ–‡ä»¶\n",
    "output_file_ytrain = os.path.join(output_dir, 'y_train_info.txt')\n",
    "\n",
    "with open(output_file_ytrain, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# y_train ä¿¡æ¯æ¸…å•\\n\")\n",
    "    f.write(f\"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"# æ•°æ®å½¢çŠ¶: {y_train.shape}\\n\")\n",
    "    f.write(f\"# æ•°æ®ç±»å‹: {y_train.dtype}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"æ ‡ç­¾åˆ†å¸ƒ:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for label, count in y_train.value_counts().sort_index().items():\n",
    "        percentage = (count / len(y_train)) * 100\n",
    "        f.write(f\"  æ ‡ç­¾ {label}: {count} ä¸ª ({percentage:.2f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ€»æ ·æœ¬æ•°: {len(y_train)}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(\"å‰20ä¸ªæ ·æœ¬é¢„è§ˆ:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, val in enumerate(y_train.head(20), 1):\n",
    "        f.write(f\"  {i}. {val}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… y_train ä¿¡æ¯å·²ä¿å­˜è‡³: {output_file_ytrain}\")\n",
    "\n",
    "# ä¿å­˜ä¸ºCSVæ ¼å¼\n",
    "csv_file_ytrain = os.path.join(output_dir, 'y_train_data.csv')\n",
    "pd.DataFrame({\n",
    "    'ç´¢å¼•': y_train.index,\n",
    "    'æ ‡ç­¾': y_train.values\n",
    "}).to_csv(csv_file_ytrain, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… y_train æ•°æ®CSVå·²ä¿å­˜è‡³: {csv_file_ytrain}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. y_test ä¿¡æ¯ä¿å­˜\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€3ã€‘y_test ä¿¡æ¯ä¿å­˜\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(f\"y_test çš„å½¢çŠ¶: {y_test.shape}\")\n",
    "print(f\"y_test æ•°æ®ç±»å‹: {y_test.dtype}\")\n",
    "print(f\"y_test æ ‡ç­¾åˆ†å¸ƒ:\\n{y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# ä¿å­˜åˆ°txtæ–‡ä»¶\n",
    "output_file_ytest = os.path.join(output_dir, 'y_test_info.txt')\n",
    "\n",
    "with open(output_file_ytest, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# y_test ä¿¡æ¯æ¸…å•\\n\")\n",
    "    f.write(f\"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"# æ•°æ®å½¢çŠ¶: {y_test.shape}\\n\")\n",
    "    f.write(f\"# æ•°æ®ç±»å‹: {y_test.dtype}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"æ ‡ç­¾åˆ†å¸ƒ:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for label, count in y_test.value_counts().sort_index().items():\n",
    "        percentage = (count / len(y_test)) * 100\n",
    "        f.write(f\"  æ ‡ç­¾ {label}: {count} ä¸ª ({percentage:.2f}%)\\n\")\n",
    "    \n",
    "    f.write(f\"\\næ€»æ ·æœ¬æ•°: {len(y_test)}\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(\"å‰20ä¸ªæ ·æœ¬é¢„è§ˆ:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, val in enumerate(y_test.head(20), 1):\n",
    "        f.write(f\"  {i}. {val}\\n\")\n",
    "\n",
    "print(f\"\\nâœ… y_test ä¿¡æ¯å·²ä¿å­˜è‡³: {output_file_ytest}\")\n",
    "\n",
    "# ä¿å­˜ä¸ºCSVæ ¼å¼\n",
    "csv_file_ytest = os.path.join(output_dir, 'y_test_data.csv')\n",
    "pd.DataFrame({\n",
    "    'ç´¢å¼•': y_test.index,\n",
    "    'æ ‡ç­¾': y_test.values\n",
    "}).to_csv(csv_file_ytest, index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(f\"âœ… y_test æ•°æ®CSVå·²ä¿å­˜è‡³: {csv_file_ytest}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 4. ç»¼åˆç»Ÿè®¡æ‘˜è¦\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ã€4ã€‘ç»¼åˆç»Ÿè®¡æ‘˜è¦\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_file = os.path.join(output_dir, 'data_summary.txt')\n",
    "\n",
    "with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ•°æ®é›†ç»¼åˆç»Ÿè®¡æ‘˜è¦\\n\")\n",
    "    f.write(f\"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. X_train (è®­ç»ƒç‰¹å¾é›†)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"   å½¢çŠ¶: {X_train.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)\\n\")\n",
    "    f.write(f\"   åˆ—æ•°: {len(X_train.columns)}\\n\")\n",
    "    f.write(f\"   é™æ€ç‰¹å¾: {len([col for col in X_train.columns if not col.startswith('baseline_')])} ä¸ª\\n\")\n",
    "    f.write(f\"   åŠ¨æ€ç‰¹å¾: {len([col for col in X_train.columns if col.startswith('baseline_')])} ä¸ª\\n\\n\")\n",
    "    \n",
    "    f.write(\"2. X_test (æµ‹è¯•ç‰¹å¾é›†)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"   å½¢çŠ¶: {X_test.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)\\n\")\n",
    "    f.write(f\"   åˆ—æ•°: {len(X_test.columns)}\\n\")\n",
    "    f.write(f\"   é™æ€ç‰¹å¾: {len([col for col in X_test.columns if not col.startswith('baseline_')])} ä¸ª\\n\")\n",
    "    f.write(f\"   åŠ¨æ€ç‰¹å¾: {len([col for col in X_test.columns if col.startswith('baseline_')])} ä¸ª\\n\\n\")\n",
    "    \n",
    "    f.write(\"3. y_train (è®­ç»ƒæ ‡ç­¾)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"   å½¢çŠ¶: {y_train.shape}\\n\")\n",
    "    f.write(f\"   æ ·æœ¬æ•°: {len(y_train)}\\n\")\n",
    "    f.write(f\"   æ ‡ç­¾åˆ†å¸ƒ: {y_train.value_counts().to_dict()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"4. y_test (æµ‹è¯•æ ‡ç­¾)\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"   å½¢çŠ¶: {y_test.shape}\\n\")\n",
    "    f.write(f\"   æ ·æœ¬æ•°: {len(y_test)}\\n\")\n",
    "    f.write(f\"   æ ‡ç­¾åˆ†å¸ƒ: {y_test.value_counts().to_dict()}\\n\\n\")\n",
    "    \n",
    "    f.write(\"5. æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    total_samples = len(X_train) + len(X_test)\n",
    "    train_ratio = (len(X_train) / total_samples) * 100\n",
    "    test_ratio = (len(X_test) / total_samples) * 100\n",
    "    f.write(f\"   æ€»æ ·æœ¬æ•°: {total_samples}\\n\")\n",
    "    f.write(f\"   è®­ç»ƒé›†: {len(X_train)} ({train_ratio:.2f}%)\\n\")\n",
    "    f.write(f\"   æµ‹è¯•é›†: {len(X_test)} ({test_ratio:.2f}%)\\n\")\n",
    "\n",
    "print(f\"\\nâœ… ç»¼åˆç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜è‡³: {summary_file}\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ€»ç»“\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… æ‰€æœ‰æ•°æ®ä¿¡æ¯ä¿å­˜å®Œæˆ!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nä¿å­˜çš„æ–‡ä»¶åˆ—è¡¨:\")\n",
    "print(f\"  1. X_train_column_names.txt\")\n",
    "print(f\"  2. X_train_column_names.csv\")\n",
    "print(f\"  3. X_test_column_names.txt\")\n",
    "print(f\"  4. X_test_column_names.csv\")\n",
    "print(f\"  5. y_train_info.txt\")\n",
    "print(f\"  6. y_train_data.csv\")\n",
    "print(f\"  7. y_test_info.txt\")\n",
    "print(f\"  8. y_test_data.csv\")\n",
    "print(f\"  9. data_summary.txt (ç»¼åˆæ‘˜è¦)\")\n",
    "print(f\"\\nä¿å­˜è·¯å¾„: {output_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dce32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# # ç‰¹å¾è¿‡æ»¤æ­¥éª¤2:åŸºäºç‰¹å¾é€‰æ‹©ç»“æœçš„æ•°æ®é›†è¿‡æ»¤æ¨¡å—\n",
    "# =============================================================================\n",
    "# ä½œç”¨: æ ¹æ®ç‰¹å¾é€‰æ‹©åˆ†æç»“æœ,ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰å‡º10ä¸ªå…³é”®ç‰¹å¾\n",
    "# è¾“å…¥: X_train, X_test (æ¸…æ´—åçš„åŸå§‹DataFrame), selected_features (ç‰¹å¾åˆ—è¡¨)\n",
    "# è¾“å‡º: X_train_filtered, X_test_filtered (ä»…åŒ…å«é€‰å®šç‰¹å¾çš„DataFrame)\n",
    "# =============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"åŸºäºç‰¹å¾é€‰æ‹©ç»“æœçš„æ•°æ®é›†è¿‡æ»¤\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 1: å®šä¹‰/åŠ è½½é€‰å®šçš„10ä¸ªç‰¹å¾\n",
    "# =============================================================================\n",
    "print(\"\\næ­¥éª¤ 1: ç¡®å®šè¦ä¿ç•™çš„ 10 ä¸ªç‰¹å¾\")\n",
    "\n",
    "# æ–¹æ¡ˆ1: æ‰‹åŠ¨æŒ‡å®šç‰¹å¾åç§°åˆ—è¡¨ï¼ˆåŸºäºæ‚¨çš„ç‰¹å¾é€‰æ‹©ç»“æœï¼‰\n",
    "# æ³¨æ„: è¿™é‡Œä½¿ç”¨çš„æ˜¯**åŸå§‹åˆ—å**ï¼ˆæ¸…æ´—åã€é¢„å¤„ç†å‰çš„åˆ—åï¼‰\n",
    "selected_features = [\n",
    "    'CTFA',                         # é™æ€ç‰¹å¾: è‡ªä½“ç§»æ¤åºè´¯\n",
    "    'EI',                           # é™æ€ç‰¹å¾: ç»“å¤–æœ‰æ— ç—…å˜\n",
    "    'CM',                           # é™æ€ç‰¹å¾: CAR-Tå…±åˆºæ¿€åˆ†å­\n",
    "    'PCT',                          # é™æ€ç‰¹å¾: æ—¢å¾€æœ‰æ— CAR-Tæ²»ç–—\n",
    "    'TYPE',                         # é™æ€ç‰¹å¾: CAR-Tç»“æ„ç±»å‹\n",
    "    'baseline_CBC013',              # åŠ¨æ€ç‰¹å¾: è¡€çº¢è›‹ç™½\n",
    "    'baseline_Vital Signs001',      # åŠ¨æ€ç‰¹å¾: ä½“æ¸©\n",
    "    'baseline_Electrolytes004',     # åŠ¨æ€ç‰¹å¾: æ ¡æ­£é’™\n",
    "    'baseline_Biochemistry002',     # åŠ¨æ€ç‰¹å¾: å¤©é—¨å†¬æ°¨é…¸æ°¨åŸºè½¬ç§»é…¶\n",
    "    'baseline_Coagulation006',      # åŠ¨æ€ç‰¹å¾: å‡è¡€é…¶æ—¶é—´\n",
    "]\n",
    "\n",
    "# æ–¹æ¡ˆ2: ä»ç‰¹å¾é€‰æ‹©ç»“æœæ–‡ä»¶ä¸­è‡ªåŠ¨åŠ è½½ï¼ˆæ¨èï¼‰\n",
    "# feature_selection_result_path = '/home/phl/PHL/Car-T/model_v1/output/feature_selection/selected_features.txt'\n",
    "# try:\n",
    "#     with open(feature_selection_result_path, 'r', encoding='utf-8') as f:\n",
    "#         selected_features = [line.strip() for line in f if line.strip() and not line.startswith('#')]\n",
    "#     print(f\"  âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(selected_features)} ä¸ªç‰¹å¾\")\n",
    "# except Exception as e:\n",
    "#     print(f\"  âš ï¸ è‡ªåŠ¨åŠ è½½å¤±è´¥: {e}, ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç‰¹å¾åˆ—è¡¨\")\n",
    "\n",
    "print(f\"\\né€‰å®šçš„ {len(selected_features)} ä¸ªç‰¹å¾:\")\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    feat_type = \"åŠ¨æ€ç‰¹å¾\" if feat.startswith('baseline_') else \"é™æ€ç‰¹å¾\"\n",
    "    print(f\"  {i}. {feat:30s} ({feat_type})\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 2: éªŒè¯ç‰¹å¾æ˜¯å¦å­˜åœ¨äºåŸå§‹æ•°æ®é›†ä¸­\n",
    "# =============================================================================\n",
    "print(\"\\næ­¥éª¤ 2: éªŒè¯ç‰¹å¾æœ‰æ•ˆæ€§...\")\n",
    "\n",
    "# æ£€æŸ¥è®­ç»ƒé›†\n",
    "missing_in_train = [f for f in selected_features if f not in X_train.columns]\n",
    "if missing_in_train:\n",
    "    print(f\"  âš ï¸ è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾åœ¨è®­ç»ƒé›†ä¸­ä¸å­˜åœ¨:\")\n",
    "    for feat in missing_in_train:\n",
    "        print(f\"    - {feat}\")\n",
    "    print(f\"\\n  å°è¯•ä»å¯ç”¨ç‰¹å¾ä¸­å¯»æ‰¾ç›¸ä¼¼ç‰¹å¾...\")\n",
    "    print(f\"  è®­ç»ƒé›†å¯ç”¨ç‰¹å¾ç¤ºä¾‹: {list(X_train.columns[:10])}\")\n",
    "\n",
    "# æ£€æŸ¥æµ‹è¯•é›†\n",
    "missing_in_test = [f for f in selected_features if f not in X_test.columns]\n",
    "if missing_in_test:\n",
    "    print(f\"  âš ï¸ è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾åœ¨æµ‹è¯•é›†ä¸­ä¸å­˜åœ¨:\")\n",
    "    for feat in missing_in_test:\n",
    "        print(f\"    - {feat}\")\n",
    "\n",
    "# ç§»é™¤ä¸å­˜åœ¨çš„ç‰¹å¾ï¼ˆç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸€è‡´æ€§ï¼‰\n",
    "valid_features = [f for f in selected_features if f in X_train.columns and f in X_test.columns]\n",
    "\n",
    "if len(valid_features) < len(selected_features):\n",
    "    print(f\"\\n  å·²è‡ªåŠ¨ç§»é™¤ä¸å­˜åœ¨çš„ç‰¹å¾\")\n",
    "    print(f\"  åŸå§‹ç‰¹å¾æ•°: {len(selected_features)}\")\n",
    "    print(f\"  æœ‰æ•ˆç‰¹å¾æ•°: {len(valid_features)}\")\n",
    "    \n",
    "    # å¦‚æœæœ‰æ•ˆç‰¹å¾ä¸è¶³10ä¸ªï¼Œä»å‰©ä½™ç‰¹å¾ä¸­è¡¥å……\n",
    "    if len(valid_features) < 10:\n",
    "        print(f\"\\n  âš ï¸ æœ‰æ•ˆç‰¹å¾ä¸è¶³10ä¸ªï¼Œä»å‰©ä½™ç‰¹å¾ä¸­è¡¥å……...\")\n",
    "        \n",
    "        # æ’é™¤å·²é€‰å’Œä¸åº”åŒ…å«çš„åˆ—\n",
    "        exclude_cols = ['ID', 'Infection', 'CRS', 'ICANS', 'E_ICAHT', 'L_ICAHT']\n",
    "        available_features = [f for f in X_train.columns \n",
    "                            if f not in valid_features \n",
    "                            and f not in exclude_cols\n",
    "                            and f in X_test.columns]  # ç¡®ä¿æµ‹è¯•é›†ä¹Ÿæœ‰\n",
    "        \n",
    "        n_needed = 10 - len(valid_features)\n",
    "        additional_features = available_features[:n_needed]\n",
    "        valid_features.extend(additional_features)\n",
    "        \n",
    "        print(f\"  è¡¥å……çš„ç‰¹å¾: {additional_features}\")\n",
    "else:\n",
    "    valid_features = selected_features\n",
    "    print(f\"  âœ… æ‰€æœ‰é€‰å®šç‰¹å¾å‡å­˜åœ¨äºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 3: ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰ç‰¹å¾ï¼ˆå…³é”®æ­¥éª¤ï¼‰\n",
    "# =============================================================================\n",
    "print(f\"\\næ­¥éª¤ 3: ä»åŸå§‹æ•°æ®ä¸­ç­›é€‰ç‰¹å¾...\")\n",
    "\n",
    "# è®°å½•åŸå§‹å½¢çŠ¶\n",
    "original_train_shape = X_train.shape\n",
    "original_test_shape = X_test.shape\n",
    "\n",
    "# ã€å…³é”®ã€‘ç›´æ¥è¦†ç›–å…¨å±€å˜é‡ï¼ˆä»…ä¿ç•™é€‰å®šçš„åˆ—ï¼‰\n",
    "# æ³¨æ„: è¿™æ˜¯åœ¨é¢„å¤„ç†ï¼ˆPipelineï¼‰ä¹‹å‰è¿›è¡Œçš„è¿‡æ»¤\n",
    "X_train = X_train[valid_features].copy()\n",
    "X_test = X_test[valid_features].copy()\n",
    "\n",
    "print(f\"\\n  ç‰¹å¾è¿‡æ»¤ç»“æœ:\")\n",
    "print(f\"    è®­ç»ƒé›†: {original_train_shape} â†’ {X_train.shape}\")\n",
    "print(f\"    æµ‹è¯•é›†: {original_test_shape} â†’ {X_test.shape}\")\n",
    "print(f\"    ç‰¹å¾å‡å°‘: {original_train_shape[1]} â†’ {X_train.shape[1]} \" +\n",
    "      f\"(å‡å°‘ {original_train_shape[1] - X_train.shape[1]} ä¸ªç‰¹å¾, \" +\n",
    "      f\"ä¿ç•™ {X_train.shape[1] / original_train_shape[1] * 100:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 4: æ•°æ®è´¨é‡æ£€æŸ¥\n",
    "# =============================================================================\n",
    "print(f\"\\næ­¥éª¤ 4: æ•°æ®è´¨é‡æ£€æŸ¥...\")\n",
    "\n",
    "# 4.1 æ£€æŸ¥ç¼ºå¤±å€¼\n",
    "train_missing = X_train.isna().sum().sum()\n",
    "test_missing = X_test.isna().sum().sum()\n",
    "\n",
    "print(f\"\\n  ã€ç¼ºå¤±å€¼ç»Ÿè®¡ã€‘\")\n",
    "print(f\"    è®­ç»ƒé›†: {train_missing} ä¸ª ({train_missing / X_train.size * 100:.2f}%)\")\n",
    "print(f\"    æµ‹è¯•é›†: {test_missing} ä¸ª ({test_missing / X_test.size * 100:.2f}%)\")\n",
    "\n",
    "if train_missing > 0 or test_missing > 0:\n",
    "    print(f\"\\n  å„ç‰¹å¾çš„ç¼ºå¤±æƒ…å†µ:\")\n",
    "    missing_stats = pd.DataFrame({\n",
    "        'ç‰¹å¾å': valid_features,\n",
    "        'è®­ç»ƒé›†ç¼ºå¤±': [X_train[f].isna().sum() for f in valid_features],\n",
    "        'è®­ç»ƒé›†ç¼ºå¤±ç‡(%)': [X_train[f].isna().sum() / len(X_train) * 100 for f in valid_features],\n",
    "        'æµ‹è¯•é›†ç¼ºå¤±': [X_test[f].isna().sum() for f in valid_features],\n",
    "        'æµ‹è¯•é›†ç¼ºå¤±ç‡(%)': [X_test[f].isna().sum() / len(X_test) * 100 for f in valid_features]\n",
    "    })\n",
    "    missing_stats = missing_stats[(missing_stats['è®­ç»ƒé›†ç¼ºå¤±'] > 0) | (missing_stats['æµ‹è¯•é›†ç¼ºå¤±'] > 0)]\n",
    "    if not missing_stats.empty:\n",
    "        print(missing_stats.to_string(index=False))\n",
    "    print(f\"\\n  ğŸ’¡ æç¤º: ç¼ºå¤±å€¼å°†ç”±Pipelineä¸­çš„SimpleImputerè‡ªåŠ¨å¤„ç†\")\n",
    "\n",
    "# 4.2 æ•°æ®ç±»å‹æ£€æŸ¥\n",
    "print(f\"\\n  ã€æ•°æ®ç±»å‹åˆ†å¸ƒã€‘\")\n",
    "print(f\"    è®­ç»ƒé›†:\")\n",
    "print(X_train.dtypes.value_counts().to_string())\n",
    "\n",
    "# 4.3 æ£€æŸ¥è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ä¸€è‡´æ€§\n",
    "assert list(X_train.columns) == list(X_test.columns), \\\n",
    "    \"âŒ é”™è¯¯: è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç‰¹å¾åˆ—ä¸ä¸€è‡´!\"\n",
    "print(f\"\\n  âœ… è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾åˆ—å®Œå…¨ä¸€è‡´\")\n",
    "\n",
    "# 4.4 åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯\n",
    "print(f\"\\n  ã€è®­ç»ƒé›†ç»Ÿè®¡æ‘˜è¦ã€‘\")\n",
    "print(X_train.describe().T.to_string())\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 5: æ›´æ–° preprocessor çš„ç‰¹å¾åˆ—è¡¨ï¼ˆè‡ªåŠ¨é€‚é…ï¼‰\n",
    "# =============================================================================\n",
    "print(f\"\\næ­¥éª¤ 5: æ›´æ–°é¢„å¤„ç†å™¨é…ç½®...\")\n",
    "\n",
    "# é‡æ–°å®šä¹‰ç‰¹å¾ç»„ï¼ˆåŸºäºç­›é€‰åçš„åˆ—ï¼‰\n",
    "numeric_features_new = [f for f in ['Age', 'BMDB'] if f in X_train.columns]\n",
    "nominal_features_new = [f for f in ['CM', 'TYPE', 'PHSC'] if f in X_train.columns]\n",
    "ordinal_features_new = [f for f in ['AAS', 'Sex', 'BMC', 'EI', 'NL', 'EM', \n",
    "                                     'B_symptoms', 'BT', 'CTFA', 'PCT'] \n",
    "                        if f in X_train.columns]\n",
    "\n",
    "# åŠ¨æ€ç‰¹å¾ï¼ˆbaseline_*ï¼‰\n",
    "other_features_new = [col for col in X_train.columns \n",
    "                      if col not in numeric_features_new + nominal_features_new + ordinal_features_new]\n",
    "\n",
    "print(f\"\\n  æ›´æ–°åçš„ç‰¹å¾åˆ†ç»„:\")\n",
    "print(f\"    æ•°å€¼ç‰¹å¾ ({len(numeric_features_new)}): {numeric_features_new}\")\n",
    "print(f\"    æ— åºç±»åˆ« ({len(nominal_features_new)}): {nominal_features_new}\")\n",
    "print(f\"    æœ‰åºç±»åˆ« ({len(ordinal_features_new)}): {ordinal_features_new}\")\n",
    "print(f\"    åŠ¨æ€ç‰¹å¾ ({len(other_features_new)}): {other_features_new[:5]}...\" if len(other_features_new) > 5 else f\"    åŠ¨æ€ç‰¹å¾ ({len(other_features_new)}): {other_features_new}\")\n",
    "\n",
    "# é‡æ–°æ„å»º ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "cols_to_numeric_new = numeric_features_new + ordinal_features_new + other_features_new\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, cols_to_numeric_new),\n",
    "        ('cat', categorical_transformer, nominal_features_new)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(f\"\\n  âœ… preprocessor å·²æ›´æ–°ä¸ºé€‚é… {X_train.shape[1]} ä¸ªç‰¹å¾çš„é…ç½®\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 6: ä¿å­˜è¿‡æ»¤åçš„æ•°æ®é›†å’Œå…ƒæ•°æ®\n",
    "# =============================================================================\n",
    "print(f\"\\næ­¥éª¤ 6: ä¿å­˜è¿‡æ»¤åçš„æ•°æ®é›†...\")\n",
    "\n",
    "output_dir_filtered = '/home/phl/PHL/Car-T/model_v1/output/filtered_features_data'\n",
    "if not os.path.exists(output_dir_filtered):\n",
    "    os.makedirs(output_dir_filtered)\n",
    "\n",
    "# 6.1 ä¿å­˜ç‰¹å¾åˆ—è¡¨\n",
    "with open(os.path.join(output_dir_filtered, 'filtered_features_list.txt'), 'w', encoding='utf-8') as f:\n",
    "    f.write(\"# æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„è¿‡æ»¤åç‰¹å¾åˆ—è¡¨\\n\")\n",
    "    f.write(f\"# ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now()}\\n\")\n",
    "    f.write(f\"# åŸå§‹ç‰¹å¾æ•°: {original_train_shape[1]}\\n\")\n",
    "    f.write(f\"# è¿‡æ»¤åç‰¹å¾æ•°: {len(valid_features)}\\n\")\n",
    "    f.write(f\"# ç‰¹å¾å‡å°‘ç‡: {(original_train_shape[1] - len(valid_features)) / original_train_shape[1] * 100:.1f}%\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i, feat in enumerate(valid_features, 1):\n",
    "        feat_type = \"åŠ¨æ€ç‰¹å¾\" if feat.startswith('baseline_') else \"é™æ€ç‰¹å¾\"\n",
    "        f.write(f\"{i}. {feat:30s} ({feat_type})\\n\")\n",
    "\n",
    "# 6.2 ä¿å­˜æ•°æ®é›†\n",
    "X_train.to_csv(os.path.join(output_dir_filtered, 'X_train_filtered.csv'), index=False)\n",
    "X_test.to_csv(os.path.join(output_dir_filtered, 'X_test_filtered.csv'), index=False)\n",
    "\n",
    "# 6.3 ä¿å­˜æ ‡ç­¾\n",
    "pd.Series(y_train, name='Infection').to_csv(\n",
    "    os.path.join(output_dir_filtered, 'y_train.csv'), index=False\n",
    ")\n",
    "pd.Series(y_test, name='Infection').to_csv(\n",
    "    os.path.join(output_dir_filtered, 'y_test.csv'), index=False\n",
    ")\n",
    "\n",
    "# 6.4 ä¿å­˜ç‰¹å¾ç±»å‹æ˜ å°„\n",
    "feature_type_mapping = pd.DataFrame({\n",
    "    'ç‰¹å¾å': valid_features,\n",
    "    'ç‰¹å¾ç±»å‹': ['åŠ¨æ€ç‰¹å¾' if f.startswith('baseline_') else 'é™æ€ç‰¹å¾' for f in valid_features],\n",
    "    'æ•°æ®ç±»å‹': [str(X_train[f].dtype) for f in valid_features],\n",
    "    'è®­ç»ƒé›†ç¼ºå¤±æ•°': [X_train[f].isna().sum() for f in valid_features],\n",
    "    'æµ‹è¯•é›†ç¼ºå¤±æ•°': [X_test[f].isna().sum() for f in valid_features]\n",
    "})\n",
    "feature_type_mapping.to_csv(\n",
    "    os.path.join(output_dir_filtered, 'feature_type_mapping.csv'), \n",
    "    index=False, encoding='utf-8-sig'\n",
    ")\n",
    "\n",
    "print(f\"\\n  âœ… æ•°æ®å·²ä¿å­˜è‡³: {output_dir_filtered}\")\n",
    "print(f\"    - X_train_filtered.csv ({X_train.shape})\")\n",
    "print(f\"    - X_test_filtered.csv ({X_test.shape})\")\n",
    "print(f\"    - y_train.csv ({len(y_train)} æ ·æœ¬)\")\n",
    "print(f\"    - y_test.csv ({len(y_test)} æ ·æœ¬)\")\n",
    "print(f\"    - filtered_features_list.txt\")\n",
    "print(f\"    - feature_type_mapping.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ 7: é˜²æ­¢æ•°æ®æ³„éœ²çš„éªŒè¯æ£€æŸ¥\n",
    "# =============================================================================\n",
    "print(f\"\\næ­¥éª¤ 7: æ•°æ®æ³„éœ²é£é™©æ£€æŸ¥...\")\n",
    "\n",
    "# 7.1 ç¡®è®¤æ²¡æœ‰ä½¿ç”¨æµ‹è¯•é›†ä¿¡æ¯è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "print(f\"\\n  ã€æ•°æ®æ³„éœ²æ£€æŸ¥æ¸…å•ã€‘\")\n",
    "print(f\"    âœ“ ç‰¹å¾é€‰æ‹©åŸºäº: è®­ç»ƒé›† (X_train)\")\n",
    "print(f\"    âœ“ ç‰¹å¾è¿‡æ»¤é¡ºåº: å…ˆåˆ’åˆ†æ•°æ®é›† â†’ å†è¿›è¡Œç‰¹å¾é€‰æ‹©\")\n",
    "print(f\"    âœ“ æ ‡å‡†åŒ–/å¡«å……: ä»…åœ¨è®­ç»ƒé›†ä¸Šfitï¼Œæµ‹è¯•é›†ä¸Štransform\")\n",
    "print(f\"    âœ“ ç‰¹å¾ä¸€è‡´æ€§: è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾åˆ—è¡¨\")\n",
    "\n",
    "# 7.2 æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡å˜é‡æ³„éœ²\n",
    "target_related_features = [f for f in valid_features if any(\n",
    "    keyword in f.lower() for keyword in ['infection', 'crs', 'icans', 'icaht', 'outcome', 'result']\n",
    ")]\n",
    "if target_related_features:\n",
    "    print(f\"\\n  âš ï¸ è­¦å‘Š: å‘ç°å¯èƒ½ä¸ç›®æ ‡å˜é‡ç›¸å…³çš„ç‰¹å¾:\")\n",
    "    for feat in target_related_features:\n",
    "        print(f\"    - {feat}\")\n",
    "    print(f\"  å»ºè®®: æ£€æŸ¥è¿™äº›ç‰¹å¾æ˜¯å¦åœ¨é¢„æµ‹æ—¶å¯ç”¨\")\n",
    "else:\n",
    "    print(f\"\\n  âœ… æœªå‘ç°æ˜æ˜¾çš„ç›®æ ‡å˜é‡æ³„éœ²ç‰¹å¾\")\n",
    "\n",
    "# =============================================================================\n",
    "# æ€»ç»“\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ç‰¹å¾è¿‡æ»¤å®Œæˆ!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nã€å…³é”®ä¿¡æ¯æ‘˜è¦ã€‘\")\n",
    "print(f\"  åŸå§‹ç‰¹å¾æ•°: {original_train_shape[1]}\")\n",
    "print(f\"  è¿‡æ»¤åç‰¹å¾æ•°: {len(valid_features)}\")\n",
    "print(f\"  ç‰¹å¾ä¿ç•™ç‡: {len(valid_features) / original_train_shape[1] * 100:.1f}%\")\n",
    "print(f\"  é™æ€ç‰¹å¾: {len([f for f in valid_features if not f.startswith('baseline_')])} ä¸ª\")\n",
    "print(f\"  åŠ¨æ€ç‰¹å¾: {len([f for f in valid_features if f.startswith('baseline_')])} ä¸ª\")\n",
    "\n",
    "print(f\"\\nã€æ•°æ®é›†ä¿¡æ¯ã€‘\")\n",
    "print(f\"  è®­ç»ƒé›†: {X_train.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)\")\n",
    "print(f\"  æµ‹è¯•é›†: {X_test.shape} (æ ·æœ¬æ•° Ã— ç‰¹å¾æ•°)\")\n",
    "print(f\"  æ ‡ç­¾åˆ†å¸ƒ: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "print(f\"\\nã€åç»­æ­¥éª¤ã€‘\")\n",
    "print(f\"  1. X_train å’Œ X_test å·²æ›´æ–°ä¸ºä»…åŒ…å« {len(valid_features)} ä¸ªç‰¹å¾\")\n",
    "print(f\"  2. preprocessor å·²è‡ªåŠ¨é‡æ–°é…ç½®ä»¥é€‚é…æ–°ç‰¹å¾\")\n",
    "print(f\"  3. åç»­çš„æ¨¡å‹è®­ç»ƒä»£ç å°†è‡ªåŠ¨ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®\")\n",
    "print(f\"  4. Pipeline ç»“æ„: preprocessor â†’ final_imputer â†’ classifier\")\n",
    "print(f\"  5. æ‰€æœ‰ä¸­é—´ç»“æœå·²ä¿å­˜è‡³: {output_dir_filtered}\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcafb9a0",
   "metadata": {},
   "source": [
    "## è¶…å‚æ•°ä¼˜åŒ–\n",
    "https://cloud.baidu.com/article/5601757\n",
    "\n",
    "### ä¸€ã€è¶…å‚æ•°ä¼˜åŒ–ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½æå‡çš„æ ¸å¿ƒå¼•æ“\n",
    "åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ï¼Œè¶…å‚æ•°ä¼˜åŒ–æ˜¯è¿æ¥ç®—æ³•ç†è®ºä¸å®é™…æ€§èƒ½çš„å…³é”®æ¡¥æ¢ã€‚ä¸åŒäºé€šè¿‡æ•°æ®è®­ç»ƒè‡ªåŠ¨è°ƒæ•´çš„æ¨¡å‹å‚æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œæƒé‡ï¼‰ï¼Œè¶…å‚æ•°éœ€åœ¨è®­ç»ƒå‰é¢„å…ˆè®¾å®šï¼Œç›´æ¥å½±å“æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ä¸æ³›åŒ–æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå†³ç­–æ ‘çš„æ·±åº¦ã€æ”¯æŒå‘é‡æœºçš„æ ¸å‡½æ•°ç±»å‹ã€ç¥ç»ç½‘ç»œçš„å­¦ä¹ ç‡ä¸å±‚æ•°ç­‰ï¼Œå‡å±äºéœ€è¦äººå·¥è°ƒä¼˜çš„è¶…å‚æ•°èŒƒç•´ã€‚\n",
    "\n",
    "è¶…å‚æ•°ä¼˜åŒ–çš„æœ¬è´¨æ˜¯é€šè¿‡ç³»ç»ŸåŒ–æ¢ç´¢è¶…å‚æ•°ç©ºé—´ï¼Œå¯»æ‰¾ä½¿æ¨¡å‹åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä¼˜çš„å‚æ•°ç»„åˆã€‚è¿™ä¸€è¿‡ç¨‹ä¸ä»…èƒ½æ˜¾è‘—æå‡æ¨¡å‹ç²¾åº¦ï¼Œè¿˜å¯é¿å…å› å‚æ•°è®¾ç½®ä¸å½“å¯¼è‡´çš„è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆé—®é¢˜ã€‚ä»¥å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œé€šè¿‡ä¼˜åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„è¶…å‚æ•°ï¼ˆå¦‚æ»¤æ³¢å™¨æ•°é‡ã€æ­¥é•¿ã€æ­£åˆ™åŒ–ç³»æ•°ï¼‰ï¼Œæ¨¡å‹åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å¯ä»75%æå‡è‡³89%ã€‚\n",
    "\n",
    "### äºŒã€ç½‘æ ¼æœç´¢ï¼šç³»ç»ŸåŒ–ç©·ä¸¾çš„ç»å…¸æ–¹æ³•\n",
    "1. åŸç†ä¸å®ç°é€»è¾‘\n",
    "ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰é€šè¿‡å®šä¹‰è¶…å‚æ•°çš„å€™é€‰å€¼é›†åˆï¼Œæ„å»ºæ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆï¼Œå¹¶é€ä¸€è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè‹¥éœ€ä¼˜åŒ–å­¦ä¹ ç‡ï¼ˆå–å€¼ä¸º{0.01, 0.001, 0.0001}ï¼‰å’Œæ­£åˆ™åŒ–ç³»æ•°ï¼ˆå–å€¼ä¸º{0.1, 0.01, 0.001}ï¼‰ï¼Œåˆ™éœ€è®­ç»ƒ3Ã—3=9ä¸ªæ¨¡å‹ï¼Œæœ€ç»ˆé€‰æ‹©éªŒè¯é›†ä¸ŠæŸå¤±æœ€å°çš„ç»„åˆã€‚\n",
    "\n",
    "2. ä¼˜ç¼ºç‚¹åˆ†æ\n",
    "- ä¼˜åŠ¿ï¼š\n",
    "    - è¦†ç›–å…¨é¢ï¼Œé¿å…é—æ¼æ½œåœ¨æœ€ä¼˜è§£\n",
    "    - å®ç°ç®€å•ï¼Œé€‚ç”¨äºå‚æ•°ç©ºé—´è¾ƒå°ï¼ˆ<10ä¸ªå‚æ•°ï¼‰æˆ–ç¦»æ•£å€¼è¾ƒå¤šçš„åœºæ™¯\n",
    "- å±€é™ï¼š\n",
    "    - è®¡ç®—æˆæœ¬éšå‚æ•°æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿ï¼ˆå¦‚10ä¸ªå‚æ•°ï¼Œæ¯ä¸ªå–3ä¸ªå€¼ï¼Œéœ€è¯„ä¼°3^10=59,049æ¬¡ï¼‰\n",
    "    - å¯¹è¿ç»­å‚æ•°éœ€é¢„å…ˆç¦»æ•£åŒ–ï¼Œå¯èƒ½é”™è¿‡æœ€ä¼˜åŒºé—´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2641383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„äº¤å‰éªŒè¯è®­ç»ƒé€»è¾‘ï¼šé›†æˆäº†å‚æ•°æœç´¢ã€å…¨é‡é‡è®­å’Œç»“æœä¿å­˜â€”â€”â€”â€”æ”¹è¿›ç‰ˆ\n",
    "# âœ… ç§»é™¤ ID è½¬æ¢é€»è¾‘: æ–°ç‰ˆæœ¬ç›´æ¥è¿”å›æ•´æ•°ç´¢å¼•ï¼Œæ— éœ€è½¬æ¢\n",
    "# âœ… ä½¿ç”¨ .iloc[indices]: ç›´æ¥ä½¿ç”¨ä½ç½®ç´¢å¼•\n",
    "# âœ… LightGBM æ”¹ç”¨ LGBMClassifier: ç¡®ä¿ sklearn æ¥å£å…¼å®¹æ€§\n",
    "\n",
    "# ä¸ºäº†å®ç°é¢„æœŸå·¥ä½œæµï¼ˆCVæ‰¾å‚æ•° -> æå–æœ€ä½³å‚æ•° -> å…¨é‡é‡è®­ -> ä¿å­˜ï¼‰ï¼Œæœ€æ ‡å‡†ä¸”ç¨³å¥çš„æ–¹æ³•æ˜¯ä½¿ç”¨ sklearn.model_selection.GridSearchCVï¼ˆç½‘æ ¼æœç´¢ï¼‰ã€‚GridSearchCV çš„ refit=True å‚æ•°å®Œç¾è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š\n",
    "# 1.è‡ªåŠ¨æ‰§è¡Œäº¤å‰éªŒè¯å¯»æ‰¾æœ€ä½³å‚æ•°ã€‚\n",
    "# 2.æ‰¾åˆ°æœ€ä½³å‚æ•°åï¼Œè‡ªåŠ¨ä½¿ç”¨è¿™äº›å‚æ•°åœ¨å®Œæ•´çš„ X_train ä¸Šé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚\n",
    "# 3.grid.best_estimator_ å°±æ˜¯æœ€ç»ˆæ¨¡å‹ã€‚\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# =============================================================================\n",
    "# 1. å®šä¹‰æ¨¡å‹åŠå…¶è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "# =============================================================================\n",
    "# æ³¨æ„ï¼šä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œè¿™é‡Œå‚æ•°èŒƒå›´è®¾ç½®è¾ƒå°ã€‚æ‚¨å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•åˆ—è¡¨ã€‚\n",
    "model_params = {\n",
    "    # 'Tabnet': {\n",
    "    #     'model': Pipeline([\n",
    "    #         ('preprocessor', preprocessor),\n",
    "    #         ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "    #         ('classifier', TabNetClassifier(\n",
    "    #             seed=42,\n",
    "    #             verbose=0  # é™é»˜æ¨¡å¼\n",
    "    #         ))\n",
    "    #     ]),\n",
    "    #     'params': {\n",
    "    #         'classifier__lr': [0.02, 0.05],\n",
    "    #         'classifier__max_epochs': [50, 100],\n",
    "    #         'classifier__batch_size': [256, 512]\n",
    "    #     }\n",
    "    # },\n",
    "    'CatBoost': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', CatBoostClassifier(\n",
    "                random_seed=42,\n",
    "                verbose=0  # é™é»˜æ¨¡å¼\n",
    "            ))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__learning_rate': [0.05, 0.1],\n",
    "            'classifier__depth': [4, 6],\n",
    "            'classifier__iterations': [100, 200]\n",
    "        }\n",
    "    },    \n",
    "    # 'GLM': {\n",
    "    #     'model': Pipeline([\n",
    "    #         ('preprocessor', preprocessor),\n",
    "    #         ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "    #         ('classifier', sm.GLM(random_state=42, max_iter=1000))\n",
    "    #     ]),\n",
    "    #     'params': {\n",
    "    #         'classifier__C': [0.1, 1, 10],\n",
    "    #         'classifier__penalty': ['l2', 'none'],\n",
    "    #         'classifier__solver': ['lbfgs', 'saga']\n",
    "    #     }\n",
    "    # },\n",
    "    'SVM': {        \n",
    "        # åŸå§‹æ­¥éª¤\n",
    "        # 'model': SVC(probability=True, random_state=42),\n",
    "        # 'params': {\n",
    "        #     'C': [0.1, 1, 10],\n",
    "        #     'kernel': ['rbf', 'linear']\n",
    "        # }\n",
    "        # æ”¹è¿›ç‰ˆæ­¥éª¤ï¼šå°†é¢„å¤„ç†å’Œæ¨¡å‹æ‰“åŒ…\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            # é’ˆå¯¹ passthrough åˆ—çš„é¢å¤–ç¼ºå¤±å€¼å¤„ç†ï¼ˆé˜²æ­¢æ¼ç½‘ä¹‹é±¼ï¼‰\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', SVC(probability=True, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            # æ³¨æ„å‚æ•°å‰ç¼€è¦åŠ ä¸Šæ­¥éª¤åç§° (classifier__)\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__kernel': ['rbf', 'linear']\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # XGBoost çš„å¾—åˆ†ä¸º 0.500000 ä¸”æ ‡å‡†å·®ä¸º 0.000000ï¼Œè¿™è¯´æ˜æ¨¡å‹å®Œå…¨æ²¡æœ‰å­¦åˆ°ä»»ä½•ä¸œè¥¿ï¼Œæˆ–è€…é¢„æµ‹ç»“æœå…¨æ˜¯åŒä¸€ä¸ªç±»åˆ«ï¼ˆä¾‹å¦‚å…¨é¢„æµ‹ä¸º0ï¼‰ã€‚\n",
    "    # è¿™é€šå¸¸æ˜¯å› ä¸ºåˆ—ååŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆå¦‚ [ã€]ã€< ç­‰ï¼‰ï¼ŒXGBoostå¯¹ç‰¹å¾åéå¸¸æ•æ„Ÿï¼Œè€Œå…¶ä»–æ¨¡å‹ï¼ˆå¦‚ SVMã€RFï¼‰åˆ™ä¸æ•æ„Ÿã€‚å½“ç‰¹å¾åä¸åˆæ³•æ—¶ï¼ŒXGBoostå¯èƒ½ä¼šé™é»˜å¤±è´¥æˆ–è¡¨ç°å¼‚å¸¸ã€‚\n",
    "    'XGBoost': {\n",
    "        'model': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "        # ä¿®æ”¹ç‚¹ï¼šç§»é™¤äº† use_label_encoder=False å’Œ eval_metric\n",
    "        # æ–°ç‰ˆ XGBoost é»˜è®¤å°±æ˜¯ binary:logisticï¼Œä¸”ä¸éœ€è¦æ‰‹åŠ¨æŒ‡å®š label_encoder\n",
    "        # ('classifier', xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42))\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            objective='binary:logistic', \n",
    "            random_state=42,\n",
    "            n_jobs=1  # å…³é”®ï¼šè®¾ä¸º1,å¼ºåˆ¶XGBoostä½¿ç”¨å•çº¿ç¨‹ï¼Œå› ä¸ºå¤–å±‚ GridSearchCV å·²ç»æ˜¯å¹¶è¡Œäº†\n",
    "            ))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__learning_rate': [0.05, 0.1],\n",
    "            'classifier__max_depth': [3, 5],\n",
    "            'classifier__n_estimators': [100, 200]\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # boosting_type='gbdt',\n",
    "    # max_depth=-1,\n",
    "    # min_data_in_leaf=20,\n",
    "    'LightGBM': {\n",
    "        'model': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "        ('classifier', lgb.LGBMClassifier(\n",
    "            objective='binary', \n",
    "            metric='binary_logloss', \n",
    "            verbose=-1, \n",
    "            random_state=42, \n",
    "            n_jobs=1  # å…³é”®ï¼šè®¾ä¸º1,å¼ºåˆ¶LightGBMä½¿ç”¨å•çº¿ç¨‹ï¼Œå› ä¸ºå¤–å±‚ GridSearchCV å·²ç»æ˜¯å¹¶è¡Œäº†\n",
    "            ))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__learning_rate': [0.05, 0.1],\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__num_leaves': [31, 50]\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "        ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__C': [0.1, 1, 10],\n",
    "            'classifier__solver': ['liblinear', 'lbfgs']\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('classifier', RandomForestClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [100, 200],\n",
    "            'classifier__max_depth': [None, 10, 20]\n",
    "        }\n",
    "    },\n",
    "    'MLP': {\n",
    "        'model': Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', MLPClassifier(max_iter=500, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': [(100,), (50, 50)],\n",
    "            'classifier__alpha': [0.0001, 0.001]\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'params': {\n",
    "            # 'classifier__n_neighbors': [3, 5, 7],     # è§†è®­ç»ƒé›†çš„æ ·æœ¬æ•°é‡è€Œå®š\n",
    "            'classifier__n_neighbors': [1, 3],\n",
    "            'classifier__weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', AdaBoostClassifier(algorithm='SAMME', random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100],\n",
    "            'classifier__learning_rate': [0.1, 0.2, 0.5, 1.0]\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 10, 20],\n",
    "            'classifier__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', GaussianNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__var_smoothing': [1e-9, 1e-8]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.05, 0.1],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. æ‰§è¡Œ GridSearchCV (CV + è‡ªåŠ¨é‡è®­)\n",
    "# =============================================================================\n",
    "# ä½¿ç”¨ç¬¬1æŠ˜è¿›è¡Œè®­ç»ƒ\n",
    "# # ========== OLDç‰ˆæœ¬ä»£ç  ==========\n",
    "# train_df, test_df, cv_folds = splitter.split(df, \"label\", \"patient_id\")\n",
    "# train_patient_ids, val_patient_ids = cv_folds[0]                          # ç¬¬1æŠ˜\n",
    "# fold1_train = train_df[train_df[\"patient_id\"].isin(train_patient_ids)]    # éœ€è¦æ‰‹åŠ¨ç­›é€‰\n",
    "# fold1_val = train_df[train_df[\"patient_id\"].isin(val_patient_ids)]\n",
    "\n",
    "# X_train = fold1_train.drop(columns=[\"label\", \"patient_id\"])\n",
    "# y_train = fold1_train[\"label\"]\n",
    "# # ========== NEWç‰ˆæœ¬ä»£ç ==========\n",
    "# # train_df, test_df, cv_folds = splitter.split(df, \"label\", \"ID\")\n",
    "# train_idx, val_idx = cv_folds_indices[0]  # ç¬¬1æŠ˜ - ç›´æ¥ç´¢å¼•!\n",
    "\n",
    "# X_train = train_df.iloc[train_idx].drop(columns=[\"Infection\", \"ID\"])\n",
    "# y_train = train_df.iloc[train_idx][\"Infection\"]\n",
    "\n",
    "\n",
    "# æ¨¡å‹è®­ç»ƒ\n",
    "# # ========== OLDç‰ˆæœ¬ä»£ç  ==========\n",
    "# train_df, test_df, cv_folds = splitter.split(df, \"label\", \"patient_id\")\n",
    "\n",
    "# for fold_idx, (train_ids, val_ids) in enumerate(cv_folds):\n",
    "#     fold_train = train_df[train_df[\"patient_id\"].isin(train_ids)] # æ‰‹åŠ¨è½¬æ¢ID â†’ è¡Œ\n",
    "#     fold_val = train_df[train_df[\"patient_id\"].isin(val_ids)]\n",
    "    \n",
    "#     # è®­ç»ƒæ¨¡å‹\n",
    "#     model.fit(fold_train[features], fold_train[\"label\"])\n",
    "\n",
    "# ========== NEWç‰ˆæœ¬ä»£ç  (2å¤„ä¿®æ”¹) ==========\n",
    "# train_df, test_df, cv_folds = splitter.split(df, \"Infection\", \"ID\")  # ä¿®æ”¹1: åˆ—å\n",
    "\n",
    "# for fold_idx, (train_idx, val_idx) in enumerate(cv_folds_indices):  # ä¿®æ”¹2: å˜é‡å\n",
    "#     # ç›´æ¥ç´¢å¼• (æ›¿æ¢ isin ä¸º iloc)\n",
    "#     fold_train = train_df.iloc[train_idx]\n",
    "#     fold_val = train_df.iloc[val_idx]\n",
    "    \n",
    "#     # è®­ç»ƒæ¨¡å‹ (æ— éœ€ä¿®æ”¹)\n",
    "#     svm = SVC(probability=True, random_state=42)\n",
    "#     # svm.fit(X_train,y_train)\n",
    "#     svm.fit(fold_train[features], fold_train[\"Infection\"])\n",
    "\n",
    "# ä½¿ç”¨é¢„å¤„ç†åçš„ X_train å’Œ y_train\n",
    "print(f\"âœ… å¼€å§‹è¶…å‚æ•°è°ƒä¼˜ä¸æœ€ç»ˆæ¨¡å‹è®­ç»ƒ...\")\n",
    "print(f\"ä½¿ç”¨æ•°æ®: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"äº¤å‰éªŒè¯ç­–ç•¥: ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds_indices ({len(cv_folds_indices)} æŠ˜)\")\n",
    "\n",
    "# å­˜å‚¨ä¼˜åŒ–åçš„ç»“æœ\n",
    "final_models = {}      # å­˜å‚¨é‡è®­åçš„æœ€ä½³æ¨¡å‹\n",
    "cv_results_data = []   # å­˜å‚¨ç»˜å›¾æ•°æ®\n",
    "\n",
    "# éå†æ¯ä¸ªæ¨¡å‹è¿›è¡Œç½‘æ ¼æœç´¢\n",
    "for model_name, config in model_params.items():\n",
    "    print(f\"\\næ­£åœ¨ä¼˜åŒ– {model_name} ...\")\n",
    "    \n",
    "    # æ ¸å¿ƒï¼šGridSearchCV\n",
    "    # cv=cv_folds_indices: ç¡®ä¿ä½¿ç”¨å’Œä¹‹å‰å®Œå…¨ä¸€è‡´çš„åˆ‡åˆ†\n",
    "    # refit=True (é»˜è®¤): æ‰¾åˆ°æœ€ä½³å‚æ•°åï¼Œè‡ªåŠ¨åœ¨æ•´ä¸ª X_train ä¸Šé‡è®­\n",
    "    gs = GridSearchCV(\n",
    "        estimator=config['model'],\n",
    "        param_grid=config['params'],\n",
    "        cv=cv_folds_indices,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,  # å¹¶è¡Œè®¡ç®—ï¼Œå¯åŠ é€Ÿè®­ç»ƒï¼ˆä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒï¼‰\n",
    "        refit=True,\n",
    "        verbose=1,\n",
    "        error_score='raise' # å¦‚æœå‡ºé”™ï¼ŒæŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯é™é»˜å¤±è´¥\n",
    "    )\n",
    "    \n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # 1. ä¿å­˜æœ€ä½³æ¨¡å‹ (å·²åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒå¥½)\n",
    "    final_models[model_name] = gs.best_estimator_\n",
    "    \n",
    "    # 2. è®°å½•ç»“æœ\n",
    "    best_score = gs.best_score_\n",
    "    best_params = gs.best_params_\n",
    "    best_std = gs.cv_results_['std_test_score'][gs.best_index_]\n",
    "    \n",
    "    print(f\"  -> æœ€ä½³ AUC: {best_score:.4f} (Â±{best_std:.4f})\")\n",
    "    print(f\"  -> æœ€ä½³å‚æ•°: {best_params}\")\n",
    "    \n",
    "    # æ”¶é›†æ¯ä¸€æŠ˜çš„è¯¦ç»†åˆ†æ•°ç”¨äºç»˜å›¾ (æå–æœ€ä½³å‚æ•°å¯¹åº”çš„é‚£äº›æŠ˜çš„åˆ†æ•°)\n",
    "    # split0_test_score, split1_test_score ...\n",
    "    fold_scores = []\n",
    "    for i in range(len(cv_folds_indices)):\n",
    "        fold_scores.append(gs.cv_results_[f'split{i}_test_score'][gs.best_index_])\n",
    "        \n",
    "    cv_results_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean AUC': best_score,\n",
    "        'Std AUC': best_std,\n",
    "        'Scores': fold_scores,\n",
    "        'Best Params': str(best_params)\n",
    "    })\n",
    "    \n",
    "    print(f\"Best AUC for {model_name}: {best_score:.4f} (+/- {best_std:.4f})\")\n",
    "    print(f\"Best Params: {best_params}\\n\")\n",
    "# =============================================================================\n",
    "# 3. ç»“æœæ±‡æ€»ä¸ä¿å­˜\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"æœ€ç»ˆä¼˜åŒ–ç»“æœæ±‡æ€»\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# å°†ç»“æœè½¬æ¢ä¸º DataFrame\n",
    "df_results = pd.DataFrame(cv_results_data).sort_values('Mean AUC', ascending=False)\n",
    "print(df_results[['Model', 'Mean AUC', 'Std AUC', 'Best Params']].to_string(index=False))\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ° CSV\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "df_results.drop(columns=['Scores']).to_csv(os.path.join(output_dir, 'cv_optimization_results.csv'), index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. å¯è§†åŒ–\n",
    "# =============================================================================\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "models_list = df_results['Model'].tolist()\n",
    "means = df_results['Mean AUC'].tolist()\n",
    "stds = df_results['Std AUC'].tolist()\n",
    "x_pos = np.arange(len(models_list))\n",
    "\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.7, \n",
    "       capsize=10, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b'])\n",
    "ax.set_ylabel('AUC Score (CV Optimized)', fontsize=12)\n",
    "ax.set_title('Best Cross-Validation Performance after Tuning', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'cv_optimized_barplot.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 5. å°†æœ€ä½³æ¨¡å‹èµ‹å€¼ç»™å…¨å±€å˜é‡\n",
    "# =============================================================================\n",
    "# è¿™æ ·åç»­å•å…ƒæ ¼ç›´æ¥ä½¿ç”¨ svm, rf ç­‰å˜é‡æ—¶ï¼Œç”¨çš„å°±æ˜¯è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹\n",
    "svm = final_models['SVM']\n",
    "xgb_model = final_models['XGBoost']\n",
    "lgb_model = final_models['LightGBM']\n",
    "log_reg = final_models['Logistic Regression']\n",
    "rf = final_models['Random Forest']\n",
    "mlp = final_models['MLP']\n",
    "knn = final_models['KNN']\n",
    "ada = final_models['AdaBoost']\n",
    "dt = final_models['Decision Tree']\n",
    "nb = final_models['Naive Bayes']\n",
    "\n",
    "print(\"\\næ‰€æœ‰æ¨¡å‹å·²ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒå®Œæˆï¼\")\n",
    "print(\"å…¨å±€å˜é‡ (svm, rf, xgb_model...) å·²æ›´æ–°ä¸ºæœ€ä½³æ¨¡å‹ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b1ba35",
   "metadata": {},
   "source": [
    "### å››ã€è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½å¯¼å‘çš„å…ˆè¿›æ–¹æ³•\n",
    "1. åŸç†ä¸æ ¸å¿ƒé€»è¾‘\n",
    "è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰é€šè¿‡æ„å»ºç›®æ ‡å‡½æ•°ï¼ˆå¦‚æ¨¡å‹å‡†ç¡®ç‡ï¼‰çš„æ¦‚ç‡ä»£ç†æ¨¡å‹ï¼ˆé€šå¸¸ä¸ºé«˜æ–¯è¿‡ç¨‹ï¼‰ï¼Œç»“åˆé‡‡é›†å‡½æ•°ï¼ˆå¦‚Expected Improvementï¼‰åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªè¯„ä¼°ç‚¹ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºâ€œè®°å¿†æ€§â€ï¼šæ¯æ¬¡è¯„ä¼°åæ›´æ–°ä»£ç†æ¨¡å‹ï¼Œå¼•å¯¼åç»­æœç´¢å‘é«˜æ½œåŠ›åŒºåŸŸé›†ä¸­ã€‚\n",
    "\n",
    "2. ä¼˜ç¼ºç‚¹åˆ†æ\n",
    "- ä¼˜åŠ¿ï¼š\n",
    "    - è®¡ç®—æ•ˆç‡æé«˜ï¼Œå°¤å…¶é€‚ç”¨äºè¯„ä¼°æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ï¼ˆå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰\n",
    "    - è‡ªåŠ¨å¹³è¡¡æ¢ç´¢ï¼ˆæ–°åŒºåŸŸï¼‰ä¸åˆ©ç”¨ï¼ˆå·²çŸ¥é«˜æ½œåŠ›åŒºåŸŸï¼‰\n",
    "- å±€é™ï¼š\n",
    "    - ä»£ç†æ¨¡å‹æ„å»ºå¤æ‚ï¼Œéœ€è°ƒæ•´è¶…å‚æ•°ï¼ˆå¦‚é«˜æ–¯è¿‡ç¨‹çš„æ ¸å‡½æ•°ï¼‰\n",
    "    - åˆå§‹é˜¶æ®µå¯èƒ½å› ä»£ç†æ¨¡å‹ä¸å‡†ç¡®è€Œé™·å…¥å±€éƒ¨æœ€ä¼˜\n",
    "\n",
    "\n",
    "3. æ–¹æ³•ï¼šé€‚ç”¨åœºæ™¯â€”â€”è®¡ç®—æ•ˆç‡â€”â€”å®ç°å¤æ‚åº¦\n",
    "- ç½‘æ ¼æœç´¢ï¼šä½ç»´ç¦»æ•£å‚æ•°ç©ºé—´â€”â€”ä½â€”â€”ä½\n",
    "- éšæœºæœç´¢ï¼šä¸­é«˜ç»´å‚æ•°ç©ºé—´ï¼Œè®¡ç®—èµ„æºæœ‰é™â€”â€”ä¸­â€”â€”ä¸­\n",
    "- è´å¶æ–¯ä¼˜åŒ–ï¼šé«˜ç»´è¿ç»­å‚æ•°ç©ºé—´ï¼Œè¯„ä¼°æˆæœ¬é«˜â€”â€”é«˜â€”â€”é«˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœæ•°æ®é›†ä¸å¹³è¡¡ä¸¥é‡ï¼Œå¯ä»¥è€ƒè™‘ï¼š\n",
    "# ä½¿ç”¨ SMOTE è¿›è¡Œè¿‡é‡‡æ ·\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# print(f\"å¹³è¡¡åçš„ç±»åˆ«åˆ†å¸ƒ:\\n{pd.Series(y_train_balanced).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# è¶…å‚æ•°ä¼˜åŒ–æ–¹å¼2â€”â€”â€”â€”è´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "# å‚è€ƒï¼šhttps://mp.weixin.qq.com/s/lvkn2PKv8AobZAsdpF9xjg\n",
    "# ä¸€ã€ç¯å¢ƒå‡†å¤‡ä¸å¯è§†åŒ–é…ç½®\n",
    "# é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„åº“ï¼Œå¹¶è®¾ç½® Nature é£æ ¼é…è‰² ä¸ç»˜å›¾å‚æ•°ï¼Œä»¥ä¿è¯è¾“å‡ºç»“æœåœ¨å­¦æœ¯è®ºæ–‡ä¸å±•ç¤ºä¸­å…·æœ‰é¡¶åˆŠæ°´å‡†ã€‚\n",
    "# è¿™æ ·é…ç½®åï¼Œæ¨¡å‹ç»“æœå›¾ï¼ˆå¦‚ROCæ›²çº¿ã€æ··æ·†çŸ©é˜µï¼‰ä¼šæ›´ç¬¦åˆå›½é™…æœŸåˆŠçš„å±•ç¤ºé£æ ¼ã€‚\n",
    "# =============================================================================\n",
    "# pip install scikit-optimize-0.10.2(pyaml-25.7.0)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import numpy as np\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import shap\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®é¡¶åˆŠé…è‰²ä¸å­—ä½“\n",
    "nature_colors = ['#E64B35', '#4DBBD5', '#00A087', '#3C5488', '#F39B7F']\n",
    "sns.set_palette(nature_colors)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--', 'axes.edgecolor': '0.3'})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "plt.rcParams['savefig.format'] = 'jpeg'\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "# å‚è€ƒï¼šhttps://cloud.baidu.com/article/5601757\n",
    "# =============================================================================\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\n",
    "search_spaces = {\n",
    "    'C': Real(0.1, 10, prior='log-uniform'),\n",
    "    'gamma': Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "    'kernel': Categorical(['rbf', 'poly']),\n",
    "    'degree': Integer(2, 5)\n",
    "}\n",
    "# åˆ›å»ºè´å¶æ–¯ä¼˜åŒ–å¯¹è±¡ï¼ˆè¯„ä¼°50æ¬¡ï¼‰\n",
    "bayes_search = BayesSearchCV(\n",
    "    SVC(), search_spaces, n_iter=50, cv=5, scoring='accuracy'\n",
    ")\n",
    "bayes_search.fit(X_train, y_train)\n",
    "print(\"æœ€ä¼˜å‚æ•°:\", bayes_search.best_params_)\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“‚ äºŒã€æ•°æ®å‡†å¤‡ä¸åˆ’åˆ†\n",
    "# è¯»å–æ ·æœ¬æ•°æ®ï¼Œæå–ç‰¹å¾å’Œæ ‡ç­¾ï¼Œå¹¶åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ã€‚åŒæ—¶ï¼Œå°†æ•°æ®é›†å¯¼å‡ºï¼Œä»¥ä¾¿åç»­åˆ†æã€‚\n",
    "# æ­¤æ­¥éª¤ä¿è¯æ•°æ®åˆ’åˆ†çš„ç‹¬ç«‹æ€§ä¸å¯è¿½æº¯æ€§ï¼Œé¿å…æ•°æ®ç»“æœä¸å¯å¤ç°ã€‚\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤1: ä½¿ç”¨é¡¹ç›®é¢„å¤„ç†åçš„æ•°æ®\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ° CSV\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output/bayes_cv_results/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# ä½¿ç”¨å·²ç»å‡†å¤‡å¥½çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ•°æ®ï¼ˆæ¥è‡ªå•å…ƒæ ¼8çš„æ•°æ®é¢„å¤„ç†ï¼‰ï¼Œç¡®ä¿ä¸å•å…ƒæ ¼12ä¿æŒä¸€è‡´çš„æ•°æ®æ ¼å¼å’Œé¢„å¤„ç†ç®¡é“\n",
    "# X_train, y_train: è®­ç»ƒé›†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "# X_test, y_test: æµ‹è¯•é›†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "# preprocessor: æ•°æ®é¢„å¤„ç†ç®¡é“\n",
    "# cv_folds_indices: 3æŠ˜äº¤å‰éªŒè¯ç´¢å¼•\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "print(f\"äº¤å‰éªŒè¯ç­–ç•¥: ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds_indices ({len(cv_folds_indices)} æŠ˜)\")\n",
    "print(f\"æ•°æ®é¢„å¤„ç†: ä½¿ç”¨ preprocessor ç®¡é“\")\n",
    "\n",
    "# å¯¼å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¾›åç»­åˆ†æ\n",
    "train_data_export = X_train.copy()\n",
    "train_data_export['label'] = y_train\n",
    "train_data_export.to_csv(output_dir + 'train_dataset_bayes.csv', index=False)\n",
    "\n",
    "test_data_export = X_test.copy()\n",
    "test_data_export['label'] = y_test\n",
    "test_data_export.to_csv(output_dir + 'test_dataset_bayes.csv', index=False)\n",
    "print(\"\\næ•°æ®å¯¼å‡ºå®Œæˆ: train_dataset_bayes.csv, test_dataset_bayes.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ” ä¸‰ã€å®šä¹‰æ¨¡å‹åŠå…¶è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´\n",
    "# ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬åœ¨éšæœºæ£®æ—çš„å‚æ•°ç©ºé—´ä¸­è®¾ç½®äº†è¾ƒä¸ºä¿å®ˆçš„èŒƒå›´ï¼Œå¹¶ä½¿ç”¨ BayesSearchCV è‡ªåŠ¨æœç´¢æœ€ä½³ç»„åˆã€‚\n",
    "# è¿™é‡Œç‰¹åˆ«åŠ å…¥äº† max_samples ä¸ ccp_alphaï¼ˆå¤æ‚åº¦å‰ªæï¼‰ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚\n",
    "# å‚è€ƒå•å…ƒæ ¼12çš„model_paramsç»“æ„ï¼Œä¸ºå¤šä¸ªæ¨¡å‹å®šä¹‰è´å¶æ–¯ä¼˜åŒ–çš„æœç´¢ç©ºé—´\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤2: å®šä¹‰æ¨¡å‹åŠå…¶è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å®šä¹‰æ‰€æœ‰æ¨¡å‹çš„å‚æ•°ç©ºé—´ï¼ˆä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–ï¼‰\n",
    "model_params_bayes = {\n",
    "    'SVM': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', SVC(probability=True, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__C': Real(0.1, 10, prior='log-uniform'),\n",
    "            'classifier__kernel': Categorical(['rbf', 'linear'])\n",
    "        }\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', xgb.XGBClassifier(\n",
    "                objective='binary:logistic', \n",
    "                random_state=42,\n",
    "                n_jobs=1\n",
    "            ))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "            'classifier__max_depth': Integer(3, 10),\n",
    "            'classifier__n_estimators': Integer(50, 300)\n",
    "        }\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', lgb.LGBMClassifier(\n",
    "                objective='binary', \n",
    "                metric='binary_logloss', \n",
    "                verbose=-1, \n",
    "                random_state=42, \n",
    "                n_jobs=1\n",
    "            ))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "            'classifier__n_estimators': Integer(50, 300),\n",
    "            'classifier__num_leaves': Integer(20, 100)\n",
    "        }\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__C': Real(0.01, 100, prior='log-uniform'),\n",
    "            'classifier__solver': Categorical(['liblinear', 'lbfgs'])\n",
    "        }\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': Integer(50, 200),     \n",
    "            'classifier__max_depth': Integer(3, 15),          \n",
    "            'classifier__min_samples_split': Integer(10, 30), \n",
    "            'classifier__min_samples_leaf': Integer(5, 20),   \n",
    "            'classifier__max_features': Categorical(['sqrt', 'log2']),\n",
    "            'classifier__bootstrap': Categorical([True]),     \n",
    "            'classifier__max_samples': Real(0.6, 0.9),        \n",
    "            'classifier__ccp_alpha': Real(0.0, 0.01)\n",
    "        }\n",
    "    },\n",
    "    'MLP': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('classifier', MLPClassifier(max_iter=500, random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__hidden_layer_sizes': Categorical([    \n",
    "                                            (50,), (100,), (150,),      # å•å±‚ç½‘ç»œ\n",
    "                                            (50, 50), (100, 50),        # åŒå±‚ç½‘ç»œ\n",
    "                                            (100, 50, 25)]),            # ä¸‰å±‚ç½‘ç»œ\n",
    "            'classifier__alpha': Real(0.0001, 0.01, prior='log-uniform'),\n",
    "            'classifier__learning_rate_init': Real(0.001, 0.01, prior='log-uniform')\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', KNeighborsClassifier())\n",
    "        ]),\n",
    "        'params': {\n",
    "            # äº¤å‰éªŒè¯çš„æŸä¸ª fold ä¸­æ ·æœ¬æ•°å¤ªå°‘ï¼ˆåªæœ‰5ä¸ªæ ·æœ¬ï¼‰ï¼Œn_neighbors=8 è¶…è¿‡äº†è¯¥ fold çš„æ ·æœ¬æ•°é‡\n",
    "            'classifier__n_neighbors': Integer(1, 3),   \n",
    "            'classifier__weights': Categorical(['uniform', 'distance']),\n",
    "            'classifier__metric': Categorical(['euclidean', 'manhattan'])\n",
    "        }\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', AdaBoostClassifier(algorithm='SAMME', random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': Integer(50, 200),\n",
    "            'classifier__learning_rate': Real(0.01, 2.0, prior='log-uniform')\n",
    "        }\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__max_depth': Integer(3, 30),\n",
    "            'classifier__min_samples_split': Integer(2, 20),\n",
    "            'classifier__min_samples_leaf': Integer(1, 10)\n",
    "        }\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', GaussianNB())\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__var_smoothing': Real(1e-10, 1e-7, prior='log-uniform')\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)), \n",
    "            ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': Integer(50, 300),\n",
    "            'classifier__learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "            'classifier__max_depth': Integer(3, 10),\n",
    "            'classifier__subsample': Real(0.6, 1.0)\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"å·²å®šä¹‰ {len(model_params_bayes)} ä¸ªæ¨¡å‹çš„è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´\")\n",
    "print(f\"æ¨¡å‹åˆ—è¡¨: {list(model_params_bayes.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# âš¡ å››ã€BayesSearchCVè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹ï¼‰\n",
    "# é€šè¿‡è´å¶æ–¯ä¼˜åŒ–+äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬åœ¨åˆç†çš„è¿­ä»£æ¬¡æ•°å†…æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆï¼Œå¹¶ä¿å­˜ä¼˜åŒ–è¿‡ç¨‹ã€‚\n",
    "# è¿™ä¸€æ­¥è®©éšæœºæ£®æ—åœ¨å‚æ•°è°ƒä¼˜è¿‡ç¨‹ä¸­é¿å…äº†ç›²ç›®æœç´¢ï¼Œæå‡äº†æ•ˆç‡å’Œæ¨¡å‹çš„é²æ£’æ€§ã€‚\n",
    "# ä½¿ç”¨ä¸å•å…ƒæ ¼12ä¸€è‡´çš„äº¤å‰éªŒè¯ç­–ç•¥ï¼ˆcv_folds_indicesï¼‰\n",
    "# refit=True ç¡®ä¿æ‰¾åˆ°æœ€ä½³å‚æ•°åè‡ªåŠ¨åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒ\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤3: æ‰§è¡Œè´å¶æ–¯è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"âœ… å¼€å§‹è¶…å‚æ•°è°ƒä¼˜ä¸æœ€ç»ˆæ¨¡å‹è®­ç»ƒ...\")\n",
    "print(f\"ä½¿ç”¨æ•°æ®: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"äº¤å‰éªŒè¯ç­–ç•¥: ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds_indices ({len(cv_folds_indices)} æŠ˜)\")\n",
    "print(f\"ä¼˜åŒ–æ–¹æ³•: BayesSearchCV (n_iter=10)\")\n",
    "\n",
    "# å­˜å‚¨ä¼˜åŒ–åçš„ç»“æœ\n",
    "final_models_bayes = {}      # å­˜å‚¨é‡è®­åçš„æœ€ä½³æ¨¡å‹\n",
    "cv_results_data_bayes = []   # å­˜å‚¨ç»“æœæ•°æ®\n",
    "optimization_details = []    # å­˜å‚¨è¯¦ç»†ä¼˜åŒ–ä¿¡æ¯\n",
    "\n",
    "# éå†æ¯ä¸ªæ¨¡å‹è¿›è¡Œè´å¶æ–¯ä¼˜åŒ–\n",
    "for idx, (model_name, config) in enumerate(model_params_bayes.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{idx}/{len(model_params_bayes)}] æ­£åœ¨ä¼˜åŒ– {model_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # æ‰§è¡ŒBayesSearchCV\n",
    "        bayes_search = BayesSearchCV(\n",
    "            estimator=config['model'],\n",
    "            search_spaces=config['params'],\n",
    "            n_iter=10,  # è´å¶æ–¯ä¼˜åŒ–è¿­ä»£æ¬¡æ•°\n",
    "            cv=cv_folds_indices,  # ä½¿ç”¨ä¸å•å…ƒæ ¼12ä¸€è‡´çš„äº¤å‰éªŒè¯\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0,  # å‡å°‘è¾“å‡º\n",
    "            refit=True  # è‡ªåŠ¨ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒ\n",
    "        )\n",
    "        \n",
    "        print(f\"  â³ å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...\")\n",
    "        bayes_search.fit(X_train, y_train)\n",
    "        \n",
    "        # 1. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆå·²åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒå¥½ï¼‰\n",
    "        final_models_bayes[model_name] = bayes_search.best_estimator_\n",
    "        \n",
    "        # 2. è®°å½•ç»“æœ\n",
    "        best_score = bayes_search.best_score_\n",
    "        best_params = bayes_search.best_params_\n",
    "        best_std = bayes_search.cv_results_['std_test_score'][bayes_search.best_index_]\n",
    "        \n",
    "        print(f\"  âœ… ä¼˜åŒ–å®Œæˆ!\")\n",
    "        print(f\"  -> æœ€ä½³ AUC: {best_score:.4f} (Â±{best_std:.4f})\")\n",
    "        print(f\"  -> æœ€ä½³å‚æ•°: {best_params}\")\n",
    "        \n",
    "        # æ”¶é›†æ¯ä¸€æŠ˜çš„è¯¦ç»†åˆ†æ•°\n",
    "        fold_scores = []\n",
    "        for i in range(len(cv_folds_indices)):\n",
    "            fold_scores.append(bayes_search.cv_results_[f'split{i}_test_score'][bayes_search.best_index_])\n",
    "        \n",
    "        cv_results_data_bayes.append({\n",
    "            'Model': model_name,\n",
    "            'Mean AUC': best_score,\n",
    "            'Std AUC': best_std,\n",
    "            'Scores': fold_scores,\n",
    "            'Best Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        optimization_details.append({\n",
    "            'Model': model_name,\n",
    "            'Status': 'Success',\n",
    "            'Best AUC': f\"{best_score:.4f}\",\n",
    "            'Std': f\"{best_std:.4f}\",\n",
    "            'Best Params': str(best_params)\n",
    "        })\n",
    "        \n",
    "        # ä¿å­˜å•ä¸ªæ¨¡å‹\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}_bayes_optimized.pkl\"\n",
    "        output_dir_bayes = os.path.join(output_dir, 'models')\n",
    "        if not os.path.exists(output_dir_bayes):\n",
    "            os.makedirs(output_dir_bayes)\n",
    "        joblib.dump(bayes_search.best_estimator_, os.path.join(output_dir_bayes, model_filename))\n",
    "        print(f\"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {model_name} ä¼˜åŒ–å¤±è´¥: {str(e)}\")\n",
    "        optimization_details.append({\n",
    "            'Model': model_name,\n",
    "            'Status': 'Failed',\n",
    "            'Best AUC': 'N/A',\n",
    "            'Std': 'N/A',\n",
    "            'Best Params': f'Error: {str(e)}'\n",
    "        })\n",
    "        continue\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š äº”ã€æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
    "# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¸Šåˆ†åˆ«è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ã€ROC AUCç­‰æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒæ—¶è¿›è¡Œè¿‡æ‹Ÿåˆåˆ†æã€‚\n",
    "# é€šè¿‡å¯¹æ¯”è®­ç»ƒé›†ä¸æµ‹è¯•é›†è¡¨ç°ï¼Œå¯ä»¥ç›´è§‚åˆ¤æ–­æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆã€‚\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤4: æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for model_name, model in final_models_bayes.items():\n",
    "    print(f\"\\nè¯„ä¼° {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # è®­ç»ƒé›†è¯„ä¼°\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "        \n",
    "        # æµ‹è¯•é›†è¯„ä¼°\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "        \n",
    "        performance_results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_accuracy,\n",
    "            'Train_Precision': train_precision,\n",
    "            'Train_Recall': train_recall,\n",
    "            'Train_F1': train_f1,\n",
    "            'Train_ROC_AUC': train_roc_auc,\n",
    "            'Test_Accuracy': test_accuracy,\n",
    "            'Test_Precision': test_precision,\n",
    "            'Test_Recall': test_recall,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_ROC_AUC': test_roc_auc,\n",
    "            'AUC_Diff': abs(train_roc_auc - test_roc_auc)\n",
    "        })\n",
    "        \n",
    "        print(f\"  è®­ç»ƒé›† - Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f}, AUC: {train_roc_auc:.4f}\")\n",
    "        print(f\"  æµ‹è¯•é›† - Acc: {test_accuracy:.4f}, Prec: {test_precision:.4f}, Rec: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_roc_auc:.4f}\")\n",
    "        print(f\"  è¿‡æ‹Ÿåˆåˆ†æ: AUCå·®å¼‚ = {abs(train_roc_auc - test_roc_auc):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {model_name} è¯„ä¼°å¤±è´¥: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ˆ å…­ã€ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–ï¼šROCä¸æ··æ·†çŸ©é˜µ\n",
    "# ç»˜åˆ¶è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„ ROCæ›²çº¿ï¼Œå¹¶è¾“å‡ºæ··æ·†çŸ©é˜µï¼Œå…¨é¢å±•ç¤ºæ¨¡å‹åˆ†ç±»æ•ˆæœã€‚\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤5: ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å°†ç»“æœè½¬æ¢ä¸º DataFrame\n",
    "df_cv_results = pd.DataFrame(cv_results_data_bayes).sort_values('Mean AUC', ascending=False)\n",
    "df_performance = pd.DataFrame(performance_results).sort_values('Test_ROC_AUC', ascending=False)\n",
    "\n",
    "print(\"\\näº¤å‰éªŒè¯ä¼˜åŒ–ç»“æœæ±‡æ€»:\")\n",
    "print(df_cv_results[['Model', 'Mean AUC', 'Std AUC']].to_string(index=False))\n",
    "\n",
    "print(\"\\næµ‹è¯•é›†æ€§èƒ½æ±‡æ€»:\")\n",
    "print(df_performance[['Model', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC', 'AUC_Diff']].to_string(index=False))\n",
    "\n",
    "df_cv_results.drop(columns=['Scores']).to_csv(os.path.join(output_dir, 'bayes_cv_optimization_results.csv'), index=False)\n",
    "df_performance.to_csv(os.path.join(output_dir, 'bayes_performance_results.csv'), index=False)\n",
    "\n",
    "print(f\"\\nç»“æœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "\n",
    "# å¯è§†åŒ–ï¼šäº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "models_list = df_cv_results['Model'].tolist()\n",
    "means = df_cv_results['Mean AUC'].tolist()\n",
    "stds = df_cv_results['Std AUC'].tolist()\n",
    "x_pos = np.arange(len(models_list))\n",
    "\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.7, \n",
    "       capsize=10, color=nature_colors[:len(models_list)])\n",
    "ax.set_ylabel('AUC Score (CV Optimized)', fontsize=12)\n",
    "ax.set_title('Cross-Validation Performance after Bayesian Optimization', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'bayes_cv_optimized_barplot.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: bayes_cv_optimized_barplot.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# ä¸ƒã€å°†æœ€ä½³æ¨¡å‹èµ‹å€¼ç»™å…¨å±€å˜é‡ï¼ˆä¿æŒä¸å•å…ƒæ ¼12çš„ä¸€è‡´æ€§ï¼‰\n",
    "# è¿™æ ·åç»­å•å…ƒæ ¼ç›´æ¥ä½¿ç”¨ svm, rf ç­‰å˜é‡æ—¶ï¼Œç”¨çš„å°±æ˜¯è°ƒä¼˜åçš„æœ€ä½³æ¨¡å‹\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤6: æ›´æ–°å…¨å±€å˜é‡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å°†ä¼˜åŒ–åçš„æ¨¡å‹èµ‹å€¼ç»™å…¨å±€å˜é‡\n",
    "if 'SVM' in final_models_bayes:\n",
    "    svm = final_models_bayes['SVM']\n",
    "    print(\"âœ… svm å·²æ›´æ–°\")\n",
    "if 'XGBoost' in final_models_bayes:\n",
    "    xgb_model = final_models_bayes['XGBoost']\n",
    "    print(\"âœ… xgb_model å·²æ›´æ–°\")\n",
    "if 'LightGBM' in final_models_bayes:\n",
    "    lgb_model = final_models_bayes['LightGBM']\n",
    "    print(\"âœ… lgb_model å·²æ›´æ–°\")\n",
    "if 'Logistic Regression' in final_models_bayes:\n",
    "    log_reg = final_models_bayes['Logistic Regression']\n",
    "    print(\"âœ… log_reg å·²æ›´æ–°\")\n",
    "if 'Random Forest' in final_models_bayes:\n",
    "    rf = final_models_bayes['Random Forest']\n",
    "    print(\"âœ… rf å·²æ›´æ–°\")\n",
    "if 'MLP' in final_models_bayes:\n",
    "    mlp = final_models_bayes['MLP']\n",
    "    print(\"âœ… mlp å·²æ›´æ–°\")\n",
    "if 'KNN' in final_models_bayes:\n",
    "    knn = final_models_bayes['KNN']\n",
    "    print(\"âœ… knn å·²æ›´æ–°\")\n",
    "if 'AdaBoost' in final_models_bayes:\n",
    "    ada = final_models_bayes['AdaBoost']\n",
    "    print(\"âœ… ada å·²æ›´æ–°\")\n",
    "if 'Decision Tree' in final_models_bayes:\n",
    "    dt = final_models_bayes['Decision Tree']\n",
    "    print(\"âœ… dt å·²æ›´æ–°\")\n",
    "if 'Naive Bayes' in final_models_bayes:\n",
    "    nb = final_models_bayes['Naive Bayes']\n",
    "    print(\"âœ… nb å·²æ›´æ–°\")\n",
    "if 'Gradient Boosting' in final_models_bayes:\n",
    "    # æ³¨æ„ï¼šGradient Boosting é€šå¸¸ä¸ä½¿ç”¨ç¼©å†™ï¼Œä½†ä¸ºäº†ä¸€è‡´æ€§ï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªå˜é‡\n",
    "    gb = final_models_bayes['Gradient Boosting']\n",
    "    print(\"âœ… gb å·²æ›´æ–°\")\n",
    "\n",
    "# ä¸ºäº†ä¸å•å…ƒæ ¼12ä¿æŒå®Œå…¨ä¸€è‡´ï¼Œä¹Ÿåˆ›å»ºfinal_modelså­—å…¸\n",
    "final_models = final_models_bayes.copy()\n",
    "print(\"\\nâœ… final_models å­—å…¸å·²æ›´æ–°ï¼ŒåŒ…å«æ‰€æœ‰ä¼˜åŒ–åçš„æ¨¡å‹\")\n",
    "\n",
    "# =============================================================================\n",
    "# å…«ã€æ€»ç»“æŠ¥å‘Š\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"å®Œæˆ! è´å¶æ–¯ä¼˜åŒ–å¤šæ¨¡å‹è®­ç»ƒæ€»ç»“\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n1. æˆåŠŸä¼˜åŒ–æ¨¡å‹æ•°é‡: {len(final_models_bayes)}/{len(model_params_bayes)}\")\n",
    "print(f\"2. æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰æµ‹è¯•é›†AUCæ’åºï¼‰:\")\n",
    "if len(df_performance) > 0:\n",
    "    for idx, row in df_performance.head(5).iterrows():\n",
    "        print(f\"   {row['Model']}: Test AUC = {row['Test_ROC_AUC']:.4f}\")\n",
    "print(f\"\\n3. è¾“å‡ºæ–‡ä»¶:\")\n",
    "print(f\"   - è®­ç»ƒæ•°æ®: train_dataset_bayes.csv\")\n",
    "print(f\"   - æµ‹è¯•æ•°æ®: test_dataset_bayes.csv\")\n",
    "print(f\"   - äº¤å‰éªŒè¯ç»“æœ: {output_dir}/bayes_cv_optimization_results.csv\")\n",
    "print(f\"   - æ€§èƒ½è¯„ä¼°ç»“æœ: {output_dir}/bayes_performance_results.csv\")\n",
    "print(f\"   - å¯è§†åŒ–å›¾è¡¨: {output_dir}/bayes_cv_optimized_barplot.png\")\n",
    "print(f\"\\n4. å…¨å±€å˜é‡å·²æ›´æ–°ï¼Œå¯ä»¥ç›´æ¥ç”¨äºåç»­ROCåˆ†æå’ŒSHAPåˆ†æ\")\n",
    "print(f\"   - final_models: åŒ…å«æ‰€æœ‰ä¼˜åŒ–åçš„æ¨¡å‹\")\n",
    "print(f\"   - å•ç‹¬å˜é‡: svm, xgb_model, lgb_model, log_reg, rf, mlp, knn, ada, dt, nb, gb\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "\n",
    "# ğŸŒ² ä¸ƒã€ç‰¹å¾é‡è¦æ€§ä¸è§£é‡Šæ€§åˆ†æï¼ˆSHAPï¼‰\n",
    "# ä¸ºäº†ç†è§£æ¨¡å‹å†³ç­–ä¾æ®ï¼Œæˆ‘ä»¬ä¸ä»…è®¡ç®—äº†éšæœºæ£®æ—è‡ªå¸¦çš„é‡è¦æ€§ï¼Œè¿˜ç»“åˆ SHAPå€¼ æä¾›æ›´ç»†è‡´çš„è§£é‡Šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# è¶…å‚æ•°ä¼˜åŒ–æ–¹å¼3â€”â€”â€”â€”Optunaè¶…å‚æ•°ä¼˜åŒ–å¤šæ¨¡å‹è®­ç»ƒä¸è¯„ä¼°\n",
    "# ä½¿ç”¨ Optuna æ›¿ä»£ BayesSearchCVï¼Œå®ç°æ›´çµæ´»çš„è¶…å‚æ•°ä¼˜åŒ–\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import shap\n",
    "import warnings\n",
    "import os\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®é¡¶åˆŠé…è‰²ä¸å­—ä½“\n",
    "nature_colors = ['#E64B35', '#4DBBD5', '#00A087', '#3C5488', '#F39B7F']\n",
    "sns.set_palette(nature_colors)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--', 'axes.edgecolor': '0.3'})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "plt.rcParams['savefig.format'] = 'jpeg'\n",
    "\n",
    "# ç¦ç”¨Optunaçš„æ—¥å¿—è¾“å‡º\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "'''\n",
    "# =============================================================================\n",
    "# Optunaè¶…å‚æ•°ä¼˜åŒ–XGBoostæ¨¡å‹\n",
    "# å‚è€ƒï¼šhttps://zhuanlan.zhihu.com/p/1964684051261666199\n",
    "# =============================================================================\n",
    "\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# åŠ è½½ç¤ºä¾‹æ•°æ®é›†\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "def objective(trial):\n",
    "    # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "    }\n",
    "\n",
    "    # åˆ›å»ºå¹¶è¯„ä¼°æ¨¡å‹\n",
    "    model = xgb.XGBClassifier(**params, random_state=42)\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "# åˆ›å»º study å¯¹è±¡å¹¶å¼€å§‹ä¼˜åŒ–\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"æœ€ä½³å‚æ•°:\", study.best_params)\n",
    "print(\"æœ€ä½³å‡†ç¡®ç‡:\", study.best_value)\n",
    "'''\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“‚ äºŒã€æ•°æ®å‡†å¤‡ä¸åˆ’åˆ†\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"æ­¥éª¤1: ä½¿ç”¨é¡¹ç›®é¢„å¤„ç†åçš„æ•°æ®\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ä¿å­˜ç»“æœåˆ° CSV\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output/optuna_cv_results/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "print(f\"äº¤å‰éªŒè¯ç­–ç•¥: ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds_indices ({len(cv_folds_indices)} æŠ˜)\")\n",
    "print(f\"æ•°æ®é¢„å¤„ç†: ä½¿ç”¨ preprocessor ç®¡é“\")\n",
    "\n",
    "# å¯¼å‡ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¾›åç»­åˆ†æ\n",
    "train_data_export = X_train.copy()\n",
    "train_data_export['label'] = y_train\n",
    "train_data_export.to_csv(output_dir + 'train_dataset_optuna.csv', index=False)\n",
    "\n",
    "test_data_export = X_test.copy()\n",
    "test_data_export['label'] = y_test\n",
    "test_data_export.to_csv(output_dir + 'test_dataset_optuna.csv', index=False)\n",
    "print(\"\\næ•°æ®å¯¼å‡ºå®Œæˆ: train_dataset_optuna.csv, test_dataset_optuna.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ” ä¸‰ã€å®šä¹‰Optunaä¼˜åŒ–å‡½æ•°\n",
    "# ä¸ºæ¯ä¸ªæ¨¡å‹å®šä¹‰objectiveå‡½æ•°ï¼Œä½¿ç”¨è¿ç»­æœç´¢ç©ºé—´\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤2: å®šä¹‰ Optuna è¶…å‚æ•°ä¼˜åŒ–å‡½æ•°\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# åˆ›å»ºè‡ªå®šä¹‰äº¤å‰éªŒè¯è¯„åˆ†å™¨\n",
    "def custom_cv_score(model, X, y, cv_indices):\n",
    "    \"\"\"ä½¿ç”¨é¢„å®šä¹‰çš„cv_folds_indicesè¿›è¡Œäº¤å‰éªŒè¯\"\"\"\n",
    "    scores = []\n",
    "    for train_idx, val_idx in cv_indices:\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1]\n",
    "        score = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "        scores.append(score)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# å®šä¹‰å„æ¨¡å‹çš„Optunaä¼˜åŒ–å‡½æ•°\n",
    "def objective_catboost(trial):\n",
    "    \"\"\"CatBoostä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 50, 300),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n",
    "        'random_state': 42,\n",
    "        'verbose': False,\n",
    "        'thread_count': 1\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', CatBoostClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_svm(trial):\n",
    "    \"\"\"SVMä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.1, 10, log=True),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear']),\n",
    "        'probability': True,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', SVC(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_xgboost(trial):\n",
    "    \"\"\"XGBoostä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'objective': 'binary:logistic',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', xgb.XGBClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_lightgbm(trial):\n",
    "    \"\"\"LightGBMä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',\n",
    "        'verbose': -1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 1\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', lgb.LGBMClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_logistic(trial):\n",
    "    \"\"\"Logistic Regressionä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'C': trial.suggest_float('C', 0.01, 100, log=True),\n",
    "        'solver': trial.suggest_categorical('solver', ['liblinear', 'lbfgs']),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', LogisticRegression(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_random_forest(trial):\n",
    "    \"\"\"Random Forestä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 10, 30),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 5, 20),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "        'bootstrap': True,\n",
    "        'max_samples': trial.suggest_float('max_samples', 0.6, 0.9),\n",
    "        'ccp_alpha': trial.suggest_float('ccp_alpha', 0.0, 0.01),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', RandomForestClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_mlp(trial):\n",
    "    \"\"\"MLPä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    # å®šä¹‰éšè—å±‚ç»“æ„\n",
    "    hidden_layer_choice = trial.suggest_categorical('hidden_layer_choice', [\n",
    "        'single_50', 'single_100', 'single_150',\n",
    "        'double_50_50', 'double_100_50',\n",
    "        'triple_100_50_25'\n",
    "    ])\n",
    "    \n",
    "    hidden_layers = {\n",
    "        'single_50': (50,),\n",
    "        'single_100': (100,),\n",
    "        'single_150': (150,),\n",
    "        'double_50_50': (50, 50),\n",
    "        'double_100_50': (100, 50),\n",
    "        'triple_100_50_25': (100, 50, 25)\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'hidden_layer_sizes': hidden_layers[hidden_layer_choice],\n",
    "        'alpha': trial.suggest_float('alpha', 0.0001, 0.01, log=True),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 0.001, 0.01, log=True),\n",
    "        'max_iter': 500,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', MLPClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_knn(trial):\n",
    "    \"\"\"KNNä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 5),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'metric': trial.suggest_categorical('metric', ['euclidean', 'manhattan'])\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', KNeighborsClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_adaboost(trial):\n",
    "    \"\"\"AdaBoostä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 2.0, log=True),\n",
    "        'algorithm': 'SAMME',\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', AdaBoostClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_decision_tree(trial):\n",
    "    \"\"\"Decision Treeä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', DecisionTreeClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_naive_bayes(trial):\n",
    "    \"\"\"Naive Bayesä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'var_smoothing': trial.suggest_float('var_smoothing', 1e-10, 1e-7, log=True)\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', GaussianNB(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "def objective_gradient_boosting(trial):\n",
    "    \"\"\"Gradient Boostingä¼˜åŒ–å‡½æ•°\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "        ('classifier', GradientBoostingClassifier(**params))\n",
    "    ])\n",
    "    \n",
    "    return custom_cv_score(model, X_train, y_train, cv_folds_indices)\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹é…ç½®å­—å…¸\n",
    "model_configs = {\n",
    "    'CatBoost': {\n",
    "        'objective_func': objective_catboost,\n",
    "        'model_class': CatBoostClassifier\n",
    "    },\n",
    "    'SVM': {\n",
    "        'objective_func': objective_svm,\n",
    "        'model_class': SVC\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'objective_func': objective_xgboost,\n",
    "        'model_class': xgb.XGBClassifier\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'objective_func': objective_lightgbm,\n",
    "        'model_class': lgb.LGBMClassifier\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'objective_func': objective_logistic,\n",
    "        'model_class': LogisticRegression\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'objective_func': objective_random_forest,\n",
    "        'model_class': RandomForestClassifier\n",
    "    },\n",
    "    'MLP': {\n",
    "        'objective_func': objective_mlp,\n",
    "        'model_class': MLPClassifier\n",
    "    },\n",
    "    'KNN': {\n",
    "        'objective_func': objective_knn,\n",
    "        'model_class': KNeighborsClassifier\n",
    "    },\n",
    "    'AdaBoost': {\n",
    "        'objective_func': objective_adaboost,\n",
    "        'model_class': AdaBoostClassifier\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'objective_func': objective_decision_tree,\n",
    "        'model_class': DecisionTreeClassifier\n",
    "    },\n",
    "    'Naive Bayes': {\n",
    "        'objective_func': objective_naive_bayes,\n",
    "        'model_class': GaussianNB\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'objective_func': objective_gradient_boosting,\n",
    "        'model_class': GradientBoostingClassifier\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"å·²å®šä¹‰ {len(model_configs)} ä¸ªæ¨¡å‹çš„ Optuna ä¼˜åŒ–å‡½æ•°\")\n",
    "print(f\"æ¨¡å‹åˆ—è¡¨: {list(model_configs.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# âš¡ å››ã€Optunaè¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ‰¹é‡å¤„ç†å¤šä¸ªæ¨¡å‹ï¼‰\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤3: æ‰§è¡Œ Optuna è¶…å‚æ•°ä¼˜åŒ–ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"âœ… å¼€å§‹è¶…å‚æ•°è°ƒä¼˜ä¸æœ€ç»ˆæ¨¡å‹è®­ç»ƒ...\")\n",
    "print(f\"ä½¿ç”¨æ•°æ®: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"äº¤å‰éªŒè¯ç­–ç•¥: ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds_indices ({len(cv_folds_indices)} æŠ˜)\")\n",
    "print(f\"ä¼˜åŒ–æ–¹æ³•: Optuna (n_trials=50)\")\n",
    "\n",
    "# å­˜å‚¨ä¼˜åŒ–åçš„ç»“æœ\n",
    "final_models = {}           # å­˜å‚¨æœ€ä½³æ¨¡å‹\n",
    "cv_results_data = []        # å­˜å‚¨ç»“æœæ•°æ®\n",
    "optimization_details = []   # å­˜å‚¨è¯¦ç»†ä¼˜åŒ–ä¿¡æ¯\n",
    "all_trials_data = []        # å­˜å‚¨æ‰€æœ‰trialä¿¡æ¯\n",
    "\n",
    "# éå†æ¯ä¸ªæ¨¡å‹è¿›è¡ŒOptunaä¼˜åŒ–\n",
    "for idx, (model_name, config) in enumerate(model_configs.items(), 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"[{idx}/{len(model_configs)}] æ­£åœ¨ä¼˜åŒ– {model_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # åˆ›å»ºOptuna study\n",
    "        study = optuna.create_study(\n",
    "            direction='maximize',\n",
    "            study_name=f'{model_name}_optimization',\n",
    "            sampler=optuna.samplers.TPESampler(seed=42)\n",
    "        )\n",
    "        \n",
    "        print(f\"  â³ å¼€å§‹ Optuna ä¼˜åŒ– (50 trials)...\")\n",
    "        \n",
    "        # æ‰§è¡Œä¼˜åŒ–\n",
    "        study.optimize(config['objective_func'], n_trials=50, show_progress_bar=False)\n",
    "        \n",
    "        # è·å–æœ€ä½³å‚æ•°\n",
    "        best_params = study.best_params\n",
    "        best_score = study.best_value\n",
    "        \n",
    "        # è®°å½•æ‰€æœ‰trialsä¿¡æ¯\n",
    "        for trial in study.trials:\n",
    "            all_trials_data.append({\n",
    "                'Model': model_name,\n",
    "                'Trial': trial.number,\n",
    "                'AUC': trial.value if trial.value is not None else 'Failed',\n",
    "                'Params': str(trial.params)\n",
    "            })\n",
    "        \n",
    "        print(f\"  âœ… ä¼˜åŒ–å®Œæˆ!\")\n",
    "        print(f\"  -> æœ€ä½³ AUC: {best_score:.4f}\")\n",
    "        print(f\"  -> æœ€ä½³å‚æ•°: {best_params}\")\n",
    "        \n",
    "        # ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹\n",
    "        print(f\"  â³ ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ...\")\n",
    "        \n",
    "        # æ ¹æ®æ¨¡å‹ç±»å‹æ„å»ºæœ€ç»ˆæ¨¡å‹\n",
    "        if model_name == 'MLP':\n",
    "            # MLPéœ€è¦ç‰¹æ®Šå¤„ç†hidden_layer_sizes\n",
    "            hidden_layers = {\n",
    "                'single_50': (50,),\n",
    "                'single_100': (100,),\n",
    "                'single_150': (150,),\n",
    "                'double_50_50': (50, 50),\n",
    "                'double_100_50': (100, 50),\n",
    "                'triple_100_50_25': (100, 50, 25)\n",
    "            }\n",
    "            final_params = {k: v for k, v in best_params.items() if k != 'hidden_layer_choice'}\n",
    "            final_params['hidden_layer_sizes'] = hidden_layers[best_params['hidden_layer_choice']]\n",
    "            final_params['max_iter'] = 500\n",
    "            final_params['random_state'] = 42\n",
    "        elif model_name == 'CatBoost':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['random_state'] = 42\n",
    "            final_params['verbose'] = False\n",
    "            final_params['thread_count'] = 1\n",
    "        elif model_name == 'XGBoost':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['objective'] = 'binary:logistic'\n",
    "            final_params['random_state'] = 42\n",
    "            final_params['n_jobs'] = 1\n",
    "        elif model_name == 'LightGBM':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['objective'] = 'binary'\n",
    "            final_params['metric'] = 'binary_logloss'\n",
    "            final_params['verbose'] = -1\n",
    "            final_params['random_state'] = 42\n",
    "            final_params['n_jobs'] = 1\n",
    "        elif model_name == 'Logistic Regression':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['max_iter'] = 1000\n",
    "            final_params['random_state'] = 42\n",
    "        elif model_name == 'Random Forest':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['bootstrap'] = True\n",
    "            final_params['random_state'] = 42\n",
    "            final_params['n_jobs'] = -1\n",
    "        elif model_name == 'SVM':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['probability'] = True\n",
    "            final_params['random_state'] = 42\n",
    "        elif model_name == 'AdaBoost':\n",
    "            final_params = best_params.copy()\n",
    "            final_params['algorithm'] = 'SAMME'\n",
    "            final_params['random_state'] = 42\n",
    "        elif model_name in ['Decision Tree', 'Gradient Boosting']:\n",
    "            final_params = best_params.copy()\n",
    "            final_params['random_state'] = 42\n",
    "        else:\n",
    "            final_params = best_params.copy()\n",
    "        \n",
    "        # åˆ›å»ºæœ€ç»ˆæ¨¡å‹\n",
    "        final_model = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "            ('classifier', config['model_class'](**final_params))\n",
    "        ])\n",
    "        \n",
    "        # åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šè®­ç»ƒ\n",
    "        final_model.fit(X_train, y_train)\n",
    "        \n",
    "        # ä¿å­˜æ¨¡å‹\n",
    "        final_models[model_name] = final_model\n",
    "        \n",
    "        # è®¡ç®—äº¤å‰éªŒè¯çš„æ ‡å‡†å·®ï¼ˆä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è¯„ä¼°ï¼‰\n",
    "        fold_scores = []\n",
    "        for train_idx, val_idx in cv_folds_indices:\n",
    "            X_train_fold = X_train.iloc[train_idx]\n",
    "            y_train_fold = y_train.iloc[train_idx]\n",
    "            X_val_fold = X_train.iloc[val_idx]\n",
    "            y_val_fold = y_train.iloc[val_idx]\n",
    "            \n",
    "            fold_model = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('final_imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "                ('classifier', config['model_class'](**final_params))\n",
    "            ])\n",
    "            fold_model.fit(X_train_fold, y_train_fold)\n",
    "            y_pred_proba = fold_model.predict_proba(X_val_fold)[:, 1]\n",
    "            score = roc_auc_score(y_val_fold, y_pred_proba)\n",
    "            fold_scores.append(score)\n",
    "        \n",
    "        best_std = np.std(fold_scores)\n",
    "        \n",
    "        cv_results_data.append({\n",
    "            'Model': model_name,\n",
    "            'Mean AUC': best_score,\n",
    "            'Std AUC': best_std,\n",
    "            'Scores': fold_scores,\n",
    "            'Best Params': str(best_params),\n",
    "            'N_Trials': len(study.trials)\n",
    "        })\n",
    "        \n",
    "        optimization_details.append({\n",
    "            'Model': model_name,\n",
    "            'Status': 'Success',\n",
    "            'Best AUC': f\"{best_score:.4f}\",\n",
    "            'Std': f\"{best_std:.4f}\",\n",
    "            'Best Params': str(best_params),\n",
    "            'N_Trials': len(study.trials)\n",
    "        })\n",
    "        \n",
    "        # ä¿å­˜å•ä¸ªæ¨¡å‹\n",
    "        model_filename = f\"{model_name.replace(' ', '_')}_optuna_optimized.pkl\"\n",
    "        output_dir_models = os.path.join(output_dir, 'models')\n",
    "        if not os.path.exists(output_dir_models):\n",
    "            os.makedirs(output_dir_models)\n",
    "        joblib.dump(final_model, os.path.join(output_dir_models, model_filename))\n",
    "        print(f\"  ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {model_name} ä¼˜åŒ–å¤±è´¥: {str(e)}\")\n",
    "        optimization_details.append({\n",
    "            'Model': model_name,\n",
    "            'Status': 'Failed',\n",
    "            'Best AUC': 'N/A',\n",
    "            'Std': 'N/A',\n",
    "            'Best Params': f'Error: {str(e)}',\n",
    "            'N_Trials': 0\n",
    "        })\n",
    "        continue\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š äº”ã€æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤4: æ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for model_name, model in final_models.items():\n",
    "    print(f\"\\nè¯„ä¼° {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # è®­ç»ƒé›†è¯„ä¼°\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        train_precision = precision_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_recall = recall_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_f1 = f1_score(y_train, y_train_pred, zero_division=0)\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "        \n",
    "        # æµ‹è¯•é›†è¯„ä¼°\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        y_test_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        test_precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "        \n",
    "        performance_results.append({\n",
    "            'Model': model_name,\n",
    "            'Train_Accuracy': train_accuracy,\n",
    "            'Train_Precision': train_precision,\n",
    "            'Train_Recall': train_recall,\n",
    "            'Train_F1': train_f1,\n",
    "            'Train_ROC_AUC': train_roc_auc,\n",
    "            'Test_Accuracy': test_accuracy,\n",
    "            'Test_Precision': test_precision,\n",
    "            'Test_Recall': test_recall,\n",
    "            'Test_F1': test_f1,\n",
    "            'Test_ROC_AUC': test_roc_auc,\n",
    "            'AUC_Diff': abs(train_roc_auc - test_roc_auc)\n",
    "        })\n",
    "        \n",
    "        print(f\"  è®­ç»ƒé›† - Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f}, AUC: {train_roc_auc:.4f}\")\n",
    "        print(f\"  æµ‹è¯•é›† - Acc: {test_accuracy:.4f}, Prec: {test_precision:.4f}, Rec: {test_recall:.4f}, F1: {test_f1:.4f}, AUC: {test_roc_auc:.4f}\")\n",
    "        print(f\"  è¿‡æ‹Ÿåˆåˆ†æ: AUCå·®å¼‚ = {abs(train_roc_auc - test_roc_auc):.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {model_name} è¯„ä¼°å¤±è´¥: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ˆ å…­ã€ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤5: ç»“æœæ±‡æ€»ä¸å¯è§†åŒ–\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å°†ç»“æœè½¬æ¢ä¸º DataFrame\n",
    "df_cv_results = pd.DataFrame(cv_results_data).sort_values('Mean AUC', ascending=False)\n",
    "df_performance = pd.DataFrame(performance_results).sort_values('Test_ROC_AUC', ascending=False)\n",
    "df_all_trials = pd.DataFrame(all_trials_data)\n",
    "\n",
    "print(\"\\näº¤å‰éªŒè¯ä¼˜åŒ–ç»“æœæ±‡æ€»:\")\n",
    "print(df_cv_results[['Model', 'Mean AUC', 'Std AUC', 'N_Trials']].to_string(index=False))\n",
    "\n",
    "print(\"\\næµ‹è¯•é›†æ€§èƒ½æ±‡æ€»:\")\n",
    "print(df_performance[['Model', 'Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1', 'Test_ROC_AUC', 'AUC_Diff']].to_string(index=False))\n",
    "\n",
    "df_cv_results.drop(columns=['Scores']).to_csv(os.path.join(output_dir, 'optuna_cv_optimization_results.csv'), index=False)\n",
    "df_performance.to_csv(os.path.join(output_dir, 'optuna_performance_results.csv'), index=False)\n",
    "df_all_trials.to_csv(os.path.join(output_dir, 'optuna_all_trials.csv'), index=False)\n",
    "\n",
    "print(f\"\\nç»“æœå·²ä¿å­˜åˆ°: {output_dir}\")\n",
    "\n",
    "# å¯è§†åŒ–ï¼šäº¤å‰éªŒè¯æ€§èƒ½å¯¹æ¯”\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "models_list = df_cv_results['Model'].tolist()\n",
    "means = df_cv_results['Mean AUC'].tolist()\n",
    "stds = df_cv_results['Std AUC'].tolist()\n",
    "x_pos = np.arange(len(models_list))\n",
    "\n",
    "ax.bar(x_pos, means, yerr=stds, align='center', alpha=0.7, \n",
    "       capsize=10, color=nature_colors[:len(models_list)])\n",
    "ax.set_ylabel('AUC Score (CV Optimized)', fontsize=12)\n",
    "ax.set_title('Cross-Validation Performance after Optuna Optimization', fontsize=14)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax.set_ylim([0.5, 1.0])\n",
    "ax.yaxis.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'optuna_cv_optimized_barplot.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: optuna_cv_optimized_barplot.png\")\n",
    "\n",
    "# =============================================================================\n",
    "# ä¸ƒã€å°†æœ€ä½³æ¨¡å‹èµ‹å€¼ç»™å…¨å±€å˜é‡ï¼ˆä¿æŒä¸åç»­å•å…ƒæ ¼çš„å…¼å®¹æ€§ï¼‰\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"æ­¥éª¤6: æ›´æ–°å…¨å±€å˜é‡\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# å°†ä¼˜åŒ–åçš„æ¨¡å‹èµ‹å€¼ç»™å…¨å±€å˜é‡\n",
    "if 'CatBoost' in final_models:\n",
    "    catboost_model = final_models['CatBoost']\n",
    "    print(\"âœ… catboost_model å·²æ›´æ–°\")\n",
    "if 'SVM' in final_models:\n",
    "    svm = final_models['SVM']\n",
    "    print(\"âœ… svm å·²æ›´æ–°\")\n",
    "if 'XGBoost' in final_models:\n",
    "    xgb_model = final_models['XGBoost']\n",
    "    print(\"âœ… xgb_model å·²æ›´æ–°\")\n",
    "if 'LightGBM' in final_models:\n",
    "    lgb_model = final_models['LightGBM']\n",
    "    print(\"âœ… lgb_model å·²æ›´æ–°\")\n",
    "if 'Logistic Regression' in final_models:\n",
    "    log_reg = final_models['Logistic Regression']\n",
    "    print(\"âœ… log_reg å·²æ›´æ–°\")\n",
    "if 'Random Forest' in final_models:\n",
    "    rf = final_models['Random Forest']\n",
    "    print(\"âœ… rf å·²æ›´æ–°\")\n",
    "if 'MLP' in final_models:\n",
    "    mlp = final_models['MLP']\n",
    "    print(\"âœ… mlp å·²æ›´æ–°\")\n",
    "if 'KNN' in final_models:\n",
    "    knn = final_models['KNN']\n",
    "    print(\"âœ… knn å·²æ›´æ–°\")\n",
    "if 'AdaBoost' in final_models:\n",
    "    ada = final_models['AdaBoost']\n",
    "    print(\"âœ… ada å·²æ›´æ–°\")\n",
    "if 'Decision Tree' in final_models:\n",
    "    dt = final_models['Decision Tree']\n",
    "    print(\"âœ… dt å·²æ›´æ–°\")\n",
    "if 'Naive Bayes' in final_models:\n",
    "    nb = final_models['Naive Bayes']\n",
    "    print(\"âœ… nb å·²æ›´æ–°\")\n",
    "if 'Gradient Boosting' in final_models:\n",
    "    gb = final_models['Gradient Boosting']\n",
    "    print(\"âœ… gb å·²æ›´æ–°\")\n",
    "\n",
    "print(\"\\nâœ… final_models å­—å…¸å·²æ›´æ–°ï¼ŒåŒ…å«æ‰€æœ‰ä¼˜åŒ–åçš„æ¨¡å‹\")\n",
    "\n",
    "# =============================================================================\n",
    "# å…«ã€æ€»ç»“æŠ¥å‘Š\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"å®Œæˆ! Optuna ä¼˜åŒ–å¤šæ¨¡å‹è®­ç»ƒæ€»ç»“\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n1. æˆåŠŸä¼˜åŒ–æ¨¡å‹æ•°é‡: {len(final_models)}/{len(model_configs)}\")\n",
    "print(f\"2. æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰æµ‹è¯•é›†AUCæ’åºï¼‰:\")\n",
    "if len(df_performance) > 0:\n",
    "    for idx, row in df_performance.head(5).iterrows():\n",
    "        print(f\"   {row['Model']}: Test AUC = {row['Test_ROC_AUC']:.4f}\")\n",
    "print(f\"\\n3. è¾“å‡ºæ–‡ä»¶:\")\n",
    "print(f\"   - è®­ç»ƒæ•°æ®: train_dataset_optuna.csv\")\n",
    "print(f\"   - æµ‹è¯•æ•°æ®: test_dataset_optuna.csv\")\n",
    "print(f\"   - äº¤å‰éªŒè¯ç»“æœ: {output_dir}/optuna_cv_optimization_results.csv\")\n",
    "print(f\"   - æ€§èƒ½è¯„ä¼°ç»“æœ: {output_dir}/optuna_performance_results.csv\")\n",
    "print(f\"   - æ‰€æœ‰trialæ•°æ®: {output_dir}/optuna_all_trials.csv\")\n",
    "print(f\"   - å¯è§†åŒ–å›¾è¡¨: {output_dir}/optuna_cv_optimized_barplot.png\")\n",
    "print(f\"\\n4. å…¨å±€å˜é‡å·²æ›´æ–°ï¼Œå¯ä»¥ç›´æ¥ç”¨äºåç»­ROCåˆ†æå’ŒSHAPåˆ†æ\")\n",
    "print(f\"   - final_models: åŒ…å«æ‰€æœ‰ä¼˜åŒ–åçš„æ¨¡å‹\")\n",
    "print(f\"   - å•ç‹¬å˜é‡: catboost_model, svm, xgb_model, lgb_model, log_reg, rf, mlp, knn, ada, dt, nb, gb\")\n",
    "print(\"\\n5. Optunaä¼˜åŒ–ç‰¹ç‚¹:\")\n",
    "print(f\"   - ä½¿ç”¨è¿ç»­æœç´¢ç©ºé—´ï¼Œæ›´ç²¾ç»†çš„å‚æ•°è°ƒä¼˜\")\n",
    "print(f\"   - æ¯ä¸ªæ¨¡å‹è¿è¡Œ50æ¬¡trialï¼Œè‡ªé€‚åº”é‡‡æ ·ç­–ç•¥\")\n",
    "print(f\"   - ä¿å­˜äº†æ‰€æœ‰trialçš„è¯¦ç»†ä¿¡æ¯ä¾›åˆ†æ\")\n",
    "print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9083269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# äº¤å‰éªŒè¯ ROC æ›²çº¿ç»˜åˆ¶ä»£ç \n",
    "## 1.plot_cv_roc_curve å‡½æ•°ï¼š\n",
    "# æ¥æ”¶ä¸€ä¸ªåŸºç¡€æ¨¡å‹ï¼ˆå¦‚ mlpï¼‰ï¼Œä½¿ç”¨ sklearn.base.clone å¤åˆ¶å®ƒï¼Œç¡®ä¿æ¯ä¸€æŠ˜éƒ½æ˜¯ä»å¤´è®­ç»ƒï¼Œäº’ä¸å¹²æ‰°ã€‚\n",
    "# ç›´æ¥éå†ä½ ä¹‹å‰ç”Ÿæˆçš„ cv_folds åˆ—è¡¨, ä½¿ç”¨ .iloc[train_idx] ä» X_train å’Œ y_train ä¸­æå–æ¯ä¸€æŠ˜çš„æ•°æ®ã€‚\n",
    "# è®¡ç®—å¹¶ç»˜åˆ¶æ¯ä¸€æŠ˜çš„ ROC æ›²çº¿ï¼Œæœ€åè®¡ç®—å¹³å‡ AUC å’Œæ ‡å‡†å·®åŒºåŸŸã€‚\n",
    "## 2.æ¨¡å‹å…¼å®¹æ€§ï¼š\n",
    "# æ­¤ä»£ç é€‚ç”¨äº svm, rf, mlp, log_reg, xgb_model ç­‰éµå¾ª Scikit-learn æ¥å£ï¼ˆæœ‰ .fit() å’Œ .predict_proba()ï¼‰çš„æ¨¡å‹ã€‚\n",
    "# å¦‚æœæ˜¯ lgb.train å‡ºæ¥çš„ Booster å¯¹è±¡åˆ™æ²¡æœ‰ fit/predict_proba æ–¹æ³•ï¼Œè¿™é‡Œå‡è®¾ä¼ å…¥çš„æ˜¯ sklearn æ¥å£çš„æ¨¡å‹\n",
    "# æ³¨æ„ï¼šå¦‚æœéœ€è¦ç”» LightGBMï¼Œè¯·ä½¿ç”¨ LGBMClassifier åŒ…è£…å™¨ã€‚å¯¹äº LightGBM (sklearn API) å’Œ XGBoostï¼Œé€šå¸¸éƒ½æœ‰ predict_proba\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_cv_roc_curve(base_model, X, y, cv_folds, model_name, ax=None, is_lgb=False):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨é¢„å®šä¹‰çš„ cv_folds ç»˜åˆ¶äº¤å‰éªŒè¯ ROC æ›²çº¿\n",
    "    \n",
    "    å‚æ•°:\n",
    "        base_model: åˆå§‹æ¨¡å‹å¯¹è±¡ (éœ€æ”¯æŒ scikit-learn æ¥å£)\n",
    "        X: è®­ç»ƒé›†ç‰¹å¾ (DataFrame)\n",
    "        y: è®­ç»ƒé›†æ ‡ç­¾ (Series)\n",
    "        cv_folds: åŒ…å« (train_idx, val_idx) å…ƒç»„çš„åˆ—è¡¨ (æ•´æ•°ç´¢å¼•!)\n",
    "        model_name: æ¨¡å‹åç§°å­—ç¬¦ä¸²\n",
    "        ax: matplotlib axes å¯¹è±¡ (å¯é€‰)\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # ğŸ†• å®šä¹‰æ›´é²œè‰³çš„é¢œè‰²æ–¹æ¡ˆï¼ˆä½¿ç”¨é¥±å’Œåº¦æ›´é«˜çš„é¢œè‰²ï¼‰\n",
    "    fold_colors = [\"#F31414\", \"#956DF1\", \"#F37A00\", \"#1862EB\", \"#F1C100\", \"#11F5E6\"]\n",
    "    \n",
    "    print(f\"æ­£åœ¨ç»˜åˆ¶ {model_name} çš„ {len(cv_folds)} æŠ˜äº¤å‰éªŒè¯ ROC æ›²çº¿...\")\n",
    "\n",
    "    # éå†æ¯ä¸€æŠ˜\n",
    "    for i, (train_idx, val_idx) in enumerate(cv_folds):\n",
    "        print(f\"  å¤„ç†ç¬¬ {i+1} æŠ˜: è®­ç»ƒé›† {len(train_idx)} æ ·æœ¬, éªŒè¯é›† {len(val_idx)} æ ·æœ¬\")\n",
    "        \n",
    "        # å…‹éš†æ¨¡å‹\n",
    "        model = clone(base_model)\n",
    "        \n",
    "        # ç›´æ¥ä½¿ç”¨æ•´æ•°ç´¢å¼•\n",
    "        X_train_fold = X.iloc[train_idx]\n",
    "        y_train_fold = y.iloc[train_idx]\n",
    "        X_val_fold = X.iloc[val_idx]\n",
    "        y_val_fold = y.iloc[val_idx]\n",
    "        \n",
    "        # è®­ç»ƒæ¨¡å‹\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # é¢„æµ‹æ¦‚ç‡\n",
    "        if is_lgb:\n",
    "            # y_prob = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "            y_prob = model.predict(X_val_fold)\n",
    "        else:\n",
    "            y_prob = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        # è®¡ç®— ROC æ›²çº¿\n",
    "        fpr, tpr, _ = roc_curve(y_val_fold, y_prob)\n",
    "        \n",
    "        # æ’å€¼\n",
    "        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        \n",
    "        # è®¡ç®— AUC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "        \n",
    "        # ç»˜åˆ¶å½“å‰æŠ˜çš„æ›²çº¿ï¼Œæé«˜é€æ˜åº¦ (alpha ä» 0.3 â†’ 0.7)ï¼Œä½¿ç”¨é²œè‰³é¢œè‰²\n",
    "        ax.plot(fpr, tpr, lw=2, alpha=0.7,\n",
    "                color=fold_colors[i % len(fold_colors)],\n",
    "                label=f'ROC fold {i+1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # ç»˜åˆ¶å¯¹è§’çº¿\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    # è®¡ç®—å¹¶ç»˜åˆ¶å¹³å‡ ROC\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # ä½¿ç”¨æ›´é†’ç›®çš„è“è‰²\n",
    "    ax.plot(mean_fpr, mean_tpr, color='#1E3A8A',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=3, alpha=.9)\n",
    "\n",
    "    # ç»˜åˆ¶æ ‡å‡†å·®é˜´å½±\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "           title=f\"ROC Curve - {model_name} ({len(cv_folds)}-Fold CV)\")\n",
    "    ax.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# ========== å‡†å¤‡æ•°æ® ==========\n",
    "# ä½¿ç”¨ X_train (å·²é¢„å¤„ç†çš„ç‰¹å¾) å’Œ y_train (æ ‡ç­¾)\n",
    "y_train_series = y_train.reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"å¼€å§‹ç»˜åˆ¶äº¤å‰éªŒè¯ ROC æ›²çº¿...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== ç»˜åˆ¶å„æ¨¡å‹çš„ CV ROC æ›²çº¿ ==========\n",
    "# 1. SVM\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(svm, X_train, y_train_series, cv_folds_indices, \"SVM\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_svm.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 2. XGBoost\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(xgb_model, X_train, y_train_series, cv_folds_indices, \"XGBoost\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_xgboost.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 3. LightGBM\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(lgb_model, X_train, y_train_series, cv_folds_indices, \"LightGBM\", ax=ax, is_lgb=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_lightgbm.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 4. Logistic Regression\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(log_reg, X_train, y_train_series, cv_folds_indices, \"Logistic Regression\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_logistic.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 5. Random Forest\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(rf, X_train, y_train_series, cv_folds_indices, \"Random Forest\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_rf.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 6. MLP\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(mlp, X_train, y_train_series, cv_folds_indices, \"MLP\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_mlp.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 7. KNN\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(knn, X_train, y_train_series, cv_folds_indices, \"KNN\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_knn.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 8. AdaBoost\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(ada, X_train, y_train_series, cv_folds_indices, \"AdaBoost\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_adaboost.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 9. Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(dt, X_train, y_train_series, cv_folds_indices, \"Decision Tree\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_decision_tree.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 10. Naive Bayes\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_cv_roc_curve(nb, X_train, y_train_series, cv_folds_indices, \"Naive Bayes\", ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/cv_roc_naive_bayes.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… äº¤å‰éªŒè¯ ROC æ›²çº¿ç»˜åˆ¶å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e458fb",
   "metadata": {},
   "source": [
    "# ğŸ“Š æœ€ç»ˆæ¨¡å‹æµ‹è¯•é›†éªŒè¯ä¸ROCæ›²çº¿ç»˜åˆ¶\n",
    "\n",
    "## å·¥ä½œæµç¨‹è¯´æ˜\n",
    "\n",
    "ç»è¿‡ç½‘æ ¼æœç´¢ (`GridSearchCV`) å’Œäº¤å‰éªŒè¯åï¼Œæˆ‘ä»¬å·²ç»å¾—åˆ°äº†æ¯ä¸ªæ¨¡å‹çš„æœ€ä½³å‚æ•°ã€‚æ¥ä¸‹æ¥çš„æ­¥éª¤ï¼š\n",
    "\n",
    "1. **ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å…¨é‡è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒ** \n",
    "   - GridSearchCV çš„ `refit=True` å‚æ•°å·²è‡ªåŠ¨å®Œæˆæ­¤æ­¥éª¤\n",
    "   - `final_models` å­—å…¸ä¸­å­˜å‚¨çš„å°±æ˜¯åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡è®­åçš„æœ€ä½³æ¨¡å‹\n",
    "\n",
    "2. **åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹å’ŒéªŒè¯** \n",
    "   - ä½¿ç”¨é‡è®­åçš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\n",
    "   - è®¡ç®—æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æŒ‡æ ‡ï¼ˆAUCã€å‡†ç¡®ç‡ç­‰ï¼‰\n",
    "\n",
    "3. **ç»˜åˆ¶æµ‹è¯•é›†ROCæ›²çº¿**\n",
    "   - ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶åœ¨æµ‹è¯•é›†ä¸Šçš„ROCæ›²çº¿\n",
    "   - å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æµ‹è¯•é›†æ€§èƒ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ea22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆæ¨¡å‹æµ‹è¯•é›†éªŒè¯ä¸ROCæ›²çº¿ç»˜åˆ¶\n",
    "# =============================================================================\n",
    "# æ­¥éª¤1: ç¡®è®¤æ¨¡å‹å·²ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å…¨é‡è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒ\n",
    "# =============================================================================\n",
    "# GridSearchCV å·²è‡ªåŠ¨å®Œæˆè¿™ä¸€æ­¥ (refit=True)\n",
    "# final_models å­—å…¸ä¸­çš„æ¨¡å‹éƒ½æ˜¯æœ€ç»ˆçš„ã€å·²é‡è®­çš„æ¨¡å‹\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤1: ç¡®è®¤æœ€ä½³æ¨¡å‹ (å·²åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡è®­)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"å¯ç”¨çš„æœ€ä½³æ¨¡å‹: {list(final_models.keys())}\")\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: X_train {X_train.shape}, y_train {len(y_train)}\")\n",
    "print(f\"æµ‹è¯•é›†å¤§å°: X_test {X_test.shape}, y_test {len(y_test)}\")\n",
    "print()\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤2: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹å’Œè¯„ä¼°\n",
    "# =============================================================================\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤2: åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡ŒéªŒè¯\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# å­˜å‚¨æµ‹è¯•é›†æ€§èƒ½\n",
    "test_results = []\n",
    "\n",
    "for model_name, model in final_models.items():\n",
    "    print(f\"\\næ­£åœ¨è¯„ä¼° {model_name} ...\")\n",
    "    \n",
    "    # é¢„æµ‹æ¦‚ç‡\n",
    "    if model_name == 'LightGBM':\n",
    "        # LightGBM ä½¿ç”¨ç‰¹æ®Šçš„é¢„æµ‹æ–¹æ³•\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # é¢„æµ‹ç±»åˆ« (ä½¿ç”¨é»˜è®¤é˜ˆå€¼ 0.5)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # è®¡ç®—å„é¡¹æŒ‡æ ‡\n",
    "    test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    test_acc = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    test_recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    test_f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    \n",
    "    # å­˜å‚¨ç»“æœ\n",
    "    test_results.append({\n",
    "        'Model': model_name,\n",
    "        'Test AUC': test_auc,\n",
    "        'Test Accuracy': test_acc,\n",
    "        'Test Precision': test_precision,\n",
    "        'Test Recall': test_recall,\n",
    "        'Test F1': test_f1\n",
    "    })\n",
    "    \n",
    "    print(f\"  æµ‹è¯•é›† AUC: {test_auc:.4f}\")\n",
    "    print(f\"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}\")\n",
    "    print(f\"  æµ‹è¯•é›†ç²¾ç¡®ç‡: {test_precision:.4f}\")\n",
    "    print(f\"  æµ‹è¯•é›†å¬å›ç‡: {test_recall:.4f}\")\n",
    "    print(f\"  æµ‹è¯•é›† F1 åˆ†æ•°: {test_f1:.4f}\")\n",
    "\n",
    "# è½¬æ¢ä¸º DataFrame å¹¶æ’åº\n",
    "df_test_results = pd.DataFrame(test_results).sort_values('Test AUC', ascending=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"æµ‹è¯•é›†æ€§èƒ½æ±‡æ€» (æŒ‰ AUC æ’åº)\")\n",
    "print(\"=\"*80)\n",
    "print(df_test_results.to_string(index=False))\n",
    "\n",
    "# ä¿å­˜æµ‹è¯•é›†ç»“æœ\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output'\n",
    "df_test_results.to_csv(os.path.join(output_dir, 'test_set_results.csv'), index=False)\n",
    "print(f\"\\nâœ… æµ‹è¯•é›†ç»“æœå·²ä¿å­˜è‡³: {os.path.join(output_dir, 'test_set_results.csv')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb94a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ­¥éª¤3: ç»˜åˆ¶æµ‹è¯•é›†ROCæ›²çº¿ - æ‰€æœ‰æ¨¡å‹åœ¨ä¸€å¼ å›¾ä¸Š\n",
    "# =============================================================================\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤3: ç»˜åˆ¶æµ‹è¯•é›†ROCæ›²çº¿\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# å®šä¹‰é¢œè‰²æ–¹æ¡ˆ (ä½¿ç”¨é²œè‰³çš„é¢œè‰²)\n",
    "colors = {\n",
    "    'SVM': '#E74C3C',           # çº¢è‰²\n",
    "    'XGBoost': '#3498DB',       # è“è‰²\n",
    "    'LightGBM': '#2ECC71',      # ç»¿è‰²\n",
    "    'Logistic Regression': '#F39C12',  # æ©™è‰²\n",
    "    'Random Forest': '#9B59B6', # ç´«è‰²\n",
    "    'MLP': '#1ABC9C',           # é’è‰²\n",
    "    'KNN': '#E67E22',           # æ·±æ©™è‰²\n",
    "    'AdaBoost': '#34495E',      # æ·±ç°è‰²\n",
    "    'Decision Tree': '#16A085', # æ·±é’è‰²\n",
    "    'Naive Bayes': '#C0392B'    # æ·±çº¢è‰²\n",
    "}\n",
    "\n",
    "# åˆ›å»ºå›¾å½¢\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶ROCæ›²çº¿\n",
    "for model_name, model in final_models.items():\n",
    "    print(f\"æ­£åœ¨ç»˜åˆ¶ {model_name} çš„æµ‹è¯•é›†ROCæ›²çº¿...\")\n",
    "    \n",
    "    # é¢„æµ‹æ¦‚ç‡\n",
    "    if model_name == 'LightGBM':\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # è®¡ç®—ROCæ›²çº¿\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # ç»˜åˆ¶æ›²çº¿\n",
    "    color = colors.get(model_name, '#95A5A6')  # é»˜è®¤ç°è‰²\n",
    "    plt.plot(fpr, tpr, color=color, lw=2.5, \n",
    "             label=f'{model_name} (AUC = {roc_auc:.3f})', alpha=0.8)\n",
    "\n",
    "# ç»˜åˆ¶å¯¹è§’çº¿ï¼ˆéšæœºåˆ†ç±»å™¨ï¼‰\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)', alpha=0.6)\n",
    "\n",
    "# å›¾å½¢ç¾åŒ–\n",
    "plt.xlim([-0.02, 1.02])\n",
    "plt.ylim([-0.02, 1.02])\n",
    "plt.xlabel('False Positive Rate (FPR)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Positive Rate (TPR)', fontsize=14, fontweight='bold')\n",
    "plt.title('ROC Curves on Test Set - All Models', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.legend(loc='lower right', fontsize=11, framealpha=0.95)\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜å›¾å½¢\n",
    "output_path = os.path.join(output_dir, 'test_set_roc_curves_all_models.png')\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ… æµ‹è¯•é›†ROCæ›²çº¿å·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n",
    "# åŒæ—¶ä¿å­˜ä¸ºPDFæ ¼å¼ï¼ˆç”¨äºè®ºæ–‡å‘è¡¨ï¼‰\n",
    "output_path_pdf = os.path.join(output_dir, 'test_set_roc_curves_all_models.pdf')\n",
    "plt.savefig(output_path_pdf, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… æµ‹è¯•é›†ROCæ›²çº¿(PDF)å·²ä¿å­˜è‡³: {output_path_pdf}\")\n",
    "\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c9c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ­¥éª¤4: ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç»˜åˆ¶æµ‹è¯•é›†ROCæ›²çº¿ï¼ˆé«˜æ¸…å¤§å›¾ï¼‰\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤4: ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç»˜åˆ¶æµ‹è¯•é›†ROCæ›²çº¿\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# åˆ›å»ºå­ç›®å½•ä¿å­˜å•ç‹¬çš„ROCæ›²çº¿å›¾\n",
    "individual_roc_dir = os.path.join(output_dir, 'individual_test_roc_curves')\n",
    "if not os.path.exists(individual_roc_dir):\n",
    "    os.makedirs(individual_roc_dir)\n",
    "    print(f\"åˆ›å»ºç›®å½•: {individual_roc_dir}\")\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç»˜åˆ¶ROCæ›²çº¿\n",
    "for model_name, model in final_models.items():\n",
    "    print(f\"\\næ­£åœ¨ä¸º {model_name} ç»˜åˆ¶å•ç‹¬çš„æµ‹è¯•é›†ROCæ›²çº¿...\")\n",
    "    \n",
    "    # é¢„æµ‹æ¦‚ç‡\n",
    "    if model_name == 'LightGBM':\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # è®¡ç®—ROCæ›²çº¿\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # åˆ›å»ºæ–°å›¾å½¢\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # ç»˜åˆ¶ROCæ›²çº¿\n",
    "    color = colors.get(model_name, '#3498DB')\n",
    "    ax.plot(fpr, tpr, color=color, lw=3, \n",
    "            label=f'{model_name}\\n(AUC = {roc_auc:.4f})', alpha=0.9)\n",
    "    \n",
    "    # ç»˜åˆ¶å¯¹è§’çº¿\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.500)', alpha=0.6)\n",
    "    \n",
    "    # å¡«å……ROCæ›²çº¿ä¸‹çš„é¢ç§¯\n",
    "    ax.fill_between(fpr, tpr, alpha=0.2, color=color)\n",
    "    \n",
    "    # å›¾å½¢ç¾åŒ–\n",
    "    ax.set_xlim([-0.02, 1.02])\n",
    "    ax.set_ylim([-0.02, 1.02])\n",
    "    ax.set_xlabel('False Positive Rate (FPR)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('True Positive Rate (TPR)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title(f'ROC Curve - {model_name}\\nTest Set Performance', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='lower right', fontsize=12, framealpha=0.95)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    \n",
    "    # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬æ¡†\n",
    "    test_acc = accuracy_score(y_test, (y_pred_proba >= 0.5).astype(int))\n",
    "    textstr = f'Test Samples: {len(y_test)}\\nAccuracy: {test_acc:.4f}\\nAUC: {roc_auc:.4f}'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)\n",
    "    ax.text(0.98, 0.02, textstr, transform=ax.transAxes, fontsize=10,\n",
    "            verticalalignment='bottom', horizontalalignment='right', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜å›¾å½¢\n",
    "    safe_name = model_name.replace(' ', '_').lower()\n",
    "    output_path = os.path.join(individual_roc_dir, f'test_roc_{safe_name}.png')\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # åŒæ—¶ä¿å­˜PDFæ ¼å¼\n",
    "    output_path_pdf = os.path.join(individual_roc_dir, f'test_roc_{safe_name}.pdf')\n",
    "    plt.savefig(output_path_pdf, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.close()\n",
    "    print(f\"  âœ… å·²ä¿å­˜: {output_path}\")\n",
    "\n",
    "print(f\"\\nâœ… æ‰€æœ‰å•ç‹¬çš„æµ‹è¯•é›†ROCæ›²çº¿å·²ä¿å­˜è‡³: {individual_roc_dir}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfee4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ­¥éª¤5: å¯¹æ¯”è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½ï¼ˆæ£€æŸ¥è¿‡æ‹Ÿåˆï¼‰\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤5: è®­ç»ƒé›† vs æµ‹è¯•é›†æ€§èƒ½å¯¹æ¯”\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# è®¡ç®—è®­ç»ƒé›†æ€§èƒ½\n",
    "train_results = []\n",
    "\n",
    "for model_name, model in final_models.items():\n",
    "    # é¢„æµ‹è®­ç»ƒé›†\n",
    "    if model_name == 'LightGBM':\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_train_pred_proba = model.predict_proba(X_train)[:, 1]\n",
    "    \n",
    "    train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "    \n",
    "    train_results.append({\n",
    "        'Model': model_name,\n",
    "        'Train AUC': train_auc\n",
    "    })\n",
    "\n",
    "df_train_results = pd.DataFrame(train_results)\n",
    "\n",
    "# åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç»“æœ\n",
    "df_comparison = df_test_results[['Model', 'Test AUC']].merge(\n",
    "    df_train_results, on='Model'\n",
    ")\n",
    "\n",
    "# è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦ (è®­ç»ƒé›†AUC - æµ‹è¯•é›†AUC)\n",
    "df_comparison['Overfit Gap'] = df_comparison['Train AUC'] - df_comparison['Test AUC']\n",
    "df_comparison = df_comparison.sort_values('Test AUC', ascending=False)\n",
    "\n",
    "print(df_comparison.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# å·¦å›¾: è®­ç»ƒé›† vs æµ‹è¯•é›† AUCå¯¹æ¯”\n",
    "models_list = df_comparison['Model'].tolist()\n",
    "train_aucs = df_comparison['Train AUC'].tolist()\n",
    "test_aucs = df_comparison['Test AUC'].tolist()\n",
    "\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, train_aucs, width, label='Train AUC', \n",
    "                alpha=0.8, color='#3498DB')\n",
    "bars2 = ax1.bar(x + width/2, test_aucs, width, label='Test AUC', \n",
    "                alpha=0.8, color='#E74C3C')\n",
    "\n",
    "ax1.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('AUC Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Train vs Test AUC Comparison', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim([0.5, 1.0])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# å³å›¾: è¿‡æ‹Ÿåˆç¨‹åº¦\n",
    "overfit_gaps = df_comparison['Overfit Gap'].tolist()\n",
    "colors_overfit = ['#27AE60' if gap < 0.1 else '#F39C12' if gap < 0.2 else '#E74C3C' \n",
    "                  for gap in overfit_gaps]\n",
    "\n",
    "bars3 = ax2.bar(x, overfit_gaps, alpha=0.8, color=colors_overfit)\n",
    "\n",
    "ax2.set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Overfit Gap (Train AUC - Test AUC)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax2.axhline(y=0.1, color='orange', linestyle='--', linewidth=2, alpha=0.7, \n",
    "            label='Moderate Overfit (0.1)')\n",
    "ax2.axhline(y=0.2, color='red', linestyle='--', linewidth=2, alpha=0.7, \n",
    "            label='High Overfit (0.2)')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜å›¾å½¢\n",
    "output_path = os.path.join(output_dir, 'train_vs_test_comparison.png')\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n",
    "output_path_pdf = os.path.join(output_dir, 'train_vs_test_comparison.pdf')\n",
    "plt.savefig(output_path_pdf, dpi=300, bbox_inches='tight')\n",
    "print(f\"âœ… è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”å›¾(PDF)å·²ä¿å­˜è‡³: {output_path_pdf}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ä¿å­˜å¯¹æ¯”ç»“æœ\n",
    "df_comparison.to_csv(os.path.join(output_dir, 'train_test_comparison.csv'), index=False)\n",
    "print(f\"âœ… å¯¹æ¯”ç»“æœå·²ä¿å­˜è‡³: {os.path.join(output_dir, 'train_test_comparison.csv')}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442b2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ­¥éª¤6: ç»˜åˆ¶Top3æ¨¡å‹çš„æµ‹è¯•é›†ROCæ›²çº¿å¯¹æ¯”å›¾ï¼ˆç”¨äºè®ºæ–‡å±•ç¤ºï¼‰\n",
    "# =============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"æ­¥éª¤6: ç»˜åˆ¶Top3æ¨¡å‹çš„æµ‹è¯•é›†ROCæ›²çº¿å¯¹æ¯”\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# é€‰æ‹©æµ‹è¯•é›†AUCæœ€é«˜çš„å‰3ä¸ªæ¨¡å‹\n",
    "top3_models = df_test_results.head(3)['Model'].tolist()\n",
    "print(f\"Top 3 æ¨¡å‹: {top3_models}\")\n",
    "\n",
    "# åˆ›å»ºé«˜è´¨é‡å›¾å½¢\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# å®šä¹‰æ›´é²œè‰³çš„é¢œè‰²\n",
    "top_colors = ['#E74C3C', '#3498DB', '#2ECC71']  # çº¢ã€è“ã€ç»¿\n",
    "\n",
    "for idx, model_name in enumerate(top3_models):\n",
    "    model = final_models[model_name]\n",
    "    \n",
    "    # é¢„æµ‹æ¦‚ç‡\n",
    "    if model_name == 'LightGBM':\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # è®¡ç®—ROCæ›²çº¿\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # ç»˜åˆ¶ROCæ›²çº¿\n",
    "    ax.plot(fpr, tpr, color=top_colors[idx], lw=3.5, \n",
    "            label=f'{model_name} (AUC = {roc_auc:.4f})', alpha=0.9)\n",
    "    \n",
    "    print(f\"  {idx+1}. {model_name}: AUC = {roc_auc:.4f}\")\n",
    "\n",
    "# ç»˜åˆ¶å¯¹è§’çº¿\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2.5, label='Random Classifier', alpha=0.6)\n",
    "\n",
    "# å›¾å½¢ç¾åŒ–\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim([-0.02, 1.02])\n",
    "ax.set_xlabel('False Positive Rate (FPR)', fontsize=16, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate (TPR)', fontsize=16, fontweight='bold')\n",
    "ax.set_title('ROC Curves - Top 3 Models (Test Set)', \n",
    "             fontsize=18, fontweight='bold', pad=20)\n",
    "ax.legend(loc='lower right', fontsize=14, framealpha=0.95, edgecolor='black', fancybox=True)\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)\n",
    "\n",
    "# è®¾ç½®åˆ»åº¦å­—ä½“å¤§å°\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜é«˜åˆ†è¾¨ç‡å›¾å½¢ï¼ˆç”¨äºè®ºæ–‡ï¼‰\n",
    "output_path = os.path.join(output_dir, 'test_roc_top3_models_publication.png')\n",
    "plt.savefig(output_path, dpi=600, bbox_inches='tight', facecolor='white')\n",
    "print(f\"\\nâœ… Top3æ¨¡å‹ROCæ›²çº¿(é«˜æ¸…PNG)å·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n",
    "output_path_pdf = os.path.join(output_dir, 'test_roc_top3_models_publication.pdf')\n",
    "plt.savefig(output_path_pdf, dpi=600, bbox_inches='tight')\n",
    "print(f\"âœ… Top3æ¨¡å‹ROCæ›²çº¿(PDF)å·²ä¿å­˜è‡³: {output_path_pdf}\")\n",
    "\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258af01e",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ€»ç»“\n",
    "\n",
    "### å®Œæ•´å·¥ä½œæµç¨‹å›é¡¾\n",
    "\n",
    "1. **ç½‘æ ¼æœç´¢ä¸äº¤å‰éªŒè¯** âœ… (å•å…ƒæ ¼23)\n",
    "   - ä½¿ç”¨ `GridSearchCV` å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜\n",
    "   - åœ¨é¢„å®šä¹‰çš„5æŠ˜äº¤å‰éªŒè¯ä¸Šæœç´¢æœ€ä½³å‚æ•°\n",
    "   - è‡ªåŠ¨ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨å®Œæ•´è®­ç»ƒé›†ä¸Šé‡æ–°è®­ç»ƒï¼ˆ`refit=True`ï¼‰\n",
    "\n",
    "2. **æµ‹è¯•é›†éªŒè¯** âœ… (åˆšåˆšå®Œæˆ)\n",
    "   - ä½¿ç”¨é‡è®­åçš„æœ€ä½³æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹\n",
    "   - è®¡ç®—æµ‹è¯•é›†ä¸Šçš„å¤šé¡¹æ€§èƒ½æŒ‡æ ‡ï¼ˆAUCã€å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°ï¼‰\n",
    "   - ä¿å­˜æµ‹è¯•é›†ç»“æœåˆ°CSVæ–‡ä»¶\n",
    "\n",
    "3. **ROCæ›²çº¿ç»˜åˆ¶** âœ… (åˆšåˆšå®Œæˆ)\n",
    "   - ç»˜åˆ¶æ‰€æœ‰æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„ROCæ›²çº¿ï¼ˆä¸€å¼ å›¾ï¼‰\n",
    "   - ä¸ºæ¯ä¸ªæ¨¡å‹å•ç‹¬ç»˜åˆ¶é«˜æ¸…ROCæ›²çº¿\n",
    "   - ç»˜åˆ¶Top3æ¨¡å‹çš„å¯¹æ¯”ROCæ›²çº¿ï¼ˆç”¨äºè®ºæ–‡å‘è¡¨ï¼‰\n",
    "\n",
    "4. **è¿‡æ‹Ÿåˆåˆ†æ** âœ… (åˆšåˆšå®Œæˆ)\n",
    "   - å¯¹æ¯”è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½\n",
    "   - è®¡ç®—è¿‡æ‹Ÿåˆç¨‹åº¦ï¼ˆTrain AUC - Test AUCï¼‰\n",
    "   - å¯è§†åŒ–è¿‡æ‹Ÿåˆåˆ†æç»“æœ\n",
    "\n",
    "### è¾“å‡ºæ–‡ä»¶æ¸…å•\n",
    "\n",
    "æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨ `/home/phl/PHL/Car-T/model_v1/output/` ç›®å½•ä¸‹ï¼š\n",
    "\n",
    "- `test_set_results.csv` - æµ‹è¯•é›†æ€§èƒ½æ±‡æ€»è¡¨\n",
    "- `train_test_comparison.csv` - è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”\n",
    "- `test_set_roc_curves_all_models.png/pdf` - æ‰€æœ‰æ¨¡å‹ROCæ›²çº¿\n",
    "- `train_vs_test_comparison.png/pdf` - è®­ç»ƒé›†vsæµ‹è¯•é›†å¯¹æ¯”å›¾\n",
    "- `test_roc_top3_models_publication.png/pdf` - Top3æ¨¡å‹ROCæ›²çº¿ï¼ˆé«˜æ¸…ï¼‰\n",
    "- `individual_test_roc_curves/` - æ¯ä¸ªæ¨¡å‹çš„å•ç‹¬ROCæ›²çº¿\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "âœ… **æ— éœ€æ‰‹åŠ¨é‡è®­ç»ƒ**: `GridSearchCV` çš„ `refit=True` å·²è‡ªåŠ¨å®Œæˆ\n",
    "\n",
    "âœ… **ä½¿ç”¨ç›¸åŒçš„æ•°æ®åˆ’åˆ†**: æµ‹è¯•é›†æ¥è‡ªåˆå§‹åˆ’åˆ†ï¼Œç¡®ä¿å…¬å¹³æ¯”è¾ƒ\n",
    "\n",
    "âœ… **å¤šæ ¼å¼è¾“å‡º**: åŒæ—¶ç”ŸæˆPNGå’ŒPDFæ ¼å¼ï¼Œæ»¡è¶³ä¸åŒéœ€æ±‚\n",
    "\n",
    "âœ… **è¿‡æ‹Ÿåˆæ£€æµ‹**: é€šè¿‡å¯¹æ¯”è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ€§èƒ½ï¼Œè¯†åˆ«è¿‡æ‹Ÿåˆé—®é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# é¢å¤–: æŸ¥çœ‹æœ€ä½³æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯å’Œæ··æ·†çŸ©é˜µ\n",
    "# =============================================================================\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"æœ€ä½³æ¨¡å‹è¯¦ç»†åˆ†æ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# è·å–æµ‹è¯•é›†AUCæœ€é«˜çš„æ¨¡å‹\n",
    "best_model_name = df_test_results.iloc[0]['Model']\n",
    "best_model = final_models[best_model_name]\n",
    "best_test_auc = df_test_results.iloc[0]['Test AUC']\n",
    "\n",
    "print(f\"\\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}\")\n",
    "print(f\"   æµ‹è¯•é›† AUC: {best_test_auc:.4f}\")\n",
    "print()\n",
    "\n",
    "# é¢„æµ‹\n",
    "if best_model_name == 'LightGBM':\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š\n",
    "print(\"åˆ†ç±»æŠ¥å‘Š:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "\n",
    "# ç»˜åˆ¶æ··æ·†çŸ©é˜µ\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap='Blues', ax=ax, values_format='d')\n",
    "ax.set_title(f'Confusion Matrix - {best_model_name}\\n(Test Set)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# ä¿å­˜æ··æ·†çŸ©é˜µ\n",
    "output_path = os.path.join(output_dir, f'confusion_matrix_{best_model_name.replace(\" \", \"_\").lower()}.png')\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nâœ… æ··æ·†çŸ©é˜µå·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# æŸ¥çœ‹æ¨¡å‹çš„æœ€ä½³å‚æ•°\n",
    "print(f\"\\n{best_model_name} çš„æœ€ä½³å‚æ•°:\")\n",
    "if hasattr(best_model, 'named_steps'):\n",
    "    # å¦‚æœæ˜¯Pipelineï¼Œè·å–classifieréƒ¨åˆ†çš„å‚æ•°\n",
    "    classifier = best_model.named_steps['classifier']\n",
    "    params = classifier.get_params()\n",
    "    # åªæ˜¾ç¤ºéé»˜è®¤çš„å…³é”®å‚æ•°\n",
    "    key_params = {k: v for k, v in params.items() \n",
    "                  if not k.endswith('_') and not callable(v) and v is not None}\n",
    "    for param, value in sorted(key_params.items())[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå‚æ•°\n",
    "        print(f\"  {param}: {value}\")\n",
    "else:\n",
    "    print(\"  \", best_model.get_params())\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"åˆ†æå®Œæˆï¼\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec16b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è‡ªå¸¦çš„ç‰¹å¾é‡è¦æ€§æ’åºï¼ˆæ”¯æŒ RFã€XGBoostã€LightGBMï¼‰\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# 1. å®šä¹‰é€šç”¨çš„ç‰¹å¾åç§°æå–å‡½æ•°\n",
    "# =============================================================================\n",
    "def get_feature_names_from_preprocessor(preprocessor):\n",
    "    \"\"\"\n",
    "    ä» ColumnTransformer ä¸­æå–æ‰€æœ‰è½¬æ¢åçš„ç‰¹å¾åç§°\n",
    "    å…¼å®¹ sklearn >= 1.0 çš„ get_feature_names_out() æ–¹æ³•\n",
    "    \"\"\"\n",
    "    # ä¼˜å…ˆä½¿ç”¨ sklearn å†…ç½®æ–¹æ³•\n",
    "    try:\n",
    "        return list(preprocessor.get_feature_names_out())\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨éå†\n",
    "    feature_names = []\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if name == 'num':\n",
    "            feature_names.extend(columns)\n",
    "        elif name == 'cat':\n",
    "            ohe = transformer.named_steps['onehot']\n",
    "            ohe_feature_names = ohe.get_feature_names_out(columns)\n",
    "            feature_names.extend(ohe_feature_names)\n",
    "        elif name == 'remainder' and transformer == 'passthrough':\n",
    "            if isinstance(columns, list):\n",
    "                feature_names.extend(columns)\n",
    "    return feature_names\n",
    "\n",
    "\n",
    "def extract_feature_importance(pipeline, model_name):\n",
    "    \"\"\"\n",
    "    ä» Pipeline ä¸­æå–ç‰¹å¾é‡è¦æ€§\n",
    "    \n",
    "    å‚æ•°:\n",
    "        pipeline: åŒ…å« preprocessor å’Œ classifier çš„ Pipeline å¯¹è±¡\n",
    "        model_name: æ¨¡å‹åç§° (ç”¨äºæ—¥å¿—è¾“å‡º)\n",
    "    \n",
    "    è¿”å›:\n",
    "        tuple: (importances, feature_names)\n",
    "    \"\"\"\n",
    "    # è·å–åˆ†ç±»å™¨å’Œé¢„å¤„ç†å™¨\n",
    "    classifier = pipeline.named_steps['classifier']\n",
    "    preprocessor = pipeline.named_steps['preprocessor']\n",
    "    \n",
    "    # è·å–ç‰¹å¾é‡è¦æ€§\n",
    "    importances = classifier.feature_importances_\n",
    "    \n",
    "    # è·å–ç‰¹å¾åç§°\n",
    "    feature_names = get_feature_names_from_preprocessor(preprocessor)\n",
    "    \n",
    "    # éªŒè¯æ•°é‡åŒ¹é…\n",
    "    if len(feature_names) != len(importances):\n",
    "        print(f\" {model_name}: ç‰¹å¾æ•°é‡ä¸åŒ¹é… ({len(feature_names)} vs {len(importances)})\")\n",
    "        feature_names = [f\"feature_{i}\" for i in range(len(importances))]\n",
    "    \n",
    "    return importances, feature_names\n",
    "\n",
    "\n",
    "def plot_feature_importance(importances, feature_names, model_name, top_n=20, \n",
    "                            output_dir=\"/home/phl/PHL/Car-T/model_v1/output\",\n",
    "                            importance_type=\"default\"):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§æ¡å½¢å›¾\n",
    "    \n",
    "    å‚æ•°:\n",
    "        importances: ç‰¹å¾é‡è¦æ€§æ•°ç»„\n",
    "        feature_names: ç‰¹å¾åç§°åˆ—è¡¨\n",
    "        model_name: æ¨¡å‹åç§°\n",
    "        top_n: æ˜¾ç¤ºå‰ N ä¸ªç‰¹å¾\n",
    "        output_dir: è¾“å‡ºç›®å½•\n",
    "        importance_type: é‡è¦æ€§ç±»å‹ (ç”¨äºæ ‡é¢˜æ˜¾ç¤º)\n",
    "    \"\"\"\n",
    "    # åˆ›å»º DataFrame å¹¶æ’åº\n",
    "    feature_imp_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values(by='importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # å– Top N\n",
    "    top_features = feature_imp_df.head(top_n)\n",
    "    plot_data = top_features.sort_values(by='importance', ascending=True)\n",
    "    \n",
    "    # ç»˜å›¾\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    y_ticks = np.arange(len(plot_data))\n",
    "    \n",
    "    # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®é¢œè‰²\n",
    "    color_map = {\n",
    "        'Random Forest': '#1f77b4',\n",
    "        'XGBoost': '#ff7f0e',\n",
    "        'LightGBM': '#2ca02c'\n",
    "    }\n",
    "    color = color_map.get(model_name, '#1f77b4')\n",
    "    \n",
    "    ax.barh(y_ticks, plot_data['importance'], color=color)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    ax.set_yticklabels(plot_data['feature'])\n",
    "    \n",
    "    # è®¾ç½®æ ‡é¢˜ (åŒ…å«é‡è¦æ€§ç±»å‹è¯´æ˜)\n",
    "    title_suffix = {\n",
    "        'Random Forest': 'MDI (Mean Decrease Impurity)',\n",
    "        'XGBoost': 'Gain',\n",
    "        'LightGBM': 'Split'\n",
    "    }\n",
    "    ax.set_title(f\"{model_name} Feature Importances (Top {top_n})\\n[{title_suffix.get(model_name, '')}]\", \n",
    "                 fontsize=14)\n",
    "    ax.set_xlabel(\"Importance Score\", fontsize=12)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    filename = f\"{model_name.lower().replace(' ', '_')}_feature_importance.png\"\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    plt.savefig(output_path, bbox_inches='tight')\n",
    "    print(f\" {model_name} ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜è‡³: {output_path}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp_df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ç»˜åˆ¶å„æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§\n",
    "# =============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"å¼€å§‹æå–å’Œç»˜åˆ¶å„æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ç»“æœ\n",
    "all_importance_results = {}\n",
    "\n",
    "# --- Random Forest ---\n",
    "print(\"\\n Random Forest:\")\n",
    "rf_importances, rf_feature_names = extract_feature_importance(rf, \"Random Forest\")\n",
    "rf_imp_df = plot_feature_importance(rf_importances, rf_feature_names, \"Random Forest\")\n",
    "all_importance_results['Random Forest'] = rf_imp_df\n",
    "\n",
    "# # --- XGBoost ---\n",
    "# print(\"\\n XGBoost:\")\n",
    "# xgb_importances, xgb_feature_names = extract_feature_importance(xgb_model, \"XGBoost\")\n",
    "# xgb_imp_df = plot_feature_importance(xgb_importances, xgb_feature_names, \"XGBoost\")\n",
    "# all_importance_results['XGBoost'] = xgb_imp_df\n",
    "\n",
    "# --- LightGBM ---\n",
    "print(\"\\n LightGBM:\")\n",
    "lgb_importances, lgb_feature_names = extract_feature_importance(lgb_model, \"LightGBM\")\n",
    "lgb_imp_df = plot_feature_importance(lgb_importances, lgb_feature_names, \"LightGBM\")\n",
    "all_importance_results['LightGBM'] = lgb_imp_df\n",
    "\n",
    "# =============================================================================\n",
    "# 3. å¤šæ¨¡å‹ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ç»˜åˆ¶å¤šæ¨¡å‹ç‰¹å¾é‡è¦æ€§å¯¹æ¯”å›¾...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# åˆå¹¶æ‰€æœ‰æ¨¡å‹çš„ Top 10 ç‰¹å¾\n",
    "top_n_compare = 10\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, imp_df in all_importance_results.items():\n",
    "    top_features = imp_df.head(top_n_compare).copy()\n",
    "    top_features['model'] = model_name\n",
    "    comparison_data.append(top_features)\n",
    "\n",
    "comparison_df = pd.concat(comparison_data, ignore_index=True)\n",
    "\n",
    "# æ‰¾å‡ºæ‰€æœ‰æ¨¡å‹ä¸­å‡ºç°è¿‡çš„ Top ç‰¹å¾çš„å¹¶é›†\n",
    "all_top_features = comparison_df['feature'].unique()\n",
    "\n",
    "# åˆ›å»ºå¯¹æ¯”çƒ­åŠ›å›¾æ•°æ®\n",
    "heatmap_data = pd.DataFrame(index=all_top_features)\n",
    "for model_name, imp_df in all_importance_results.items():\n",
    "    imp_dict = dict(zip(imp_df['feature'], imp_df['importance']))\n",
    "    heatmap_data[model_name] = [imp_dict.get(f, 0) for f in all_top_features]\n",
    "\n",
    "# æŒ‰å¹³å‡é‡è¦æ€§æ’åº\n",
    "heatmap_data['mean'] = heatmap_data.mean(axis=1)\n",
    "heatmap_data = heatmap_data.sort_values('mean', ascending=False).drop(columns='mean').head(15)\n",
    "\n",
    "# ç»˜åˆ¶çƒ­åŠ›å›¾\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(heatmap_data.values, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# è®¾ç½®åæ ‡è½´\n",
    "ax.set_xticks(np.arange(len(heatmap_data.columns)))\n",
    "ax.set_yticks(np.arange(len(heatmap_data.index)))\n",
    "ax.set_xticklabels(heatmap_data.columns)\n",
    "ax.set_yticklabels(heatmap_data.index)\n",
    "\n",
    "# æ·»åŠ é¢œè‰²æ¡\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "cbar.ax.set_ylabel(\"Importance Score\", rotation=-90, va=\"bottom\")\n",
    "\n",
    "# åœ¨å•å…ƒæ ¼ä¸­æ·»åŠ æ•°å€¼\n",
    "for i in range(len(heatmap_data.index)):\n",
    "    for j in range(len(heatmap_data.columns)):\n",
    "        text = ax.text(j, i, f\"{heatmap_data.iloc[i, j]:.3f}\",\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "ax.set_title(\"Feature Importance Comparison Across Models\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "output_path = os.path.join(\"/home/phl/PHL/Car-T/model_v1/output\", \"feature_importance_comparison.png\")\n",
    "plt.savefig(output_path, bbox_inches='tight')\n",
    "print(f\"âœ… å¤šæ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: {output_path}\")\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# 4. æ‰“å°å„æ¨¡å‹ Top 10 ç‰¹å¾æ±‡æ€»è¡¨\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å„æ¨¡å‹ Top 10 ç‰¹å¾æ±‡æ€»\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for model_name, imp_df in all_importance_results.items():\n",
    "    print(f\"\\nã€{model_name}ã€‘\")\n",
    "    print(imp_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1446066f",
   "metadata": {},
   "source": [
    "ä¸»è¦ç‰¹æ€§:å¯ä»¥é€šè¿‡ä¿®æ”¹ models_to_analyze åˆ—è¡¨æ¥é€‰æ‹©è¦åˆ†æçš„æ¨¡å‹ã€‚\n",
    "```\n",
    "ç‰¹æ€§\t            è¯´æ˜\n",
    "å¤šæ¨¡å‹æ”¯æŒ\t        é€šè¿‡ models_to_analyze åˆ—è¡¨çµæ´»é€‰æ‹©è¦åˆ†æçš„æ¨¡å‹\n",
    "è‡ªåŠ¨é€‰æ‹©Explainer\tæ ‘æ¨¡å‹ç”¨ TreeExplainerï¼Œçº¿æ€§æ¨¡å‹ç”¨ LinearExplainerï¼Œå…¶ä»–ç”¨ KernelExplainer\n",
    "æ‰¹é‡ç”Ÿæˆå›¾è¡¨\t    æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ 3 ç§å›¾ï¼šBar Plotã€Beeswarm Plotã€è‡ªå®šä¹‰æ¡å½¢å›¾\n",
    "å¤šæ¨¡å‹å¯¹æ¯”çƒ­åŠ›å›¾\t è‡ªåŠ¨ç”Ÿæˆç‰¹å¾é‡è¦æ€§å¯¹æ¯”çƒ­åŠ›å›¾\n",
    "ç»“æœä¿å­˜\t       æ‰€æœ‰å›¾ç‰‡è‡ªåŠ¨ä¿å­˜åˆ° output ç›®å½•\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šæ¨¡å‹ SHAP å¯è§£é‡Šæ€§åˆ†æ\n",
    "import matplotlib.cm as cm\n",
    "import shap\n",
    "\n",
    "# =============================================================================\n",
    "# 1. å®šä¹‰æ”¯æŒ SHAP åˆ†æçš„æ¨¡å‹åˆ—è¡¨\n",
    "# =============================================================================\n",
    "# ä¸åŒæ¨¡å‹éœ€è¦ä¸åŒçš„ Explainer\n",
    "tree_based_models = ['LightGBM', 'XGBoost', 'Random Forest', 'Decision Tree', 'AdaBoost']\n",
    "linear_models = ['Logistic Regression']\n",
    "kernel_models = ['SVM', 'KNN', 'MLP', 'Naive Bayes']  # éœ€è¦ KernelExplainer (è¾ƒæ…¢)\n",
    "\n",
    "# é€‰æ‹©è¦åˆ†æçš„æ¨¡å‹ (å¯æ ¹æ®éœ€è¦ä¿®æ”¹)\n",
    "models_to_analyze = ['SVM', 'Random Forest', 'MLP', 'Logistic Regression']\n",
    "# models_to_analyze = list(final_models.keys())  # åˆ†ææ‰€æœ‰æ¨¡å‹\n",
    "\n",
    "# =============================================================================\n",
    "# 2. æ•°æ®é¢„å¤„ç† - è·å–è½¬æ¢åçš„ç‰¹å¾\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å‡†å¤‡ SHAP åˆ†ææ•°æ®...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ä½¿ç”¨ä»»æ„ä¸€ä¸ª Pipeline è·å–é¢„å¤„ç†å™¨ (æ‰€æœ‰æ¨¡å‹å…±äº«ç›¸åŒçš„é¢„å¤„ç†å™¨)\n",
    "sample_pipeline = final_models['LightGBM']\n",
    "preprocessor = sample_pipeline.named_steps['preprocessor']\n",
    "final_imputer = sample_pipeline.named_steps['final_imputer']\n",
    "\n",
    "# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œé¢„å¤„ç†è½¬æ¢\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_train_transformed = final_imputer.transform(X_train_transformed)\n",
    "\n",
    "# è·å–è½¬æ¢åçš„ç‰¹å¾åç§°\n",
    "try:\n",
    "    feature_names_transformed = preprocessor.get_feature_names_out().tolist()\n",
    "except AttributeError:\n",
    "    feature_names_transformed = [f\"feature_{i}\" for i in range(X_train_transformed.shape[1])]\n",
    "\n",
    "# è½¬æ¢ä¸º DataFrame\n",
    "X_train_for_shap = pd.DataFrame(X_train_transformed, columns=feature_names_transformed)\n",
    "print(f\"åŸå§‹ç‰¹å¾æ•°: {X_train.shape[1]}, è½¬æ¢åç‰¹å¾æ•°: {X_train_for_shap.shape[1]}\")\n",
    "\n",
    "# å¯¹äº KernelExplainerï¼Œä½¿ç”¨é‡‡æ ·æ•°æ®åŠ é€Ÿ\n",
    "X_background = shap.sample(X_train_for_shap, 50)  # èƒŒæ™¯æ•°æ®é‡‡æ ·\n",
    "\n",
    "# =============================================================================\n",
    "# 3. å®šä¹‰ SHAP åˆ†æå‡½æ•°\n",
    "# =============================================================================\n",
    "def analyze_shap_for_model(model_name, pipeline, X_data, feature_names, output_dir):\n",
    "    \"\"\"\n",
    "    å¯¹å•ä¸ªæ¨¡å‹è¿›è¡Œ SHAP åˆ†æ\n",
    "    \n",
    "    å‚æ•°:\n",
    "        model_name: æ¨¡å‹åç§°\n",
    "        pipeline: è®­ç»ƒå¥½çš„ Pipeline\n",
    "        X_data: é¢„å¤„ç†åçš„ç‰¹å¾æ•°æ® (DataFrame)\n",
    "        feature_names: ç‰¹å¾åç§°åˆ—è¡¨\n",
    "        output_dir: è¾“å‡ºç›®å½•\n",
    "    \n",
    "    è¿”å›:\n",
    "        dict: åŒ…å« SHAP å€¼å’Œé‡è¦æ€§çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æ­£åœ¨åˆ†æ: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    classifier = pipeline.named_steps['classifier']\n",
    "    \n",
    "    # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©åˆé€‚çš„ Explainer\n",
    "    try:\n",
    "        if model_name in ['LightGBM', 'XGBoost', 'Random Forest', 'Decision Tree', 'AdaBoost']:\n",
    "            # æ ‘æ¨¡å‹ä½¿ç”¨ TreeExplainer (æœ€å¿«)\n",
    "            explainer = shap.TreeExplainer(classifier)\n",
    "            shap_values = explainer(X_data)\n",
    "            \n",
    "        elif model_name == 'Logistic Regression':\n",
    "            # çº¿æ€§æ¨¡å‹ä½¿ç”¨ LinearExplainer\n",
    "            explainer = shap.LinearExplainer(classifier, X_data)\n",
    "            shap_values = explainer(X_data)\n",
    "            \n",
    "        else:\n",
    "            # å…¶ä»–æ¨¡å‹ä½¿ç”¨ KernelExplainer (è¾ƒæ…¢ï¼Œä½¿ç”¨é‡‡æ ·)\n",
    "            print(f\"  ä½¿ç”¨ KernelExplainer (è¾ƒæ…¢ï¼Œé‡‡æ · 100 ä¸ªæ ·æœ¬)...\")\n",
    "            \n",
    "            def predict_proba_func(x):\n",
    "                # åˆ›å»ºä¸´æ—¶ DataFrame ä¿æŒåˆ—å\n",
    "                x_df = pd.DataFrame(x, columns=feature_names)\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç»•è¿‡ preprocessorï¼Œå› ä¸ºæ•°æ®å·²ç»é¢„å¤„ç†è¿‡\n",
    "                return classifier.predict_proba(x_df)[:, 1]\n",
    "            \n",
    "            explainer = shap.KernelExplainer(predict_proba_func, X_background)\n",
    "            # ä»…å¯¹éƒ¨åˆ†æ ·æœ¬è®¡ç®— SHAP å€¼ä»¥åŠ é€Ÿ\n",
    "            X_sample = X_data.sample(n=min(100, len(X_data)), random_state=42)\n",
    "            shap_values = explainer(X_sample)\n",
    "            X_data = X_sample  # æ›´æ–°ç”¨äºç»˜å›¾çš„æ•°æ®\n",
    "        \n",
    "        # æå– SHAP å€¼æ•°ç»„\n",
    "        shap_values_array = np.array(shap_values.values)\n",
    "        \n",
    "        # è®¡ç®— Mean |SHAP|\n",
    "        mean_abs_shap = np.mean(np.abs(shap_values_array), axis=0)\n",
    "        \n",
    "        # åˆ›å»ºç‰¹å¾é‡è¦æ€§ DataFrame\n",
    "        df_importance = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Mean_Abs_SHAP': mean_abs_shap\n",
    "        }).sort_values(by='Mean_Abs_SHAP', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # æ¯ä¸ªæ¨¡å‹ç”Ÿæˆ 3 ç§å›¾ï¼šBar Plotã€Beeswarm Plotã€è‡ªå®šä¹‰æ¡å½¢å›¾\n",
    "        # ============ ç»˜å›¾ 1: SHAP Bar Plot ============\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} - SHAP Feature Importance', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'shap_bar_{model_name.lower().replace(\" \", \"_\")}.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # ============ ç»˜å›¾ 2: SHAP Summary Plot (Beeswarm) ============\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.plots.beeswarm(shap_values, max_display=20, show=False)\n",
    "        plt.title(f'{model_name} - SHAP Summary Plot', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'shap_beeswarm_{model_name.lower().replace(\" \", \"_\")}.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # ============ ç»˜å›¾ 3: è‡ªå®šä¹‰ Mean |SHAP| æ¡å½¢å›¾ ============\n",
    "        df_plot = df_importance.head(20).sort_values(by='Mean_Abs_SHAP', ascending=True)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        colors = cm.RdPu(np.linspace(1, 0.4, len(df_plot)))\n",
    "        bars = plt.barh(df_plot['Feature'], df_plot['Mean_Abs_SHAP'], color=colors)\n",
    "        \n",
    "        for bar in bars:\n",
    "            plt.text(bar.get_width() + 0.002, bar.get_y() + bar.get_height()/2, \n",
    "                     f'{bar.get_width():.4f}', va='center', ha='left', fontsize=8)\n",
    "        \n",
    "        plt.xlabel('Mean |SHAP Value|')\n",
    "        plt.title(f'{model_name} - Feature Importance (Top 20)')\n",
    "        ax = plt.gca()\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'shap_custom_{model_name.lower().replace(\" \", \"_\")}.png'), \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\" {model_name} SHAP åˆ†æå®Œæˆ!\")\n",
    "        print(f\"\\nTop 10 ç‰¹å¾:\")\n",
    "        print(df_importance.head(10).to_string(index=False))\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'shap_values': shap_values,\n",
    "            'importance_df': df_importance,\n",
    "            'mean_abs_shap': mean_abs_shap\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" {model_name} SHAP åˆ†æå¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# 4. æ‰¹é‡æ‰§è¡Œ SHAP åˆ†æ\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"å¼€å§‹å¤šæ¨¡å‹ SHAP å¯è§£é‡Šæ€§åˆ†æ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output/shap_analysis'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„ç»“æœ\n",
    "all_shap_results = {}\n",
    "\n",
    "for model_name in models_to_analyze:\n",
    "    if model_name in final_models:\n",
    "        result = analyze_shap_for_model(\n",
    "            model_name=model_name,\n",
    "            pipeline=final_models[model_name],\n",
    "            X_data=X_train_for_shap.copy(),\n",
    "            feature_names=feature_names_transformed,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        if result is not None:\n",
    "            all_shap_results[model_name] = result\n",
    "    else:\n",
    "        print(f\" æ¨¡å‹ {model_name} æœªæ‰¾åˆ°ï¼Œè·³è¿‡...\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. å¤šæ¨¡å‹ SHAP å¯¹æ¯”åˆ†æ\n",
    "# =============================================================================\n",
    "if len(all_shap_results) > 1:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"å¤šæ¨¡å‹ SHAP ç‰¹å¾é‡è¦æ€§å¯¹æ¯”\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # åˆ›å»ºå¯¹æ¯” DataFrame\n",
    "    comparison_data = {}\n",
    "    for model_name, result in all_shap_results.items():\n",
    "        importance_dict = dict(zip(\n",
    "            result['importance_df']['Feature'], \n",
    "            result['importance_df']['Mean_Abs_SHAP']\n",
    "        ))\n",
    "        comparison_data[model_name] = importance_dict\n",
    "    \n",
    "    # è·å–æ‰€æœ‰æ¨¡å‹ Top 15 ç‰¹å¾çš„å¹¶é›†\n",
    "    all_top_features = set()\n",
    "    for model_name, result in all_shap_results.items():\n",
    "        top_features = result['importance_df'].head(15)['Feature'].tolist()\n",
    "        all_top_features.update(top_features)\n",
    "    \n",
    "    # æ„å»ºå¯¹æ¯”çŸ©é˜µ\n",
    "    comparison_df = pd.DataFrame(index=list(all_top_features))\n",
    "    for model_name, importance_dict in comparison_data.items():\n",
    "        comparison_df[model_name] = [importance_dict.get(f, 0) for f in comparison_df.index]\n",
    "    \n",
    "    # æŒ‰å¹³å‡å€¼æ’åº\n",
    "    comparison_df['Mean'] = comparison_df.mean(axis=1)\n",
    "    comparison_df = comparison_df.sort_values('Mean', ascending=False).drop(columns='Mean')\n",
    "    \n",
    "    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å¯¹æ¯”çƒ­åŠ›å›¾\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(comparison_df.values, cmap='YlOrRd', aspect='auto')\n",
    "    \n",
    "    plt.xticks(np.arange(len(comparison_df.columns)), comparison_df.columns, rotation=45, ha='right')\n",
    "    plt.yticks(np.arange(len(comparison_df.index)), comparison_df.index)\n",
    "    \n",
    "    # æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "    for i in range(len(comparison_df.index)):\n",
    "        for j in range(len(comparison_df.columns)):\n",
    "            text = plt.text(j, i, f'{comparison_df.iloc[i, j]:.3f}',\n",
    "                           ha='center', va='center', color='black', fontsize=8)\n",
    "    \n",
    "    plt.colorbar(im, label='Mean |SHAP Value|')\n",
    "    plt.title('Multi-Model SHAP Feature Importance Comparison', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'shap_multi_model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # æ‰“å°å¯¹æ¯”è¡¨\n",
    "    print(\"\\nç‰¹å¾é‡è¦æ€§å¯¹æ¯”è¡¨:\")\n",
    "    print(comparison_df.round(4).to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" å¤šæ¨¡å‹ SHAP åˆ†æå…¨éƒ¨å®Œæˆ!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81e8d0",
   "metadata": {},
   "source": [
    "åŒ…å«çš„è¯„ä¼°æŒ‡æ ‡\n",
    "```\n",
    "æŒ‡æ ‡                è¯´æ˜\t        è®¡ç®—å…¬å¼\n",
    "Accuracy\t  å‡†ç¡®ç‡\t        (TP+TN) / (TP+TN+FP+FN)\n",
    "AUC\t          ROCæ›²çº¿ä¸‹é¢ç§¯\t    åŸºäºé¢„æµ‹æ¦‚ç‡è®¡ç®—\n",
    "Sensitivity\t  çµæ•åº¦/çœŸé˜³æ€§ç‡\t TP / (TP+FN)\n",
    "Specificity\t  ç‰¹å¼‚æ€§/çœŸé˜´æ€§ç‡\t TN / (TN+FP)\n",
    "Precision\t  ç²¾ç¡®ç‡\t        TP / (TP+FP)\n",
    "Recall\t          å¬å›ç‡\t        TP / (TP+FN)\n",
    "F1 Score\t  F1åˆ†æ•°\t        2Ã—(PrecisionÃ—Recall)/(Precision+Recall)\n",
    "G-Mean\t          å‡ ä½•å‡å€¼\t   âˆš(Sensitivity Ã— Specificity)\n",
    "```\n",
    "è¾“å‡ºå†…å®¹\n",
    "```\n",
    "è®­ç»ƒé›†è¯„ä¼°ç»“æœè¡¨ - æ‰€æœ‰æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æŒ‡æ ‡\n",
    "æµ‹è¯•é›†è¯„ä¼°ç»“æœè¡¨ - æ‰€æœ‰æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æŒ‡æ ‡\n",
    "å„æ¨¡å‹è¯¦ç»†å¯¹æ¯” - è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„å·®å¼‚ï¼ˆç”¨äºæ£€æµ‹è¿‡æ‹Ÿåˆï¼‰\n",
    "CSV æ–‡ä»¶ä¿å­˜ - ä¾¿äºåç»­åˆ†æ\n",
    "å¯è§†åŒ–å¯¹æ¯”å›¾ - 4ä¸ªå­å›¾å±•ç¤ºä¸åŒæŒ‡æ ‡å¯¹æ¯”\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, precision_score, recall_score, \n",
    "    f1_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# 1. å®šä¹‰è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°\n",
    "# =============================================================================\n",
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å®Œæ•´çš„åˆ†ç±»è¯„ä¼°æŒ‡æ ‡\n",
    "    \n",
    "    å‚æ•°:\n",
    "        y_true: çœŸå®æ ‡ç­¾\n",
    "        y_pred: é¢„æµ‹æ ‡ç­¾\n",
    "        y_proba: é¢„æµ‹æ¦‚ç‡ (ç”¨äºè®¡ç®— AUC)\n",
    "    \n",
    "    è¿”å›:\n",
    "        dict: åŒ…å«æ‰€æœ‰æŒ‡æ ‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # æ··æ·†çŸ©é˜µ: [[TN, FP], [FN, TP]]\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    # åŸºç¡€æŒ‡æ ‡\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # çµæ•åº¦ (Sensitivity) = å¬å›ç‡ (Recall) = TP / (TP + FN)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    recall = sensitivity  # å¬å›ç‡ä¸çµæ•åº¦ç›¸åŒ\n",
    "    \n",
    "    # ç‰¹å¼‚æ€§ (Specificity) = TN / (TN + FP)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    # ç²¾ç¡®ç‡ (Precision) = TP / (TP + FP)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    \n",
    "    # F1 åˆ†æ•° = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Gå‡å€¼ (G-Mean) = sqrt(Sensitivity * Specificity)\n",
    "    g_mean = np.sqrt(sensitivity * specificity)\n",
    "    \n",
    "    # AUC (å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹)\n",
    "    auc = roc_auc_score(y_true, y_proba) if y_proba is not None else None\n",
    "    \n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'AUC': auc,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'G-Mean': g_mean,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# 2. è®¡ç®—è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„è¯„ä¼°æŒ‡æ ‡\n",
    "# =============================================================================\n",
    "print(\"=\" * 100)\n",
    "print(\"å¤šæ¨¡å‹åˆ†ç±»è¯„ä¼°æŒ‡æ ‡æ±‡æ€»\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# å­˜å‚¨æ‰€æœ‰ç»“æœ\n",
    "train_results = []\n",
    "test_results = []\n",
    "\n",
    "for model_name, model in final_models.items():\n",
    "    # ===================== è®­ç»ƒé›†è¯„ä¼° =====================\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_train_proba = model.predict_proba(X_train)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    train_metrics = calculate_metrics(y_train, y_train_pred, y_train_proba)\n",
    "    train_metrics['Model'] = model_name\n",
    "    train_results.append(train_metrics)\n",
    "    \n",
    "    # ===================== æµ‹è¯•é›†è¯„ä¼° =====================\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    test_metrics = calculate_metrics(y_test, y_test_pred, y_test_proba)\n",
    "    test_metrics['Model'] = model_name\n",
    "    test_results.append(test_metrics)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. æ˜¾ç¤ºè®­ç»ƒé›†è¯„ä¼°ç»“æœ\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ã€è®­ç»ƒé›†è¯„ä¼°ç»“æœã€‘\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "df_train_results = pd.DataFrame(train_results)\n",
    "# è°ƒæ•´åˆ—é¡ºåº\n",
    "cols_order = ['Model', 'Accuracy', 'AUC', 'Sensitivity', 'Specificity', \n",
    "              'Precision', 'Recall', 'F1 Score', 'G-Mean', 'TP', 'TN', 'FP', 'FN']\n",
    "df_train_results = df_train_results[cols_order]\n",
    "\n",
    "# æŒ‰ AUC é™åºæ’åˆ—\n",
    "df_train_results = df_train_results.sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# æ ¼å¼åŒ–æ˜¾ç¤º\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "print(df_train_results.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 4. æ˜¾ç¤ºæµ‹è¯•é›†è¯„ä¼°ç»“æœ\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ã€æµ‹è¯•é›†è¯„ä¼°ç»“æœã€‘\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "df_test_results = pd.DataFrame(test_results)\n",
    "df_test_results = df_test_results[cols_order]\n",
    "df_test_results = df_test_results.sort_values('AUC', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_test_results.to_string(index=False))\n",
    "\n",
    "# =============================================================================\n",
    "# 5. è¯¦ç»†è¾“å‡ºæ¯ä¸ªæ¨¡å‹çš„æŒ‡æ ‡\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(\"ã€å„æ¨¡å‹è¯¦ç»†è¯„ä¼°æŒ‡æ ‡ã€‘\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for model_name in final_models.keys():\n",
    "    train_row = df_train_results[df_train_results['Model'] == model_name].iloc[0]\n",
    "    test_row = df_test_results[df_test_results['Model'] == model_name].iloc[0]\n",
    "    \n",
    "    print(f\"\\n{'â”€' * 80}\")\n",
    "    print(f\"æ¨¡å‹: {model_name}\")\n",
    "    print(f\"{'â”€' * 80}\")\n",
    "    print(f\"{'æŒ‡æ ‡':<15} {'è®­ç»ƒé›†':>12} {'æµ‹è¯•é›†':>12} {'å·®å¼‚':>12}\")\n",
    "    print(f\"{'â”€' * 51}\")\n",
    "    \n",
    "    metrics_to_show = ['Accuracy', 'AUC', 'Sensitivity', 'Specificity', \n",
    "                       'Precision', 'Recall', 'F1 Score', 'G-Mean']\n",
    "    \n",
    "    for metric in metrics_to_show:\n",
    "        train_val = train_row[metric]\n",
    "        test_val = test_row[metric]\n",
    "        diff = train_val - test_val if train_val is not None and test_val is not None else None\n",
    "        \n",
    "        train_str = f\"{train_val:.4f}\" if train_val is not None else \"N/A\"\n",
    "        test_str = f\"{test_val:.4f}\" if test_val is not None else \"N/A\"\n",
    "        diff_str = f\"{diff:+.4f}\" if diff is not None else \"N/A\"\n",
    "        \n",
    "        print(f\"{metric:<15} {train_str:>12} {test_str:>12} {diff_str:>12}\")\n",
    "    \n",
    "    # æ··æ·†çŸ©é˜µ\n",
    "    print(f\"\\næ··æ·†çŸ©é˜µ (æµ‹è¯•é›†):\")\n",
    "    print(f\"  TP={int(test_row['TP'])}, TN={int(test_row['TN'])}, FP={int(test_row['FP'])}, FN={int(test_row['FN'])}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. ä¿å­˜ç»“æœåˆ° CSV\n",
    "# =============================================================================\n",
    "output_dir = '/home/phl/PHL/Car-T/model_v1/output'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_train_results.to_csv(os.path.join(output_dir, 'train_evaluation_metrics.csv'), index=False)\n",
    "df_test_results.to_csv(os.path.join(output_dir, 'test_evaluation_metrics.csv'), index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100)\n",
    "print(f\"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³:\")\n",
    "print(f\"   - è®­ç»ƒé›†: {os.path.join(output_dir, 'train_evaluation_metrics.csv')}\")\n",
    "print(f\"   - æµ‹è¯•é›†: {os.path.join(output_dir, 'test_evaluation_metrics.csv')}\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. å¯è§†åŒ–ï¼šæŒ‡æ ‡å¯¹æ¯”å›¾\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "models = df_test_results['Model'].tolist()\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "# å›¾1: Accuracy & AUC å¯¹æ¯”\n",
    "ax1 = axes[0, 0]\n",
    "train_acc = df_train_results.set_index('Model').loc[models, 'Accuracy'].values\n",
    "test_acc = df_test_results.set_index('Model').loc[models, 'Accuracy'].values\n",
    "ax1.bar(x - width/2, train_acc, width, label='Train', alpha=0.8)\n",
    "ax1.bar(x + width/2, test_acc, width, label='Test', alpha=0.8)\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Accuracy: Train vs Test')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.set_ylim([0, 1.1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# å›¾2: Sensitivity & Specificity\n",
    "ax2 = axes[0, 1]\n",
    "test_sens = df_test_results.set_index('Model').loc[models, 'Sensitivity'].values\n",
    "test_spec = df_test_results.set_index('Model').loc[models, 'Specificity'].values\n",
    "ax2.bar(x - width/2, test_sens, width, label='Sensitivity', color='#2ca02c', alpha=0.8)\n",
    "ax2.bar(x + width/2, test_spec, width, label='Specificity', color='#d62728', alpha=0.8)\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Sensitivity vs Specificity (Test Set)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax2.legend()\n",
    "ax2.set_ylim([0, 1.1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# å›¾3: Precision & Recall\n",
    "ax3 = axes[1, 0]\n",
    "test_prec = df_test_results.set_index('Model').loc[models, 'Precision'].values\n",
    "test_rec = df_test_results.set_index('Model').loc[models, 'Recall'].values\n",
    "ax3.bar(x - width/2, test_prec, width, label='Precision', color='#9467bd', alpha=0.8)\n",
    "ax3.bar(x + width/2, test_rec, width, label='Recall', color='#ff7f0e', alpha=0.8)\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_title('Precision vs Recall (Test Set)')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.set_ylim([0, 1.1])\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# å›¾4: F1 Score & G-Mean\n",
    "ax4 = axes[1, 1]\n",
    "test_f1 = df_test_results.set_index('Model').loc[models, 'F1 Score'].values\n",
    "test_gmean = df_test_results.set_index('Model').loc[models, 'G-Mean'].values\n",
    "ax4.bar(x - width/2, test_f1, width, label='F1 Score', color='#1f77b4', alpha=0.8)\n",
    "ax4.bar(x + width/2, test_gmean, width, label='G-Mean', color='#17becf', alpha=0.8)\n",
    "ax4.set_ylabel('Score')\n",
    "ax4.set_title('F1 Score vs G-Mean (Test Set)')\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax4.legend()\n",
    "ax4.set_ylim([0, 1.1])\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'evaluation_metrics_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nè¯„ä¼°æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
