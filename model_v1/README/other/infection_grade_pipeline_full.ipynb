{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infection_grade: End-to-End ML Pipeline\n",
    "\n",
    "This notebook implements a complete, reproducible pipeline for predicting `Infection_grade` (binary outcome) using static and dynamic features. It includes:\n",
    "\n",
    "- data loading and patient-level stratified split\n",
    "- dynamic time-series aggregation (Day -15 .. +2)\n",
    "- a no-leak preprocessing pipeline (imputation, encoding, scaling)\n",
    "- Optuna hyperparameter optimization with 5-fold cross-validation and pruning\n",
    "- final training on the full training set and evaluation on an independent test set\n",
    "- bootstrap confidence intervals for metrics\n",
    "- SHAP explanations and visualizations\n",
    "\n",
    "**Before running:** update the `CONFIG` cell with paths to your data and adjust feature lists.\n",
    "\n",
    "本笔记本实现了一个完整的、可复现的流程，用于使用静态和动态特征预测 `Infection_grade`（二元结果）。它包括：\n",
    "- 数据加载和患者层面的分层分割\n",
    "- 动态时间序列聚合（第 -15 天至第 +2 天）\n",
    "- 无泄漏预处理流程（插补、编码、缩放）\n",
    "- 使用 5 折交叉验证和剪枝进行 Optuna 超参数优化\n",
    "- 在完整训练集上进行最终训练，并在独立测试集上进行评估\n",
    "- 指标的自助法置信区间\n",
    "- SHAP 解释和可视化\n",
    "**运行前：**请更新 `CONFIG` 单元格，填写数据路径并调整特征列表。\n",
    "\n",
    "\n",
    "# Infection_grade: End-to-End ML Pipeline with Optuna HPO and SHAP\n",
    "**目的**：为 `Infection_grade`（二分类）构建无泄漏、可复现的端到端机器学习流程。  \n",
    "内容包括数据分割、特征工程（静态 + 动态）、Optuna 超参数优化（5 折 CV）、最终训练、独立测试评估与 SHAP 可解释性分析。\n",
    "\n",
    "**注意**：在运行前请修改 `CONFIG` 区域的文件路径以匹配你本地环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b347006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Dependencies & Config\n",
    "# Install required packages if needed (uncomment to run in a fresh environment)\n",
    "# !pip install pandas numpy scikit-learn lightgbm optuna shap matplotlib seaborn joblib\n",
    "\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, precision_score, recall_score,\n",
    "    f1_score, brier_score_loss, precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# CONFIG - update paths and parameters as needed\n",
    "CONFIG = {\n",
    "    \"STATIC_CSV\": \"data/static/encoded_standardized.csv\",\n",
    "    \"DYNAMIC_DIR\": \"data/dynamic/processed_standardized\",\n",
    "    \"OUTPUT_DIR\": \"artifacts/infection_pipeline\",\n",
    "    \"TEST_SIZE\": 0.30,\n",
    "    \"N_FOLDS\": 5,\n",
    "    \"OPTUNA_TRIALS\": 40,\n",
    "    \"OBS_START\": -15,\n",
    "    \"OBS_END\": 2,\n",
    "    \"N_BOOTSTRAP\": 1000,\n",
    "    \"N_SHAP_SAMPLES\": 300\n",
    "}\n",
    "\n",
    "Path(CONFIG[\"OUTPUT_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('CONFIG loaded. Output dir:', CONFIG['OUTPUT_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities: label binarization, dynamic aggregation, stratified patient split, bootstrap CI\n",
    "\n",
    "These helper functions are used throughout the notebook. Dynamic aggregation converts each patient's time series CSV into aggregated numeric features (mean/std/min/max/count/slope/auc/last)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_label(df: pd.DataFrame, col: str = \"Infection_grade\", threshold: int = 2) -> pd.Series:\n",
    "    \"\"\"Binarize toxicity label: <= threshold -> 0, > threshold -> 1.\"\"\"\n",
    "    return (df[col].astype(float) > threshold).astype(int)\n",
    "\n",
    "\n",
    "def aggregate_dynamic_for_patient(csv_path: str, obs_start: int, obs_end: int) -> Dict[str, float]:\n",
    "    \"\"\"Aggregate a single patient's dynamic CSV into features.\n",
    "    Returns dict of features (may be empty if file missing).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return {}\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'Day' not in df.columns:\n",
    "        df = df.rename(columns={df.columns[0]: 'Day'})\n",
    "    df = df[(df['Day'] >= obs_start) & (df['Day'] <= obs_end)]\n",
    "    features = {}\n",
    "    for col in df.columns:\n",
    "        if col == 'Day':\n",
    "            continue\n",
    "        s = pd.to_numeric(df[col], errors='coerce')\n",
    "        features[f\"{col}_mean\"] = s.mean()\n",
    "        features[f\"{col}_std\"] = s.std()\n",
    "        features[f\"{col}_min\"] = s.min()\n",
    "        features[f\"{col}_max\"] = s.max()\n",
    "        features[f\"{col}_count\"] = s.count()\n",
    "        # last\n",
    "        try:\n",
    "            features[f\"{col}_last\"] = s.dropna().iloc[-1]\n",
    "        except Exception:\n",
    "            features[f\"{col}_last\"] = np.nan\n",
    "        # AUC\n",
    "        try:\n",
    "            x = df['Day'].values\n",
    "            y = s.fillna(method='ffill').fillna(0).values\n",
    "            features[f\"{col}_auc\"] = np.trapz(y, x)\n",
    "        except Exception:\n",
    "            features[f\"{col}_auc\"] = np.nan\n",
    "        # slope\n",
    "        try:\n",
    "            non_na = ~s.isna()\n",
    "            if non_na.sum() >= 2:\n",
    "                xs = df.loc[non_na, 'Day'].values\n",
    "                ys = s.dropna().values\n",
    "                slope = np.polyfit(xs, ys, 1)[0]\n",
    "                features[f\"{col}_slope\"] = slope\n",
    "            else:\n",
    "                features[f\"{col}_slope\"] = np.nan\n",
    "        except Exception:\n",
    "            features[f\"{col}_slope\"] = np.nan\n",
    "    return features\n",
    "\n",
    "\n",
    "def aggregate_dynamic_table(static_df: pd.DataFrame, dynamic_dir: str, obs_start=-15, obs_end=2) -> pd.DataFrame:\n",
    "    \"\"\"Aggregate dynamic features for every patient in static_df['patient_id'].\n",
    "    Returns DataFrame aligned with static_df index containing aggregated features.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    sample_features = {}\n",
    "    # find first existing file for template\n",
    "    for pid in static_df['patient_id']:\n",
    "        p = os.path.join(dynamic_dir, f\"{pid}.csv\")\n",
    "        if os.path.exists(p):\n",
    "            sample_features = aggregate_dynamic_for_patient(p, obs_start, obs_end)\n",
    "            break\n",
    "    feat_keys = list(sample_features.keys())\n",
    "    for pid in static_df['patient_id']:\n",
    "        p = os.path.join(dynamic_dir, f\"{pid}.csv\")\n",
    "        feat = aggregate_dynamic_for_patient(p, obs_start, obs_end)\n",
    "        row = {k: feat.get(k, np.nan) for k in feat_keys}\n",
    "        rows.append(row)\n",
    "    if len(rows) == 0:\n",
    "        return pd.DataFrame(index=static_df.index)\n",
    "    return pd.DataFrame(rows, index=static_df.index)\n",
    "\n",
    "\n",
    "def stratified_patient_split(df: pd.DataFrame, label_col: str, test_size: float = 0.3, seed: int = 42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "    for train_idx, test_idx in sss.split(df, df[label_col].values):\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        test_df = df.iloc[test_idx].reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def bootstrap_ci(metric_fn, y_true, y_score, n_boot=1000, alpha=0.95, seed=42):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    stats = []\n",
    "    n = len(y_true)\n",
    "    for i in range(n_boot):\n",
    "        idx = rng.randint(0, n, n)\n",
    "        try:\n",
    "            val = metric_fn(y_true[idx], y_score[idx])\n",
    "        except Exception:\n",
    "            val = np.nan\n",
    "        stats.append(val)\n",
    "    arr = np.array(stats)\n",
    "    low = np.nanpercentile(arr, (1-alpha)/2*100)\n",
    "    high = np.nanpercentile(arr, (1+alpha)/2*100)\n",
    "    return float(np.nanmean(arr)), float(low), float(high)\n",
    "\n",
    "print('Utility functions defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load static data and basic checks\n",
    "Edit the path in `CONFIG` before running. The static CSV must contain `patient_id` and `Infection_grade` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load static dataframe\n",
    "static_path = CONFIG['STATIC_CSV']\n",
    "if not os.path.exists(static_path):\n",
    "    raise FileNotFoundError(f\"Static CSV not found: {static_path}\")\n",
    "static_df = pd.read_csv(static_path)\n",
    "print('Static shape:', static_df.shape)\n",
    "if 'patient_id' not in static_df.columns:\n",
    "    raise ValueError(\"STATIC CSV must contain 'patient_id' column\")\n",
    "if 'Infection_grade' not in static_df.columns:\n",
    "    raise ValueError(\"STATIC CSV must contain 'Infection_grade' column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize label and drop constant columns\n",
    "We transform `Infection_grade` into binary label: `label = 1` if Infection_grade > 2 else 0. Constant columns (zero variance) are safe to drop before split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize label\n",
    "static_df['label'] = binarize_label(static_df, col='Infection_grade', threshold=2)\n",
    "print('Label distribution:\\n', static_df['label'].value_counts())\n",
    "\n",
    "# Drop constant columns (e.g., disease if all B-NHL)\n",
    "const_cols = [c for c in static_df.columns if static_df[c].nunique() <= 1]\n",
    "if const_cols:\n",
    "    print('Dropping constant columns:', const_cols)\n",
    "    static_df = static_df.drop(columns=const_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define feature types\n",
    "Update the `numeric_cols`, `categorical_cols`, and `ordinal_cols` lists to match your dataset columns. These lists must refer to columns present in the static CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MODIFY these lists to match your dataset columns ===\n",
    "# Example placeholders (replace with your real column names)\n",
    "numeric_cols = [\n",
    "    'age', 'bm_disease_burden'  # replace with real numeric static columns\n",
    "]\n",
    "categorical_cols = [\n",
    "    'sex', 'bridging_therapy'  # replace with real categorical columns\n",
    "]\n",
    "ordinal_cols = [\n",
    "    'ann_arbor_stage'  # replace with real ordinal columns (ensure order when encoding)\n",
    "]\n",
    "\n",
    "# Validate existence\n",
    "all_cols = set(static_df.columns)\n",
    "for lst in (numeric_cols, categorical_cols, ordinal_cols):\n",
    "    for c in lst:\n",
    "        if c not in all_cols:\n",
    "            print(f\"Warning: column {c} not found in static data. Remove or correct list.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient-level stratified split (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = stratified_patient_split(static_df, label_col='label', test_size=CONFIG['TEST_SIZE'], seed=RANDOM_SEED)\n",
    "print('Train shape:', train_df.shape, 'Test shape:', test_df.shape)\n",
    "train_df['label'].value_counts(), test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate dynamic features for train and test\n",
    "This step reads per-patient CSVs and computes aggregated statistics. It may be time-consuming depending on data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aggregating dynamic for train...')\n",
    "train_dyn = aggregate_dynamic_table(train_df, CONFIG['DYNAMIC_DIR'], CONFIG['OBS_START'], CONFIG['OBS_END'])\n",
    "print('Train dynamic shape:', train_dyn.shape)\n",
    "\n",
    "print('Aggregating dynamic for test...')\n",
    "test_dyn = aggregate_dynamic_table(test_df, CONFIG['DYNAMIC_DIR'], CONFIG['OBS_START'], CONFIG['OBS_END'])\n",
    "print('Test dynamic shape:', test_dyn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model input tables\n",
    "Concatenate selected static features and dynamic aggregated features. Keep patient_id and label in separate objects for tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_features = [c for c in (numeric_cols + categorical_cols + ordinal_cols) if c in train_df.columns]\n",
    "X_train_static = train_df[static_features].reset_index(drop=True)\n",
    "X_test_static = test_df[static_features].reset_index(drop=True)\n",
    "\n",
    "X_train = pd.concat([X_train_static.reset_index(drop=True), train_dyn.reset_index(drop=True)], axis=1)\n",
    "X_test = pd.concat([X_test_static.reset_index(drop=True), test_dyn.reset_index(drop=True)], axis=1)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print('X_train shape:', X_train.shape, 'X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a no-leak preprocessing ColumnTransformer\n",
    "- numeric: median imputer + StandardScaler\n",
    "- categorical: most_frequent imputer + OneHotEncoder\n",
    "- ordinal: most_frequent imputer + OrdinalEncoder\n",
    "\n",
    "We treat dynamic aggregated features as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic columns are everything in X_train that's not static features\n",
    "dyn_cols = [c for c in X_train.columns if c not in static_features]\n",
    "numeric_pipeline_cols = [c for c in numeric_cols if c in X_train.columns] + dyn_cols\n",
    "categorical_pipeline_cols = [c for c in categorical_cols if c in X_train.columns]\n",
    "ordinal_pipeline_cols = [c for c in ordinal_cols if c in X_train.columns]\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_transformer = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                                StandardScaler())\n",
    "cat_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                                OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "ord_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                                OrdinalEncoder())\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, numeric_pipeline_cols),\n",
    "        ('cat', cat_transformer, categorical_pipeline_cols),\n",
    "        ('ord', ord_transformer, ordinal_pipeline_cols)\n",
    "    ],\n",
    "    remainder='drop',\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "# Fit preprocessor on training data only (no leakage)\n",
    "preprocessor.fit(X_train)\n",
    "X_train_t = preprocessor.transform(X_train)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "print('Transformed shapes:', X_train_t.shape, X_test_t.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna objective with 5-fold CV and LightGBM\n",
    "We use StratifiedKFold on the training set. The objective returns mean AUPRC across folds. LightGBM's pruner integration is used for early stopping/pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=CONFIG['N_FOLDS'], shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'objective': 'binary',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.2),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n",
    "        'min_split_gain': trial.suggest_loguniform('min_split_gain', 1e-8, 1.0)\n",
    "    }\n",
    "    aucs = []\n",
    "    for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train_t, y_train), 1):\n",
    "        X_tr, X_val = X_train_t[tr_idx], X_train_t[val_idx]\n",
    "        y_tr, y_val = y_train[tr_idx], y_train[val_idx]\n",
    "        dtrain = lgb.Dataset(X_tr, label=y_tr)\n",
    "        dval = lgb.Dataset(X_val, label=y_val, reference=dtrain)\n",
    "        # scale_pos_weight\n",
    "        pos = y_tr.sum()\n",
    "        neg = len(y_tr) - pos\n",
    "        if pos > 0:\n",
    "            param['scale_pos_weight'] = float(neg/pos)\n",
    "        else:\n",
    "            param['scale_pos_weight'] = 1.0\n",
    "        bst = lgb.train(param, dtrain, num_boost_round=2000, valid_sets=[dval],\n",
    "                        early_stopping_rounds=50, verbose_eval=False,\n",
    "                        callbacks=[optuna.integration.LightGBMPruningCallback(trial, 'binary_logloss')])\n",
    "        pred = bst.predict(X_val, num_iteration=bst.best_iteration)\n",
    "        ap = average_precision_score(y_val, pred)\n",
    "        aucs.append(ap)\n",
    "        trial.report(np.mean(aucs), fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    return float(np.mean(aucs))\n",
    "\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED),\n",
    "                            pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=CONFIG['OPTUNA_TRIALS'], show_progress_bar=True)\n",
    "\n",
    "print('Best trial params:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Optuna study and plot simple optimization history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, os.path.join(CONFIG['OUTPUT_DIR'], 'optuna_study.pkl'))\n",
    "joblib.dump(study.best_trial.params, os.path.join(CONFIG['OUTPUT_DIR'], 'best_params.pkl'))\n",
    "\n",
    "# Simple plot of trial values\n",
    "vals = [t.value for t in study.trials if t.value is not None]\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(vals, marker='o')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('AUPRC')\n",
    "plt.title('Optuna: AUPRC per trial')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'optuna_history.png'), dpi=150)\n",
    "plt.close()\n",
    "print('Optuna artifacts saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain final model on full training set using best params and save pipeline\n",
    "We will use LightGBM's sklearn wrapper within a sklearn Pipeline for easier serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_trial.params\n",
    "# Prepare LGBMClassifier with chosen params\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    objective='binary',\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=best.get('num_leaves', 31),\n",
    "    max_depth=best.get('max_depth', -1),\n",
    "    learning_rate=best.get('learning_rate', 0.05),\n",
    "    min_child_samples=best.get('min_child_samples', 20),\n",
    "    subsample=best.get('subsample', 1.0),\n",
    "    colsample_bytree=best.get('colsample_bytree', 1.0),\n",
    "    reg_alpha=best.get('reg_alpha', 0.0),\n",
    "    reg_lambda=best.get('reg_lambda', 0.0),\n",
    "    min_split_gain=best.get('min_split_gain', 0.0)\n",
    ")\n",
    "\n",
    "# set scale_pos_weight on full train\n",
    "pos = y_train.sum(); neg = len(y_train)-pos\n",
    "if pos > 0:\n",
    "    lgbm.set_params(scale_pos_weight=float(neg/pos))\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', lgbm)\n",
    "])\n",
    "\n",
    "# fit final pipeline\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "joblib.dump(final_pipeline, os.path.join(CONFIG['OUTPUT_DIR'], 'final_pipeline.pkl'))\n",
    "print('Final pipeline trained and saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate final model on the independent test set\n",
    "Compute point estimates and bootstrap 95% CI for AUPRC. Save metrics and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = joblib.load(os.path.join(CONFIG['OUTPUT_DIR'], 'final_pipeline.pkl'))\n",
    "probs_test = pipe.predict_proba(X_test)[:, 1]\n",
    "preds_test = (probs_test >= 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    'AUPRC': float(average_precision_score(y_test, probs_test)),\n",
    "    'ROC_AUC': float(roc_auc_score(y_test, probs_test)),\n",
    "    'Precision': float(precision_score(y_test, preds_test, zero_division=0)),\n",
    "    'Recall': float(recall_score(y_test, preds_test, zero_division=0)),\n",
    "    'F1': float(f1_score(y_test, preds_test, zero_division=0)),\n",
    "    'Brier': float(brier_score_loss(y_test, probs_test))\n",
    "}\n",
    "print('Test metrics:', metrics)\n",
    "pd.DataFrame([metrics]).to_csv(os.path.join(CONFIG['OUTPUT_DIR'], 'test_metrics.csv'), index=False)\n",
    "\n",
    "# Bootstrap CI for AUPRC\n",
    "mean_ap, low_ap, high_ap = bootstrap_ci(average_precision_score, np.array(y_test), np.array(probs_test),\n",
    "                                        n_boot=CONFIG['N_BOOTSTRAP'], alpha=0.95, seed=RANDOM_SEED)\n",
    "print(f'AUPRC mean={mean_ap:.4f}, 95% CI=({low_ap:.4f}, {high_ap:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and PR curve plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC\n",
    "fpr, tpr, _ = roc_curve(y_test, probs_test)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f'AUC={metrics[\"ROC_AUC\"]:.3f}')\n",
    "plt.plot([0,1],[0,1],'--', color='grey')\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'roc_curve.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# PR\n",
    "prec, rec, _ = precision_recall_curve(y_test, probs_test)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(rec, prec, label=f'AUPRC={metrics[\"AUPRC\"]:.3f}')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curve'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'pr_curve.png'), dpi=150)\n",
    "plt.close()\n",
    "print('ROC/PR plots saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP explanations (global & feature importance)\n",
    "We use TreeExplainer for LightGBM. To save memory, we explain a random subset of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a subset for SHAP\n",
    "model = final_pipeline.named_steps['classifier']\n",
    "preproc = final_pipeline.named_steps['preprocessor']\n",
    "\n",
    "X_train_proc = preproc.transform(X_train)\n",
    "n_samples = min(CONFIG['N_SHAP_SAMPLES'], X_train_proc.shape[0])\n",
    "idx = np.random.choice(X_train_proc.shape[0], n_samples, replace=False)\n",
    "X_shap = X_train_proc[idx]\n",
    "\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "# Summary plot (dot)\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_shap, show=False)\n",
    "plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'shap_summary.png'), bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Bar plot\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_shap, plot_type='bar', show=False)\n",
    "plt.savefig(os.path.join(CONFIG['OUTPUT_DIR'], 'shap_bar.png'), bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Save mean absolute shap importance\n",
    "mean_abs = np.abs(shap_values).mean(axis=0)\n",
    "# attempt to get feature names from preprocessor\n",
    "def get_feature_names_from_preprocessor(ct: ColumnTransformer):\n",
    "    names = []\n",
    "    for name, trans, cols in ct.transformers_:\n",
    "        if name == 'remainder' and trans == 'drop':\n",
    "            continue\n",
    "        if hasattr(trans, 'named_steps'):\n",
    "            last = trans.named_steps[list(trans.named_steps.keys())[-1]]\n",
    "            if isinstance(last, OneHotEncoder):\n",
    "                ohe = last\n",
    "                names.extend(ohe.get_feature_names_out(cols).tolist())\n",
    "            else:\n",
    "                names.extend(cols)\n",
    "        else:\n",
    "            names.extend(cols)\n",
    "    return names\n",
    "\n",
    "try:\n",
    "    feat_names = get_feature_names_from_preprocessor(preproc)\n",
    "    fi = pd.DataFrame({'feature': feat_names, 'mean_abs_shap': mean_abs})\n",
    "    fi = fi.sort_values('mean_abs_shap', ascending=False)\n",
    "    fi.to_csv(os.path.join(CONFIG['OUTPUT_DIR'], 'shap_feature_importance.csv'), index=False)\n",
    "except Exception:\n",
    "    pd.DataFrame({'idx': list(range(len(mean_abs))), 'mean_abs_shap': mean_abs}).to_csv(os.path.join(CONFIG['OUTPUT_DIR'], 'shap_feat_idx.csv'), index=False)\n",
    "\n",
    "print('SHAP artifacts saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save run metadata and artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump({'config': CONFIG, 'seed': RANDOM_SEED}, os.path.join(CONFIG['OUTPUT_DIR'], 'run_metadata.pkl'))\n",
    "print('All artifacts saved to', CONFIG['OUTPUT_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and next steps\n",
    "- You can adjust OPTUNA_TRIALS to control HPO time/cost.\n",
    "- If classes are extremely imbalanced consider alternative metrics or resampling inside CV folds.\n",
    "- For productionize, extract modular parts into scripts under `split/`, `train/`, `eval/`, `explain/`.\n",
    "\n",
    "---\n",
    "\n",
    "End of notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
