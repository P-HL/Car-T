{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f72ed4",
   "metadata": {},
   "source": [
    "#### Bç«™ä¸´åºŠé¢„æµ‹æ¨¡å‹å‚è€ƒï¼š\n",
    "https://www.bilibili.com/video/BV1TDRGYfEVm/?spm_id_from=333.1387.collection.video_card.click&vd_source=fe18fc7331f8146ed53da453b38c3226\n",
    "\n",
    "#### å…¶ä»–å®˜æ–¹å®ç°ä»£ç ï¼š\n",
    "- scikit-learnä¸­æ–‡ç¤¾åŒºï¼šhttps://scikit-learn.org.cn/\n",
    "- ã€å¸ƒå®¢ã€‘sklearn ä¸­æ–‡ç¿»è¯‘ï¼šhttps://sklearn.apachecn.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ee7464",
   "metadata": {},
   "source": [
    "--------------------\n",
    "ä»£ç ç›®æ ‡æ˜¯å®ç°æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€è°ƒä¼˜ã€è¯„ä¼°ä»¥åŠç»“æœçš„å¯è§†åŒ–åˆ†æã€‚\n",
    "é¡¹ç›®ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š\n",
    "- æ•°æ®åŠ è½½ä¸é¢„å¤„ç†ï¼š\n",
    "é¦–å…ˆä»Excelæ–‡ä»¶ä¸­åŠ è½½äº†åŒ…å«å¤šç‰¹å¾çš„åŒ»å­¦æ•°æ®ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨dfä¸­ï¼Œä½¿ç”¨LightGBMæ¨¡å‹è¿›è¡Œç‰¹å¾çš„é‡è¦æ€§æ’åºã€‚é€šè¿‡è®­ç»ƒLGBMæ¨¡å‹å¹¶è·å–ç‰¹å¾é‡è¦æ€§ï¼ˆfeature_importances_ï¼‰ï¼Œå¯¹ç‰¹å¾è¿›è¡Œæ’åºï¼Œå¹¶é€‰å–å‰30ä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œæœ€åæ ¹æ®LGBMæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§ï¼Œé€‰æ‹©äº†å‰8ä¸ªç‰¹å¾è¿›è¡Œæ¨¡å‹è®­ç»ƒã€‚è¿™äº›ç‰¹å¾æ˜¯ï¼šX30.X 39.X 46.X 32, X 34ï¼ŒX 33.X 9. X 28.ç›®æ ‡å˜é‡æ˜¯yï¼ˆå³åˆ†ç±»æ ‡ç­¾ï¼‰ï¼Œå¹¶å°†ç‰¹å¾å’Œç›®æ ‡å˜é‡åˆ†å¼€å­˜å‚¨ï¼Œè¿›è¡Œåç»­çš„æ¨¡å‹è®­ç»ƒ\n",
    "- æ¨¡å‹è®­ç»ƒä¸è°ƒä¼˜ï¼š\n",
    "ä½¿ç”¨å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ŒåŒ…æ‹¬äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ã€å†³ç­–æ ‘ï¼ˆDTï¼‰ã€æé™éšæœºæ ‘ï¼ˆETï¼‰ã€æ¢¯åº¦æå‡æœºï¼ˆGBMï¼‰ã€Kè¿‘é‚»ï¼ˆKNNï¼‰ã€LightGBMï¼ˆLGBMï¼‰ã€éšæœºæ£®æ—ï¼ˆRFï¼‰ã€æ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰å’ŒXGBoostï¼ˆXGBï¼‰ã€‚ä¸ºæ¯ä¸ªæ¨¡å‹å®šä¹‰äº†ä¸åŒçš„è¶…å‚æ•°ï¼Œå¹¶é€šè¿‡GridSearchCVè¿›è¡Œç½‘æ ¼æœç´¢ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„å‚æ•°é…å·²ç½®ï¼Œæ¯ä¸ªæ¨¡å‹ä½¿ç”¨5æŠ˜äº¤å‰éªŒè¯æ¥è¯„ä¼°å…¶æ€§èƒ½ï¼Œç¡®ä¿æ¨¡å‹åœ¨ä¸åŒæ•°æ®å­é›†ä¸Šçš„è¡¨ç°ç¨³å®š\n",
    "- æ¨¡å‹è¯„ä¼°ä¸æ€§èƒ½å¯¹æ¯”ï¼š\n",
    "å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œè®¡ç®—å¸¸è§çš„åˆ†ç±»æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å‡†ç¡®ç‡ã€æ•æ„Ÿæ€§ï¼ˆå¬å›ç‡ï¼‰ã€ç‰¹å¼‚æ€§ã€ç²¾ç¡®åº¦ï¼ˆé˜³æ€§é¢„æµ‹å€¼ï¼‰ã€è´Ÿæ€§é¢„æµ‹å€¼ã€F1åˆ†æ•°ä»¥åŠKappaç³»æ•°ç­‰ï¼Œç»˜åˆ¶å¤šä¸ªå›¾è¡¨è¿›è¡Œæ¨¡å‹æ€§èƒ½å¯¹æ¯”ï¼ŒåŒ…æ‹¬ROCæ›²çº¿ã€Precision-Recallæ›²çº¿ç­‰ï¼Œä»¥å±•ç¤ºä¸åŒæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°\n",
    "- æ¨¡å‹ç»“æœçš„å¯è§†åŒ–ï¼š\n",
    "å¯¹æœ€ä¼˜æ¨¡å‹ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯ExtraTreesClassifierï¼‰è¿›è¡Œäº†æ·±å…¥çš„å¯è§†åŒ–åˆ†æã€‚é€šè¿‡SHAPå€¼è§£é‡Šæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹ï¼Œå±•ç¤ºç‰¹å¾çš„é‡è¦æ€§å’Œå¯¹æ¯ä¸ªæ ·æœ¬é¢„æµ‹çš„å½±å“ï¼Œä½¿ç”¨PCAè¿›è¡Œé™ç»´ï¼Œå°†æ•°æ®æŠ•å½±åˆ°ä¸€ç»´ç©ºé—´ï¼Œå¹¶ç»˜åˆ¶æ•£ç‚¹å›¾å±•ç¤ºæ¯ä¸ªæ•°æ®ç‚¹åœ¨é™ç»´ç©ºé—´ä¸­çš„åˆ†å¸ƒï¼ŒåŒæ—¶æ ¹æ®é¢„æµ‹çš„æ¦‚ç‡å€¼å¯¹æ•°æ®ç‚¹è¿›è¡Œé¢œè‰²ç¼–ç ï¼ŒåŒºåˆ†ä¸åŒç±»åˆ«ï¼Œç»˜åˆ¶äº†å¤šä¸ªSHAPå›¾ï¼ˆå¦‚æ¡å½¢å›¾ã€æ•£ç‚¹å›¾ã€ç€‘å¸ƒå›¾ç­‰ï¼‰ï¼Œè¿™äº›å›¾è¡¨å¸®åŠ©è§£é‡Šæ¨¡å‹çš„å†³ç­–ä¾æ®å’Œå„ç‰¹å¾å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6e6713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•ä¸€ï¼šç‹¬çƒ­ç¼–ç ï¼ˆOne-Hot Encodingï¼‰\n",
    "# ç‹¬çƒ­ç¼–ç æ˜¯ä¸€ç§å°†ç±»åˆ«æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‹æ•°æ®çš„æ–¹æ³•ã€‚å¯¹äºæ¯ä¸ªç±»åˆ«æ•°æ®ï¼Œæˆ‘ä»¬ä¸ºå…¶åˆ›å»ºä¸€ä¸ªæ–°çš„äºŒè¿›åˆ¶åˆ—ï¼Œå¹¶æ ‡è®°è¯¥åˆ—çš„å€¼ä¸º1ã€‚å…¶ä»–åˆ—çš„å€¼å‡ä¸º0ã€‚è¿™æ ·ï¼Œæ¯ä¸ªç±»åˆ«çš„å”¯ä¸€å€¼éƒ½ä¼šè¢«è¡¨ç¤ºä¸ºä¸€ä¸ªç‹¬çƒ­å‘é‡ã€‚\n",
    "# ä¾‹å¦‚ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåä¸ºâ€œæœˆä»½â€çš„åˆ—ï¼ŒåŒ…å«ä»¥ä¸‹ç±»åˆ«æ•°æ®ï¼šâ€™1æœˆâ€™ã€â€™2æœˆâ€™ã€â€™3æœˆâ€™ã€â€™4æœˆâ€™ã€â€™5æœˆâ€™ã€â€™6æœˆâ€™ã€â€™7æœˆâ€™ã€â€™8æœˆâ€™ã€â€™9æœˆâ€™ã€â€™10æœˆâ€™ã€â€™11æœˆâ€™å’Œâ€™12æœˆâ€™ã€‚\n",
    "# æˆ‘ä»¬å¯ä»¥ä¸ºæ¯ä¸ªæœˆä»½åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶åˆ—ï¼Œä¾‹å¦‚ï¼š\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "data = {'month': ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ'], 'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "# ä½¿ç”¨LabelEncoderè¿›è¡Œæ ‡ç­¾ç¼–ç \n",
    "le = LabelEncoder()\n",
    "le_fit = le.fit(df['month']) # Fit the LabelEncoder to the month column data.\n",
    "label_encoded = le_fit.transform(df['month']) # Transform the month column data.\n",
    "print(label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66868b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ–¹æ³•äºŒï¼šæ ‡ç­¾ç¼–ç ï¼ˆLabel Encodingï¼‰\n",
    "# æ ‡ç­¾ç¼–ç æ˜¯ä¸€ç§ç®€å•åœ°å°†ç±»åˆ«æ•°æ®è½¬æ¢ä¸ºæ•°å€¼å‹æ•°æ®çš„æ–¹æ³•ã€‚å®ƒä¸ºæ¯ä¸ªå”¯ä¸€ç±»åˆ«åˆ†é…ä¸€ä¸ªæ•´æ•°ã€‚é€šå¸¸ç”¨äºç±»åˆ«æ•°é‡ä¸å¤šçš„æƒ…å†µã€‚\n",
    "# Pythonä»£ç å®ç°å¦‚ä¸‹ï¼š\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# ç¤ºä¾‹æ•°æ®\n",
    "data = {'month': ['1æœˆ', '2æœˆ', '3æœˆ', '4æœˆ', '5æœˆ', '6æœˆ', '7æœˆ', '8æœˆ', '9æœˆ', '10æœˆ', '11æœˆ', '12æœˆ'], 'value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}\n",
    "df = pd.DataFrame(data)\n",
    "# ä½¿ç”¨OneHotEncoderè¿›è¡Œç‹¬çƒ­ç¼–ç \n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ohe_fit = ohe.fit(df[['month']])\n",
    "one_hot_encoded = ohe_fit.transform(df[['month']])\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b050c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f96e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfd4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 0.è®¾ç½®å·¥ä½œè·¯å¾„\n",
    "import os\n",
    "\n",
    "# è®¾ç½®å·¥ä½œè·¯å¾„\n",
    "os.chdir(\"/home/phl/PHL/Car-T/model-1\")\n",
    "\n",
    "# è·å–å¹¶æ‰“å°å½“å‰å·¥ä½œè·¯å¾„\n",
    "current_path = os.getcwd()\n",
    "print(\"å½“å‰å·¥ä½œè·¯å¾„ï¼š\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465e736",
   "metadata": {},
   "outputs": [],
   "source": [
    "### é’ˆå¯¹åŠ¨æ€æ•°æ®ç‰¹å®šæ—¶é—´çª—å£ï¼ˆDay 0-14ï¼‰çš„ç¼ºå¤±å€¼åˆ†æä»£ç ï¼ˆgeminiç”Ÿæˆï¼‰\n",
    "\n",
    "# é”å®šæ—¶é—´çª—å£ï¼šä»…ç­›é€‰ 0 <= Day <= 14 çš„æ•°æ®ã€‚\n",
    "# è®¡ç®—è¦†ç›–ç‡ï¼šç»Ÿè®¡æ¯ä¸ªæŒ‡æ ‡åœ¨è¯¥çª—å£å†…æœ‰å¤šå°‘æ‚£è€…æ‹¥æœ‰è‡³å°‘ä¸€æ¬¡æœ‰æ•ˆè§‚æµ‹ã€‚\n",
    "# è¯„ä¼°ç¼ºå¤±ç‡ï¼šåŸºäºæ€»æ‚£è€…æ•°è®¡ç®—ç¼ºå¤±æ¯”ä¾‹ï¼Œå¹¶æ ¹æ® 20% çš„é˜ˆå€¼ç»™å‡ºä¿ç•™/å‰”é™¤å»ºè®®ã€‚\n",
    "\n",
    "# åŠ¨æ€æ•°æ®ç¼ºå¤±å€¼åˆ†æ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# 1. è®¾ç½®è·¯å¾„ä¸åŠ è½½åŸºç¡€æ•°æ®\n",
    "# =============================================================================\n",
    "# å¼•ç”¨ split.ipynb ä¸­çš„è·¯å¾„é…ç½®\n",
    "static_path = \"/home/phl/PHL/Car-T/model-1/B-NHL_reindexed_example/csv/B-NHL_static_data.csv\"\n",
    "dynamic_dir = \"/home/phl/PHL/Car-T/model-1/B-NHL_reindexed_example/processed\"\n",
    "\n",
    "# è¯»å–é™æ€æ•°æ®ä»¥è·å–åˆæ³•çš„æ‚£è€… ID åˆ—è¡¨\n",
    "df_static = pd.read_csv(static_path)\n",
    "\n",
    "# å¤ç”¨ split.ipynb çš„æ¸…æ´—é€»è¾‘ï¼Œç¡®ä¿åˆ†æçš„æ‚£è€…ç¾¤ä½“ä¸åç»­å»ºæ¨¡ä¸€è‡´\n",
    "# å‰”é™¤ AAS (åˆ†æœŸ) ä¸ºç©ºçš„æ‚£è€…\n",
    "if 'AAS' in df_static.columns:\n",
    "    df_static = df_static.dropna(subset=['AAS'])\n",
    "# å‰”é™¤ Infection (ç›®æ ‡å˜é‡) ä¸ºç©ºçš„æ‚£è€…\n",
    "if 'Infection' in df_static.columns:\n",
    "    df_static = df_static.dropna(subset=['Infection'])\n",
    "\n",
    "valid_patient_ids = df_static['ID'].unique()\n",
    "total_patients = len(valid_patient_ids)\n",
    "print(f\"åˆ†æçº³å…¥æ‚£è€…æ€»æ•°: {total_patients}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. åŠ¨æ€æ•°æ®ç¼ºå¤±å€¼åˆ†æé€»è¾‘\n",
    "# =============================================================================\n",
    "# åˆ†æå‚æ•°è®¾ç½®\n",
    "START_DAY = -15\n",
    "END_DAY = 14\n",
    "MISSING_THRESHOLD = 0.20  # 20% ç¼ºå¤±ç‡é˜ˆå€¼\n",
    "\n",
    "print(f\"æ­£åœ¨åˆ†æåŠ¨æ€æ•°æ®çª—å£: Day {START_DAY} è‡³ Day {END_DAY} ...\")\n",
    "\n",
    "# ç”¨äºå­˜å‚¨æ¯ä¸ªå˜é‡åœ¨å¤šå°‘ä¸ªç—…äººä¸­å‡ºç°è¿‡ï¼ˆè‡³å°‘æœ‰ä¸€æ¬¡éç©ºè§‚æµ‹ï¼‰\n",
    "variable_presence_count = {} \n",
    "\n",
    "for pid in tqdm(valid_patient_ids):\n",
    "    file_path = os.path.join(dynamic_dir, f\"{pid}.csv\")\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df_dyn = pd.read_csv(file_path)\n",
    "            \n",
    "            # æ ‡å‡†åŒ–æ—¶é—´åˆ—åå¤„ç†\n",
    "            time_col = None\n",
    "            for col in ['Day', 'day', 'Time', 'time']:\n",
    "                if col in df_dyn.columns:\n",
    "                    time_col = col\n",
    "                    break\n",
    "            \n",
    "            if time_col:\n",
    "                # 1. æ—¶é—´çª—å£èšç„¦ï¼šä»…ä¿ç•™ Day 0 åˆ° Day 14 çš„æ•°æ®\n",
    "                # ä¾æ®ï¼šæ’é™¤ä¸ªä½“å·®å¼‚è¾ƒå¤§çš„è¿œæœŸæ•°æ®ï¼Œèšç„¦æ²»ç–—åå‰ä¸¤å‘¨çš„å…³é”®æœŸ\n",
    "                mask = (df_dyn[time_col] >= START_DAY) & (df_dyn[time_col] <= END_DAY)\n",
    "                df_window = df_dyn[mask]\n",
    "                \n",
    "                if not df_window.empty:\n",
    "                    # 2. éå†è¯¥æ‚£è€…çš„æ‰€æœ‰å˜é‡\n",
    "                    for col in df_window.columns:\n",
    "                        # æ’é™¤ ID, æ—¶é—´åˆ— å’Œ è„æ•°æ®åˆ—(Unnamed)\n",
    "                        if col in [time_col, 'ID'] or 'Unnamed' in col:\n",
    "                            continue\n",
    "                        \n",
    "                        # 3. æ£€æŸ¥æœ‰æ•ˆæ€§ï¼šå¦‚æœè¯¥å˜é‡åœ¨è¿™ä¸ªçª—å£å†…è‡³å°‘æœ‰ä¸€ä¸ªéç©ºå€¼ï¼Œåˆ™è§†ä¸º\"å­˜åœ¨\"\n",
    "                        # (æ³¨ï¼šè¿™é‡Œæš‚ä¸å¼ºåˆ¶è¦æ±‚æ¯3-4å¤©ä¸€æ¬¡çš„é¢‘ç‡ï¼Œåªè¦çª—å£å†…æœ‰æ•°æ®å³è§†ä¸ºå¯ç”¨)\n",
    "                        if df_window[col].notna().any():\n",
    "                            variable_presence_count[col] = variable_presence_count.get(col, 0) + 1\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"è¯»å–æ‚£è€… {pid} æ•°æ®å¤±è´¥: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. ç”Ÿæˆåˆ†ææŠ¥å‘Š\n",
    "# =============================================================================\n",
    "analysis_results = []\n",
    "\n",
    "for var, count in variable_presence_count.items():\n",
    "    missing_count = total_patients - count\n",
    "    missing_rate = missing_count / total_patients\n",
    "    \n",
    "    analysis_results.append({\n",
    "        'Variable': var,\n",
    "        'Present_Patients': count,\n",
    "        'Missing_Patients': missing_count,\n",
    "        'Missing_Rate': missing_rate,\n",
    "        'Status': 'Keep' if missing_rate <= MISSING_THRESHOLD else 'Drop'\n",
    "    })\n",
    "\n",
    "# è½¬æ¢ä¸º DataFrame å¹¶æ’åº\n",
    "df_analysis = pd.DataFrame(analysis_results)\n",
    "if not df_analysis.empty:\n",
    "    df_analysis = df_analysis.sort_values('Missing_Rate', ascending=True)\n",
    "\n",
    "    # åˆ’åˆ†ä¿ç•™å’Œå‰”é™¤çš„å˜é‡\n",
    "    vars_to_keep = df_analysis[df_analysis['Status'] == 'Keep']\n",
    "    vars_to_drop = df_analysis[df_analysis['Status'] == 'Drop']\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"ã€åŠ¨æ€å˜é‡ç¼ºå¤±å€¼åˆ†ææŠ¥å‘Šã€‘\")\n",
    "    print(f\"æ—¶é—´çª—å£: Day {START_DAY} - {END_DAY}\")\n",
    "    print(f\"ç¼ºå¤±ç‡é˜ˆå€¼: {MISSING_THRESHOLD:.0%}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"æ£€æµ‹åˆ°çš„åŠ¨æ€å˜é‡æ€»æ•°: {len(df_analysis)}\")\n",
    "    print(f\"å»ºè®®ä¿ç•™å˜é‡æ•° (Missing <= 20%): {len(vars_to_keep)}\")\n",
    "    print(f\"å»ºè®®å‰”é™¤å˜é‡æ•° (Missing > 20%): {len(vars_to_drop)}\")\n",
    "\n",
    "    print(\"\\n>>> å»ºè®®ä¿ç•™çš„é«˜è´¨é‡å˜é‡ (Top 10):\")\n",
    "    print(vars_to_keep[['Variable', 'Missing_Rate', 'Present_Patients']].head(10).to_string(index=False))\n",
    "\n",
    "    print(\"\\n>>> å»ºè®®å‰”é™¤çš„é«˜ç¼ºå¤±å˜é‡ (Top 10):\")\n",
    "    print(vars_to_drop[['Variable', 'Missing_Rate', 'Present_Patients']].tail(10).to_string(index=False))\n",
    "    \n",
    "    # æå–å»ºè®®ä¿ç•™çš„å˜é‡ååˆ—è¡¨ï¼Œä¾›åç»­ä½¿ç”¨\n",
    "    suggested_features = vars_to_keep['Variable'].tolist()\n",
    "    print(f\"\\n[ä»£ç æç¤º] åç»­ç‰¹å¾æå–æ—¶ï¼Œå»ºè®®ä»…æå–ä»¥ä¸‹ {len(suggested_features)} ä¸ªå˜é‡:\")\n",
    "    print(suggested_features)\n",
    "else:\n",
    "    print(\"æœªæ£€æµ‹åˆ°ä»»ä½•æœ‰æ•ˆçš„åŠ¨æ€å˜é‡ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„æˆ–æ—¶é—´çª—å£è®¾ç½®ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e74bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§å®˜æ–¹å®ç°å‚è€ƒï¼šhttps://scikit-learn.org.cn/view/255.htmlï¼ˆç½®æ¢é‡è¦æ€§ä¸éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§(MDI)ï¼‰\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "rng = np.random.RandomState(seed=42)\n",
    "X['random_cat'] = rng.randint(3, size=X.shape[0])\n",
    "X['random_num'] = rng.randn(X.shape[0])\n",
    "\n",
    "categorical_columns = ['pclass', 'sex', 'embarked', 'random_cat']\n",
    "numerical_columns = ['age', 'sibsp', 'parch', 'fare', 'random_num']\n",
    "\n",
    "X = X[categorical_columns + numerical_columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=42)\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "numerical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    [('cat', categorical_pipe, categorical_columns),\n",
    "     ('num', numerical_pipe, numerical_columns)])\n",
    "\n",
    "rf = Pipeline([\n",
    "    ('preprocess', preprocessing),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(\"RF train accuracy: %0.3f\" % rf.score(X_train, y_train))\n",
    "print(\"RF test accuracy: %0.3f\" % rf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "ohe = (rf.named_steps['preprocess']\n",
    "         .named_transformers_['cat']\n",
    "         .named_steps['onehot'])\n",
    "feature_names = ohe.get_feature_names_out(input_features=categorical_columns)\n",
    "feature_names = np.r_[feature_names, numerical_columns]\n",
    "\n",
    "tree_feature_importances = (\n",
    "    rf.named_steps['classifier'].feature_importances_)\n",
    "sorted_idx = tree_feature_importances.argsort()\n",
    "\n",
    "y_ticks = np.arange(0, len(feature_names))\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "ax.set_yticklabels(feature_names[sorted_idx])\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_title(\"Random Forest Feature Importances (MDI)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47909e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®˜æ–¹å®ç°Spearmanç§©åºç›¸å…³æ€§ä¸ç½®æ¢é‡è¦æ€§ï¼šhttps://scikit-learn.org.cn/view/254.htmlï¼ˆå…·æœ‰å¤šé‡å…±çº¿æ€§æˆ–ç›¸å…³ç‰¹å¾çš„ç½®æ¢é‡è¦æ€§ï¼‰\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# é¦–å…ˆï¼Œåœ¨ä¹³è…ºç™Œæ•°æ®é›†ä¸Šè®­ç»ƒéšæœºæ£®æ—ï¼Œå¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å…¶å‡†ç¡®æ€§ï¼š\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy on test data: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "# å…¶æ¬¡ï¼Œç»˜åˆ¶åŸºäºæ ‘çš„ç‰¹å¾é‡è¦æ€§å’Œç½®æ¢é‡è¦æ€§ã€‚\n",
    "# ç½®æ¢é‡è¦æ€§å›¾æ˜¾ç¤ºï¼Œå˜æ¢ä¸€ä¸ªç‰¹å¾æœ€å¤šé™ä½äº†0.012çš„å‡†ç¡®æ€§ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰ä»»ä½•ä¸€ä¸ªç‰¹å¾æ˜¯é‡è¦çš„ã€‚\n",
    "# è¿™ä¸ä¸Šé¢è®¡ç®—çš„é«˜ç²¾åº¦æµ‹è¯•æ˜¯çŸ›ç›¾çš„ï¼šæŸäº›ç‰¹æ€§å¿…é¡»æ˜¯é‡è¦çš„ã€‚åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç½®æ¢çš„é‡è¦æ€§ï¼Œä»¥æ˜¾ç¤ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹æ¯ä¸ªç‰¹å¾çš„ä¾èµ–ç¨‹åº¦ã€‚\n",
    "result = permutation_importance(clf, X_train, y_train, n_repeats=10,\n",
    "                                random_state=42)\n",
    "perm_sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "tree_importance_sorted_idx = np.argsort(clf.feature_importances_)\n",
    "tree_indices = np.arange(0, len(clf.feature_importances_)) + 0.5\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.barh(tree_indices,\n",
    "         clf.feature_importances_[tree_importance_sorted_idx], height=0.7)\n",
    "ax1.set_yticklabels(data.feature_names[tree_importance_sorted_idx])\n",
    "ax1.set_yticks(tree_indices)\n",
    "ax1.set_ylim((0, len(clf.feature_importances_)))\n",
    "ax2.boxplot(result.importances[perm_sorted_idx].T, vert=False,\n",
    "            labels=data.feature_names[perm_sorted_idx])\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# å¤„ç†å¤šçº¿æ€§ç‰¹å¾\n",
    "# é€šè¿‡å¯¹Spearmanç§©åºç›¸å…³æ€§æ‰§è¡Œåˆ†å±‚èšç±»ï¼Œé€‰æ‹©ä¸€ä¸ªé˜ˆå€¼ï¼Œå¹¶ä»æ¯ä¸ªèšç±»ä¸­ä¿ç•™ä¸€ä¸ªç‰¹å¾\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "corr = spearmanr(X).correlation\n",
    "corr_linkage = hierarchy.ward(corr)\n",
    "dendro = hierarchy.dendrogram(corr_linkage, labels=data.feature_names, ax=ax1,\n",
    "                              leaf_rotation=90)\n",
    "dendro_idx = np.arange(0, len(dendro['ivl']))\n",
    "\n",
    "ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']])\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "ax2.set_xticklabels(dendro['ivl'], rotation='vertical')\n",
    "ax2.set_yticklabels(dendro['ivl'])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aad7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ˜¾ç¤ºå¯¹è±¡çš„å¯è§†åŒ–â€”â€”æ„é€ æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿https://scikit-learn.org.cn/view/273.html\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml(data_id=1464, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), LogisticRegression(random_state=0))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ä½¿ç”¨æ‹Ÿåˆçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè®¡ç®—æ¨¡å‹çš„é¢„æµ‹ã€‚ è¿™äº›é¢„æµ‹ç”¨äºè®¡ç®—ä½¿ç”¨ConfusionMatrixDisplayç»˜åˆ¶çš„æ··æ·†çŸ©é˜µã€‚\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "# rocæ›²çº¿éœ€è¦ä¼°ç®—å™¨çš„æ¦‚ç‡æˆ–éé˜ˆå€¼å†³ç­–å€¼ã€‚ ç”±äºé€»è¾‘å›å½’æä¾›äº†å†³ç­–å‡½æ•°ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥ç»˜åˆ¶rocæ›²çº¿\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "y_score = clf.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score, pos_label=clf.classes_[1])\n",
    "roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot()\n",
    "\n",
    "# ä½¿ç”¨é¢„ç½®éƒ¨åˆ†ä¸­çš„y_scoreç»˜åˆ¶ç²¾åº¦å¬å›æ›²çº¿\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_score,\n",
    "                                         pos_label=clf.classes_[1])\n",
    "pr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot()\n",
    "\n",
    "\n",
    "# å°†æ˜¾ç¤ºå¯¹è±¡åˆå¹¶ä¸ºä¸€ä¸ªå›¾\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "roc_display.plot(ax=ax1)\n",
    "pr_display.plot(ax=ax2)\n",
    "plt.show()\n",
    "\n",
    "# # ç¤ºä¾‹ï¼šç»˜åˆ¶ SVM çš„æ··æ·†çŸ©é˜µå®˜æ–¹å®ç°ï¼šhttps://scikit-learn.org.cn/view/273.html\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# metrics_train = calculate_metrics(svm, X_train, y_train, label='SVM')\n",
    "# cm_display = ConfusionMatrixDisplay(metrics_train['Confusion_Matrix']).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafb3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¸¦äº¤å‰éªŒè¯çš„ROCæ›²çº¿https://scikit-learn.org.cn/view/289.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "# from sklearn.metrics import plot_roc_curve  # æ—§ç‰ˆæœ¬ä»£ç ï¼Œå·²ç§»é™¤\n",
    "from sklearn.metrics import RocCurveDisplay   # æ–°ç‰ˆæœ¬å¯¼å…¥æ–¹å¼\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# #############################################################################\n",
    "# è·å–æ•°æ®\n",
    "\n",
    "# å¯¼å…¥å¾…å¤„ç†çš„æ•°æ®\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X, y = X[y != 2], y[y != 2]\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# å¢åŠ ä¸€äº›å™ªéŸ³ç‰¹å¾\n",
    "random_state = np.random.RandomState(0)\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# #############################################################################\n",
    "# åˆ†ç±»ï¼Œå¹¶ä½¿ç”¨ROCæ›²çº¿åˆ†æç»“æœ\n",
    "\n",
    "# ä½¿ç”¨äº¤å‰éªŒè¯è¿è¡Œåˆ†ç±»å™¨å¹¶ç»˜åˆ¶ROCæ›²çº¿\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = svm.SVC(kernel='linear', probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "    classifier.fit(X[train], y[train])\n",
    "    # === ä¿®æ”¹å¼€å§‹ ===\n",
    "    # æ—§ä»£ç : \n",
    "#     viz = plot_roc_curve(classifier, X[test], y[test],\n",
    "#                          name='ROC fold {}'.format(i),\n",
    "#                          alpha=0.3, lw=1, ax=ax)\n",
    "\n",
    "    # æ–°ä»£ç : ä½¿ç”¨ RocCurveDisplay.from_estimator\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier, \n",
    "        X[test], \n",
    "        y[test],\n",
    "        name='ROC fold {}'.format(i),\n",
    "        alpha=0.3, \n",
    "        lw=1, \n",
    "        ax=ax\n",
    "    )\n",
    "    # === ä¿®æ”¹ç»“æŸ ===\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "        label='Chance', alpha=.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "        lw=2, alpha=.8)\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "       title=\"Receiver operating characteristic example\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# éªŒè¯æ›²çº¿ï¼šç»˜åˆ¶åˆ†æ•°ä»¥è¯„ä¼°æ¨¡å‹https://scikit-learn.org.cn/view/116.html#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "param_range = np.logspace(-6, -1, 5)\n",
    "train_scores, test_scores = validation_curve(\n",
    "    SVC(), X, y, param_name=\"gamma\", param_range=param_range,\n",
    "    scoring=\"accuracy\", n_jobs=1)\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Validation Curve with SVM\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c310d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lassoå›å½’ç­›é€‰å˜é‡å‚è€ƒï¼šhttps://www.bilibili.com/video/BV1mQ4y1A7os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "xlsx1_filePath =\"/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/csv/B-NHL_static_data.csv\"\n",
    "xlsx2_filePath ='/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/processed/1.csv'\n",
    "data_1 = pd.read_csv(xlsx1_filePath)\n",
    "data_2 = pd.read_csv(xlsx2_filePath)\n",
    "rows_1, _ = data_1.shape\n",
    "rows_2, _ = data_2.shape\n",
    "data_1.insert(0, 'label', [0] * rows_1)\n",
    "data_2.insert(0, 'label', [1] * rows_2)\n",
    "data = pd.concat([data_1, data_2])\n",
    "data = shuffle(data)\n",
    "data = data.fillna(0)\n",
    "X = data.iloc[:, 1:]\n",
    "y = data['label']\n",
    "ColNames = X.columns\n",
    "# X = X.astype(np.float64)\n",
    "X = StandardScaler().fit_transform(X)  # æ–°çŸ¥è¯†ç‚¹\n",
    "X = pd.DataFrame(X)\n",
    "X.columns = ColNames\n",
    "\n",
    "\n",
    "alphas = np.logspace(-3,1,50)\n",
    "model_lassoCv = LassoCV(alphas = alphas,cv = 10,max_iter = 100000).fit(X,y)\n",
    "\n",
    "print(model_lassoCv.alpha_)\n",
    "coef = pd.Series(model_lassoCv.coef_, index = X.columns) # æ–°çŸ¥è¯†ç‚¹\n",
    "print(\"Lasso picked \"+ str(sum(coef != 0))+\" variables and eliminated the other \"+ str(sum(coef == 0)))\n",
    "\n",
    "\n",
    "index = coef[coef != 0].index\n",
    "X = X[index]\n",
    "# X.head()\n",
    "print(coef[coef != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527600fb",
   "metadata": {},
   "source": [
    "### å‰è¨€\n",
    "Borutaç®—æ³•æ˜¯ä¸€ç§ç”¨äºç‰¹å¾é€‰æ‹©çš„åŒ…è£¹å¼ç®—æ³•ï¼Œä¸“é—¨è®¾è®¡ç”¨äºç¡®å®šæ•°æ®é›†ä¸­å“ªäº›ç‰¹å¾å¯¹é¢„æµ‹æ¨¡å‹æ˜¯é‡è¦çš„\n",
    "### Borutaç®—æ³•åŸç†\n",
    "```\n",
    "1. æ„å»ºéšæœºæ£®æ—æ¨¡å‹ï¼šé¦–å…ˆï¼ŒBorutaç®—æ³•ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹æ¥è®¡ç®—æ¯ä¸ªç‰¹å¾çš„é‡è¦æ€§ï¼Œéšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ„å»ºå¤šä¸ªå†³ç­–æ ‘å¹¶å–å¹³å‡å€¼æ¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§\n",
    "2. åˆ›å»ºå½±å­ç‰¹å¾ï¼šä¸ºäº†è¯„ä¼°åŸå§‹ç‰¹å¾çš„é‡è¦æ€§ï¼ŒBorutaç®—æ³•ä¼šå¯¹æ•°æ®è¿›è¡Œæ‰“ä¹±ï¼Œç”Ÿæˆä¸€ç»„ä¸åŸå§‹ç‰¹å¾å¯¹åº”çš„å½±å­ç‰¹å¾ï¼Œè¿™äº›å½±å­ç‰¹å¾æ˜¯é€šè¿‡éšæœºæ’åˆ—åŸå§‹ç‰¹å¾çš„æ•°æ®ç”Ÿæˆçš„ï¼Œå®ƒä»¬ä¸åº”è¯¥å¯¹ç›®æ ‡å˜é‡æœ‰é¢„æµ‹èƒ½åŠ›\n",
    "3. æ¯”è¾ƒç‰¹å¾é‡è¦æ€§ï¼šç®—æ³•å°†åŸå§‹ç‰¹å¾çš„é‡è¦æ€§ä¸å½±å­ç‰¹å¾çš„é‡è¦æ€§è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœä¸€ä¸ªåŸå§‹ç‰¹å¾çš„é‡è¦æ€§æ˜¾è‘—é«˜äºæ‰€æœ‰å½±å­ç‰¹å¾çš„æœ€å¤§é‡è¦æ€§ï¼Œé‚£ä¹ˆå®ƒè¢«è®¤ä¸ºæ˜¯â€œé‡è¦çš„â€ï¼›å¦‚æœå®ƒçš„é‡è¦æ€§ä½äºå½±å­ç‰¹å¾çš„æœ€å¤§é‡è¦æ€§ï¼Œåˆ™è¢«è®¤ä¸ºæ˜¯â€œæ— å…³çš„â€ï¼Œå¯¹äºé‚£äº›æ— æ³•æ˜æ˜¾åˆ¤æ–­çš„é‡è¦æ€§ï¼Œç®—æ³•ä¼šå°†å…¶æ ‡è®°ä¸ºâ€œå¾…å®šâ€\n",
    "4. é€æ­¥æ¶ˆé™¤æ— å…³ç‰¹å¾ï¼šç®—æ³•é€šè¿‡è¿­ä»£çš„æ–¹å¼ï¼Œé€æ­¥å‰”é™¤é‚£äº›è¢«æ ‡è®°ä¸ºâ€œæ— å…³â€çš„ç‰¹å¾ï¼Œç„¶åé‡æ–°æ„å»ºæ¨¡å‹ï¼Œç›´åˆ°æ‰€æœ‰ç‰¹å¾éƒ½è¢«åˆ†ç±»ä¸ºâ€œé‡è¦â€æˆ–â€œæ— å…³â€ï¼Œæˆ–è€…è¾¾åˆ°è®¾å®šçš„è¿­ä»£æ¬¡æ•°\n",
    "5. è¾“å‡ºç»“æœï¼šæœ€ç»ˆï¼ŒBorutaç®—æ³•è¾“å‡ºä¸‰ç±»ç‰¹å¾â€”â€”é‡è¦çš„ã€æ— å…³çš„å’Œå¾…å®šçš„ï¼Œé‡è¦ç‰¹å¾å¯ä»¥ä¿ç•™ç”¨äºæ¨¡å‹æ„å»ºï¼Œè€Œæ— å…³ç‰¹å¾åˆ™å¯ä»¥è¢«èˆå¼ƒï¼Œå¯¹äºå¾…å®šçš„ç‰¹å¾ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„åˆ†ææˆ–é€‰æ‹©æ€§ä¿ç•™\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borutaç‰¹å¾é€‰æ‹©ï¼šåŸºäºéšæœºæ£®æ—çš„Borutaç®—æ³•åº”ç”¨\n",
    "# ä»£ç å‚è€ƒï¼šhttps://mp.weixin.qq.com/s/wX1B_2wxp3kUVjQKQE5XLw\n",
    "# è®²è§£å‚è€ƒï¼šhttps://www.bilibili.com/video/BV1T5sVeMEGC/\n",
    "\n",
    "# =================================================================================\n",
    "# 1.æ•°æ®è¯»å–å¤„ç†\n",
    "# è¯»å–æ•°æ®ï¼Œå°†å…¶åˆ†ä¸ºç‰¹å¾ï¼ˆXï¼‰å’Œç›®æ ‡å˜é‡ï¼ˆyï¼‰ï¼Œç„¶åå°†æ•°æ®é›†æŒ‰80%è®­ç»ƒé›†å’Œ20%æµ‹è¯•é›†è¿›è¡Œåˆ’åˆ†ï¼Œä½¿ç”¨çš„æ˜¯ä¸€ä¸ªå¿ƒè„ç”µå¤å¾‹çš„æ•°æ®é›†åŒ…å«46ä¸ªç‰¹å¾å˜é‡ä¸€ä¸ªç›®æ ‡å˜é‡ä¸ºäºŒåˆ†ç±»ä»»åŠ¡\n",
    "# =================================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/csv/B-NHL_static_data.csv')\n",
    "# åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡å˜é‡\n",
    "X = df.drop(['Infection'], axis=1)\n",
    "y = df['Infection']\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df['Infection'])\n",
    "df.head()\n",
    "\n",
    "# =================================================================================\n",
    "# 2.Borutaç®—æ³•ç‰¹å¾é€‰æ‹©\n",
    "# åˆå§‹åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨æ¨¡å‹ç”¨äºè¯„ä¼°ç‰¹å¾é‡è¦æ€§ï¼Œå¹¶é€šè¿‡Borutaç‰¹å¾é€‰æ‹©å™¨ä¸å½±å­ç‰¹å¾è¿›è¡Œå¯¹æ¯”ï¼Œç¡®å®šå“ªäº›ç‰¹å¾æ˜¯é‡è¦çš„ï¼Œéšåå¯¹è®­ç»ƒæ•°æ®åº”ç”¨Borutaç®—æ³•ï¼Œä»¥ç­›é€‰å‡ºå¯¹æ¨¡å‹é¢„æµ‹æœ€æœ‰ç”¨çš„ç‰¹å¾ï¼ŒBorutaç®—æ³•é»˜è®¤åœ¨å†…éƒ¨è¿­ä»£100æ¬¡ï¼Œå¦‚éœ€æ›´æ”¹è¿­ä»£æ¬¡æ•°ï¼Œå¯é€šè¿‡æ·»åŠ å‚æ•°max_iterè¿›è¡Œè°ƒæ•´\n",
    "# =================================================================================\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "# åˆå§‹åŒ–éšæœºæ£®æ—æ¨¡å‹\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "# åˆå§‹åŒ–Borutaç‰¹å¾é€‰æ‹©å™¨\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=42)\n",
    "# å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "boruta_selector.fit(X_train.values, y_train.values)\n",
    "# æ£€æŸ¥é€‰ä¸­çš„ç‰¹å¾\n",
    "selected_features = X_train.columns[boruta_selector.support_].to_list()\n",
    "# æ‰“å°è¢«é€‰æ‹©çš„ç‰¹å¾\n",
    "print(\"Selected Features: \", selected_features)\n",
    "# æ‰“å°è¢«å‰”é™¤çš„ç‰¹å¾\n",
    "rejected_features = X_train.columns[~boruta_selector.support_].to_list()\n",
    "print(\"Rejected Features: \", rejected_features)\n",
    "# æ‰“å°æœ‰å¾…å®šæ€§çš„ç‰¹å¾\n",
    "tentative_features = X_train.columns[boruta_selector.support_weak_].to_list()\n",
    "print(\"Tentative Features: \", tentative_features)\n",
    "\n",
    "\n",
    "# è·å–ç‰¹å¾æ’å\n",
    "# ä½¿ç”¨boruta_selector.ranking_è·å–æ¯ä¸ªç‰¹å¾çš„æ’åï¼Œè¿™ä¸ªæ’åè¡¨ç¤ºäº†ç‰¹å¾çš„é‡è¦æ€§ï¼Œæ•°å€¼è¶Šå°ä»£è¡¨ç‰¹å¾è¶Šé‡è¦ï¼Œè¿™é‡Œåªå±•ç¤ºéƒ¨åˆ†ç‰¹å¾ï¼Œæœ€åç¡®å®šé‡è¦çš„ç‰¹å¾ä¸º['Type_of_atrial_fibrillation', 'BMI', 'Left_atrial_diameter', 'Systolic_blood_pressure', 'NtproBNP']\n",
    "# feature_ranks = boruta_selector.ranking_\n",
    "# å°†ç‰¹å¾åç§°å’Œæ’åç»“åˆæˆä¸€ä¸ªDataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Rank': feature_ranks\n",
    "})\n",
    "feature_importance_df\n",
    "\n",
    "\n",
    "# =================================================================================\n",
    "# 3.å¤šæ¬¡è¿è¡ŒBorutaç®—æ³•ä»¥è¯„ä¼°ç‰¹å¾æ’åç¨³å®šæ€§\n",
    "# å¤šæ¬¡è¿è¡ŒBorutaç®—æ³•ï¼Œä»¥ä¸åŒçš„éšæœºç§å­ç”Ÿæˆç‰¹å¾æ’åï¼Œå¹¶å°†æ¯æ¬¡çš„æ’åç»“æœä¿å­˜åˆ°ä¸€ä¸ªDataFrameä¸­ï¼Œç”¨äºåˆ†æç‰¹å¾æ’åçš„ä¸€è‡´æ€§ï¼Œå¹¶ä¸”æŒ‡å®šmax_iter=50ä¸åœ¨æ˜¯é»˜è®¤çš„100ï¼Œæé«˜è¿è¡Œé€Ÿåº¦\n",
    "# =================================================================================\n",
    "# åˆå§‹åŒ–éšæœºæ£®æ—æ¨¡å‹\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "# åˆå§‹åŒ–å­˜å‚¨ç‰¹å¾æ’åçš„ DataFrame\n",
    "ranking_df = pd.DataFrame(index=range(1, 21), columns=X_train.columns)\n",
    "\n",
    "# è¿è¡Œ Boruta 20 æ¬¡\n",
    "for i in range(20):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    \n",
    "    # åˆå§‹åŒ–Borutaç‰¹å¾é€‰æ‹©å™¨\n",
    "    boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=i, max_iter=50)\n",
    "    \n",
    "    # å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œç‰¹å¾é€‰æ‹©\n",
    "    boruta_selector.fit(X_train.values, y_train.values)\n",
    "    \n",
    "    # è·å–ç‰¹å¾æ’å\n",
    "    feature_ranks = boruta_selector.ranking_\n",
    "    \n",
    "    # å°†ç‰¹å¾æ’åä¿å­˜åˆ° DataFrame ä¸­\n",
    "    ranking_df.loc[i+1] = feature_ranks\n",
    "ranking_df\n",
    "\n",
    "# =================================================================================\n",
    "# å¯è§†åŒ–æ’åç¨³å®šæ€§\n",
    "# ä»å›¾è¡¨ä¸­å¯ä»¥çœ‹å‡ºï¼Œæœ€å·¦ä¾§çš„ç‰¹å¾ï¼ˆType_of_atrial_fibrillationã€BMIç­‰ï¼‰åœ¨å¤šæ¬¡è¿­ä»£ä¸­æ’åè¾ƒé«˜ï¼Œè¡¨æ˜å®ƒä»¬æ˜¯æ¨¡å‹ä¸­è¾ƒä¸ºé‡è¦çš„ç‰¹å¾ï¼Œè€Œæœ€å³ä¾§çš„ç‰¹å¾ï¼ˆå¦‚COPDã€Amiodaroneç­‰ï¼‰åˆ™åœ¨å¤šæ¬¡è¿­ä»£ä¸­æ’åè¾ƒä½ï¼Œè¯´æ˜å®ƒä»¬åœ¨æ¨¡å‹ä¸­è¢«è®¤ä¸ºä¸å¤ªé‡è¦ï¼Œä¸å‰é¢å•æ¬¡è¿è¡ŒBorutaç®—æ³•çš„ç»“æœä¸€è‡´ï¼Œä½†æ˜¯ç›¸æ¯”äºå•æ¬¡è¿è¡Œï¼Œé€šè¿‡è¿™ä¸ªå¯è§†åŒ–å¯ä»¥å‘ç°æŸäº›ç‰¹å¾åœ¨ä¸ªåˆ«è¿­ä»£ä¸­çš„é‡è¦æ€§æ’åä¸å¤§å¤šæ•°è¿­ä»£ç»“æœä¸åŒï¼Œä»è€Œåˆ¤æ–­è¿™äº›ç‰¹å¾çš„é‡è¦æ€§æ˜¯å¦ç¨³å®šï¼Œå›¾ä¸­ä½äºé¡»çº¿å¤–çš„ç‚¹æ­£æ˜¯ä»£è¡¨äº†è¿™äº›å¼‚å¸¸çš„æ’åç»“æœï¼Œå¦‚æœè¿‡å¤šè¿™ç§æƒ…å†µå‡ºç°ä½œè€…è®¤ä¸ºéœ€è¦ç€é‡å»è€ƒè™‘è¿™ç§ç‰¹å¾\n",
    "# =================================================================================\n",
    "# ç¡®ä¿æ•°æ®é›†ä¸­åªæœ‰æ•°å€¼åˆ—\n",
    "numeric_ranking_df = ranking_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# è®¡ç®—æ¯ä¸ªç‰¹å¾çš„ä¸­ä½æ•°\n",
    "median_values = numeric_ranking_df.median()\n",
    "\n",
    "# æ ¹æ®ä¸­ä½æ•°å¯¹åˆ—è¿›è¡Œæ’åº\n",
    "sorted_columns = median_values.sort_values().index\n",
    "\n",
    "# è®¾ç½®ç»˜å›¾é£æ ¼\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ç»˜åˆ¶ç®±çº¿å›¾\n",
    "sns.boxplot(data=numeric_ranking_df[sorted_columns], palette=\"Greens\")\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Sorted Feature Ranking Distribution by Boruta\", fontsize=16)\n",
    "plt.xlabel(\"Attributes\", fontsize=14)\n",
    "plt.ylabel(\"Importance\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680cd53",
   "metadata": {},
   "source": [
    "# Borutashap\n",
    "\n",
    "- ä»£ç é“¾æ¥ï¼šhttps://github.com/Ekeany/Boruta-Shap/tree/master\n",
    "- å¯¹åº”å«ä¹‰ï¼šhttps://blog.csdn.net/gitblog_00754/article/details/155299350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f27799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ¨¡å— 5_Borutashap: Using Shap and Basic Random Forest\n",
    "# =============================================================================\n",
    "\n",
    "from BorutaShap import BorutaShap, load_data\n",
    "  \n",
    "X, y = load_data(data_type='regression')\n",
    "X.head()\n",
    "\n",
    "# no model selected default is Random Forest, if classification is True it is a Classification problem\n",
    "Feature_Selector = BorutaShap(importance_measure='shap',\n",
    "                              classification=False)\n",
    "\n",
    "'''\n",
    "Sample: Boolean\n",
    "\tif true then a rowise sample of the data will be used to calculate the feature importance values\n",
    "\n",
    "sample_fraction: float\n",
    "\tThe sample fraction of the original data used in calculating the feature importance values only\n",
    "        used if Sample==True.\n",
    "\n",
    "train_or_test: string\n",
    "\tDecides whether the feature improtance should be calculated on out of sample data see the dicussion here.\n",
    "        https://slds-lmu.github.io/iml_methods_limitations/pfi-data.html\n",
    "\n",
    "normalize: boolean\n",
    "            if true the importance values will be normalized using the z-score formula\n",
    "\n",
    "verbose: Boolean\n",
    "\ta flag indicator to print out all the rejected or accepted features.\n",
    "'''\n",
    "Feature_Selector.fit(X=X, y=y, n_trials=100, sample=False,\n",
    "            \t     train_or_test = 'test', normalize=True,\n",
    "\t\t     verbose=True)\n",
    "\n",
    "# Returns Boxplot of features\n",
    "Feature_Selector.plot(which_features='all')\n",
    "\n",
    "\n",
    "# Returns a subset of the original data with the selected features\n",
    "subset = Feature_Selector.Subset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe6164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ¨¡å— 5_Borutashap: Using BorutaShap with another model XGBoost\n",
    "# =============================================================================\n",
    "from BorutaShap import BorutaShap, load_data\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X, y = load_data(data_type='classification')\n",
    "X.head()\n",
    "\n",
    "\n",
    "model = XGBClassifier()\n",
    "\n",
    "# if classification is False it is a Regression problem\n",
    "Feature_Selector = BorutaShap(model=model,\n",
    "                              importance_measure='shap',\n",
    "                              classification=True)\n",
    "\n",
    "Feature_Selector.fit(X=X, y=y, n_trials=100, sample=False,\n",
    "            \t     train_or_test = 'test', normalize=True,\n",
    "\t\t     verbose=True)\n",
    "\n",
    "# Returns Boxplot of features\n",
    "Feature_Selector.plot(which_features='all')\n",
    "\n",
    "# Returns a subset of the original data with the selected features\n",
    "subset = Feature_Selector.Subset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1e050d",
   "metadata": {},
   "source": [
    "## è¶…å‚æ•°ä¼˜åŒ–\n",
    "https://cloud.baidu.com/article/5601757\n",
    "\n",
    "### ä¸€ã€è¶…å‚æ•°ä¼˜åŒ–ï¼šæœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½æå‡çš„æ ¸å¿ƒå¼•æ“\n",
    "åœ¨æœºå™¨å­¦ä¹ æ¨¡å‹å¼€å‘è¿‡ç¨‹ä¸­ï¼Œè¶…å‚æ•°ä¼˜åŒ–æ˜¯è¿æ¥ç®—æ³•ç†è®ºä¸å®é™…æ€§èƒ½çš„å…³é”®æ¡¥æ¢ã€‚ä¸åŒäºé€šè¿‡æ•°æ®è®­ç»ƒè‡ªåŠ¨è°ƒæ•´çš„æ¨¡å‹å‚æ•°ï¼ˆå¦‚ç¥ç»ç½‘ç»œæƒé‡ï¼‰ï¼Œè¶…å‚æ•°éœ€åœ¨è®­ç»ƒå‰é¢„å…ˆè®¾å®šï¼Œç›´æ¥å½±å“æ¨¡å‹çš„å­¦ä¹ èƒ½åŠ›ä¸æ³›åŒ–æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå†³ç­–æ ‘çš„æ·±åº¦ã€æ”¯æŒå‘é‡æœºçš„æ ¸å‡½æ•°ç±»å‹ã€ç¥ç»ç½‘ç»œçš„å­¦ä¹ ç‡ä¸å±‚æ•°ç­‰ï¼Œå‡å±äºéœ€è¦äººå·¥è°ƒä¼˜çš„è¶…å‚æ•°èŒƒç•´ã€‚\n",
    "\n",
    "è¶…å‚æ•°ä¼˜åŒ–çš„æœ¬è´¨æ˜¯é€šè¿‡ç³»ç»ŸåŒ–æ¢ç´¢è¶…å‚æ•°ç©ºé—´ï¼Œå¯»æ‰¾ä½¿æ¨¡å‹åœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè¡¨ç°æœ€ä¼˜çš„å‚æ•°ç»„åˆã€‚è¿™ä¸€è¿‡ç¨‹ä¸ä»…èƒ½æ˜¾è‘—æå‡æ¨¡å‹ç²¾åº¦ï¼Œè¿˜å¯é¿å…å› å‚æ•°è®¾ç½®ä¸å½“å¯¼è‡´çš„è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆé—®é¢˜ã€‚ä»¥å›¾åƒåˆ†ç±»ä»»åŠ¡ä¸ºä¾‹ï¼Œé€šè¿‡ä¼˜åŒ–å·ç§¯ç¥ç»ç½‘ç»œçš„è¶…å‚æ•°ï¼ˆå¦‚æ»¤æ³¢å™¨æ•°é‡ã€æ­¥é•¿ã€æ­£åˆ™åŒ–ç³»æ•°ï¼‰ï¼Œæ¨¡å‹åœ¨CIFAR-10æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å¯ä»75%æå‡è‡³89%ã€‚\n",
    "\n",
    "### äºŒã€ç½‘æ ¼æœç´¢ï¼šç³»ç»ŸåŒ–ç©·ä¸¾çš„ç»å…¸æ–¹æ³•\n",
    "1. åŸç†ä¸å®ç°é€»è¾‘\n",
    "ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰é€šè¿‡å®šä¹‰è¶…å‚æ•°çš„å€™é€‰å€¼é›†åˆï¼Œæ„å»ºæ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆï¼Œå¹¶é€ä¸€è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œè‹¥éœ€ä¼˜åŒ–å­¦ä¹ ç‡ï¼ˆå–å€¼ä¸º{0.01, 0.001, 0.0001}ï¼‰å’Œæ­£åˆ™åŒ–ç³»æ•°ï¼ˆå–å€¼ä¸º{0.1, 0.01, 0.001}ï¼‰ï¼Œåˆ™éœ€è®­ç»ƒ3Ã—3=9ä¸ªæ¨¡å‹ï¼Œæœ€ç»ˆé€‰æ‹©éªŒè¯é›†ä¸ŠæŸå¤±æœ€å°çš„ç»„åˆã€‚\n",
    "\n",
    "2. ä¼˜ç¼ºç‚¹åˆ†æ\n",
    "- ä¼˜åŠ¿ï¼š\n",
    "    - è¦†ç›–å…¨é¢ï¼Œé¿å…é—æ¼æ½œåœ¨æœ€ä¼˜è§£\n",
    "    - å®ç°ç®€å•ï¼Œé€‚ç”¨äºå‚æ•°ç©ºé—´è¾ƒå°ï¼ˆ<10ä¸ªå‚æ•°ï¼‰æˆ–ç¦»æ•£å€¼è¾ƒå¤šçš„åœºæ™¯\n",
    "- å±€é™ï¼š\n",
    "    - è®¡ç®—æˆæœ¬éšå‚æ•°æ•°é‡å‘ˆæŒ‡æ•°å¢é•¿ï¼ˆå¦‚10ä¸ªå‚æ•°ï¼Œæ¯ä¸ªå–3ä¸ªå€¼ï¼Œéœ€è¯„ä¼°3^10=59,049æ¬¡ï¼‰\n",
    "    - å¯¹è¿ç»­å‚æ•°éœ€é¢„å…ˆç¦»æ•£åŒ–ï¼Œå¯èƒ½é”™è¿‡æœ€ä¼˜åŒºé—´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64197e",
   "metadata": {},
   "source": [
    "### å››ã€è´å¶æ–¯ä¼˜åŒ–ï¼šæ™ºèƒ½å¯¼å‘çš„å…ˆè¿›æ–¹æ³•\n",
    "1. åŸç†ä¸æ ¸å¿ƒé€»è¾‘\n",
    "è´å¶æ–¯ä¼˜åŒ–ï¼ˆBayesian Optimizationï¼‰é€šè¿‡æ„å»ºç›®æ ‡å‡½æ•°ï¼ˆå¦‚æ¨¡å‹å‡†ç¡®ç‡ï¼‰çš„æ¦‚ç‡ä»£ç†æ¨¡å‹ï¼ˆé€šå¸¸ä¸ºé«˜æ–¯è¿‡ç¨‹ï¼‰ï¼Œç»“åˆé‡‡é›†å‡½æ•°ï¼ˆå¦‚Expected Improvementï¼‰åŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªè¯„ä¼°ç‚¹ã€‚å…¶æ ¸å¿ƒä¼˜åŠ¿åœ¨äºâ€œè®°å¿†æ€§â€ï¼šæ¯æ¬¡è¯„ä¼°åæ›´æ–°ä»£ç†æ¨¡å‹ï¼Œå¼•å¯¼åç»­æœç´¢å‘é«˜æ½œåŠ›åŒºåŸŸé›†ä¸­ã€‚\n",
    "\n",
    "2. ä¼˜ç¼ºç‚¹åˆ†æ\n",
    "- ä¼˜åŠ¿ï¼š\n",
    "    - è®¡ç®—æ•ˆç‡æé«˜ï¼Œå°¤å…¶é€‚ç”¨äºè¯„ä¼°æˆæœ¬é«˜æ˜‚çš„åœºæ™¯ï¼ˆå¦‚æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼‰\n",
    "    - è‡ªåŠ¨å¹³è¡¡æ¢ç´¢ï¼ˆæ–°åŒºåŸŸï¼‰ä¸åˆ©ç”¨ï¼ˆå·²çŸ¥é«˜æ½œåŠ›åŒºåŸŸï¼‰\n",
    "- å±€é™ï¼š\n",
    "    - ä»£ç†æ¨¡å‹æ„å»ºå¤æ‚ï¼Œéœ€è°ƒæ•´è¶…å‚æ•°ï¼ˆå¦‚é«˜æ–¯è¿‡ç¨‹çš„æ ¸å‡½æ•°ï¼‰\n",
    "    - åˆå§‹é˜¶æ®µå¯èƒ½å› ä»£ç†æ¨¡å‹ä¸å‡†ç¡®è€Œé™·å…¥å±€éƒ¨æœ€ä¼˜\n",
    "\n",
    "\n",
    "3. æ–¹æ³•ï¼šé€‚ç”¨åœºæ™¯â€”â€”è®¡ç®—æ•ˆç‡â€”â€”å®ç°å¤æ‚åº¦\n",
    "- ç½‘æ ¼æœç´¢ï¼šä½ç»´ç¦»æ•£å‚æ•°ç©ºé—´â€”â€”ä½â€”â€”ä½\n",
    "- éšæœºæœç´¢ï¼šä¸­é«˜ç»´å‚æ•°ç©ºé—´ï¼Œè®¡ç®—èµ„æºæœ‰é™â€”â€”ä¸­â€”â€”ä¸­\n",
    "- è´å¶æ–¯ä¼˜åŒ–ï¼šé«˜ç»´è¿ç»­å‚æ•°ç©ºé—´ï¼Œè¯„ä¼°æˆæœ¬é«˜â€”â€”é«˜â€”â€”é«˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a2175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‚è€ƒï¼šhttps://cloud.baidu.com/article/5601757\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# å®šä¹‰å‚æ•°æœç´¢ç©ºé—´\n",
    "search_spaces = {\n",
    "    'C': Real(0.1, 10, prior='log-uniform'),\n",
    "    'gamma': Real(1e-4, 1e-1, prior='log-uniform'),\n",
    "    'kernel': Categorical(['rbf', 'poly']),\n",
    "    'degree': Integer(2, 5)\n",
    "}\n",
    "# åˆ›å»ºè´å¶æ–¯ä¼˜åŒ–å¯¹è±¡ï¼ˆè¯„ä¼°50æ¬¡ï¼‰\n",
    "bayes_search = BayesSearchCV(\n",
    "    SVC(), search_spaces, n_iter=50, cv=5, scoring='accuracy'\n",
    ")\n",
    "bayes_search.fit(X_train, y_train)\n",
    "print(\"æœ€ä¼˜å‚æ•°:\", bayes_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8796925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# å‚è€ƒï¼šhttps://mp.weixin.qq.com/s/lvkn2PKv8AobZAsdpF9xjg\n",
    "# ä¸€ã€ç¯å¢ƒå‡†å¤‡ä¸å¯è§†åŒ–é…ç½®\n",
    "# é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥æ‰€éœ€çš„åº“ï¼Œå¹¶è®¾ç½® Nature é£æ ¼é…è‰² ä¸ç»˜å›¾å‚æ•°ï¼Œä»¥ä¿è¯è¾“å‡ºç»“æœåœ¨å­¦æœ¯è®ºæ–‡ä¸å±•ç¤ºä¸­å…·æœ‰é¡¶åˆŠæ°´å‡†ã€‚\n",
    "# è¿™æ ·é…ç½®åï¼Œæ¨¡å‹ç»“æœå›¾ï¼ˆå¦‚ROCæ›²çº¿ã€æ··æ·†çŸ©é˜µï¼‰ä¼šæ›´ç¬¦åˆå›½é™…æœŸåˆŠçš„å±•ç¤ºé£æ ¼ã€‚\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import numpy as np\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical, Real\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®é¡¶åˆŠé…è‰²ä¸å­—ä½“\n",
    "nature_colors = ['#E64B35', '#4DBBD5', '#00A087', '#3C5488', '#F39B7F']\n",
    "sns.set_palette(nature_colors)\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--', 'axes.edgecolor': '0.3'})\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 600\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "plt.rcParams['savefig.format'] = 'jpeg'\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“‚ äºŒã€æ•°æ®å‡†å¤‡ä¸åˆ’åˆ†\n",
    "# è¯»å–æ ·æœ¬æ•°æ®ï¼Œæå–ç‰¹å¾å’Œæ ‡ç­¾ï¼Œå¹¶åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†ã€‚åŒæ—¶ï¼Œå°†æ•°æ®é›†å¯¼å‡ºï¼Œä»¥ä¾¿åç»­åˆ†æã€‚\n",
    "# æ­¤æ­¥éª¤ä¿è¯æ•°æ®åˆ’åˆ†çš„ç‹¬ç«‹æ€§ä¸å¯è¿½æº¯æ€§ï¼Œé¿å…æ•°æ®ç»“æœä¸å¯å¤ç°ã€‚\n",
    "# =============================================================================\n",
    "# å¯¼å…¥CSVæ•°æ®\n",
    "data = pd.read_csv(\"sample.csv\")\n",
    "\n",
    "# ç‰¹å¾ä¸æ ‡ç­¾\n",
    "X = data.drop(columns=[\"label\"])\n",
    "y = data[\"label\"]\n",
    "\n",
    "# åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# å¯¼å‡ºæ•°æ®\n",
    "train_data = X_train.copy(); train_data['label'] = y_train\n",
    "train_data.to_csv('train_dataset.csv', index=False)\n",
    "test_data = X_test.copy(); test_data['label'] = y_test\n",
    "test_data.to_csv('test_dataset.csv', index=False)\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ” ä¸‰ã€è´å¶æ–¯ä¼˜åŒ–æœç´¢ç©ºé—´è®¾ç½®\n",
    "# ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬åœ¨éšæœºæ£®æ—çš„å‚æ•°ç©ºé—´ä¸­è®¾ç½®äº†è¾ƒä¸ºä¿å®ˆçš„èŒƒå›´ï¼Œå¹¶ä½¿ç”¨ BayesSearchCV è‡ªåŠ¨æœç´¢æœ€ä½³ç»„åˆã€‚\n",
    "# è¿™é‡Œç‰¹åˆ«åŠ å…¥äº† max_samples ä¸ ccp_alphaï¼ˆå¤æ‚åº¦å‰ªæï¼‰ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ³›åŒ–æ€§èƒ½ã€‚\n",
    "# =============================================================================\n",
    "# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 200),     \n",
    "    'max_depth': Integer(3, 15),          \n",
    "    'min_samples_split': Integer(10, 30), \n",
    "    'min_samples_leaf': Integer(5, 20),   \n",
    "    'max_features': Categorical(['sqrt', 'log2']),\n",
    "    'bootstrap': Categorical([True]),     \n",
    "    'max_samples': Real(0.6, 0.9),        \n",
    "    'ccp_alpha': Real(0.0, 0.01)          \n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# âš¡ å››ã€BayesSearchCVè¶…å‚æ•°ä¼˜åŒ–\n",
    "# é€šè¿‡è´å¶æ–¯ä¼˜åŒ–+äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬åœ¨åˆç†çš„è¿­ä»£æ¬¡æ•°å†…æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆï¼Œå¹¶ä¿å­˜ä¼˜åŒ–è¿‡ç¨‹ã€‚\n",
    "# è¿™ä¸€æ­¥è®©éšæœºæ£®æ—åœ¨å‚æ•°è°ƒä¼˜è¿‡ç¨‹ä¸­é¿å…äº†ç›²ç›®æœç´¢ï¼Œæå‡äº†æ•ˆç‡å’Œæ¨¡å‹çš„é²æ£’æ€§ã€‚\n",
    "# =============================================================================\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "# æ‰§è¡ŒBayesSearchCV\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=rf,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "bayes_search.fit(X_train, y_train)\n",
    "print(f\"æœ€ä½³è¶…å‚æ•°: {bayes_search.best_params_}\")\n",
    "print(f\"æœ€ä½³äº¤å‰éªŒè¯ROC AUC: {bayes_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“Š äº”ã€æ¨¡å‹è®­ç»ƒä¸æ€§èƒ½è¯„ä¼°\n",
    "# ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹ï¼Œå¹¶åœ¨è®­ç»ƒé›†ä¸æµ‹è¯•é›†ä¸Šåˆ†åˆ«è®¡ç®—å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡ã€F1ã€ROC AUCç­‰æŒ‡æ ‡ï¼ŒåŒæ—¶è¿›è¡Œè¿‡æ‹Ÿåˆåˆ†æã€‚\n",
    "# é€šè¿‡å¯¹æ¯”è®­ç»ƒé›†ä¸æµ‹è¯•é›†è¡¨ç°ï¼Œå¯ä»¥ç›´è§‚åˆ¤æ–­æ¨¡å‹æ˜¯å¦å­˜åœ¨è¿‡æ‹Ÿåˆã€‚\n",
    "# =============================================================================\n",
    "# ä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒæ¨¡å‹\n",
    "best_params = bayes_search.best_params_\n",
    "rf_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# æµ‹è¯•é›†é¢„æµ‹\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "y_test_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "print(f\"Testing Accuracy: {test_accuracy:.4f}, ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸ“ˆ å…­ã€ç»“æœå¯è§†åŒ–ï¼šROCä¸æ··æ·†çŸ©é˜µ\n",
    "# ç»˜åˆ¶è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„ ROCæ›²çº¿ï¼Œå¹¶è¾“å‡ºæ··æ·†çŸ©é˜µï¼Œå…¨é¢å±•ç¤ºæ¨¡å‹åˆ†ç±»æ•ˆæœã€‚\n",
    "# =============================================================================\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color=nature_colors[1], lw=2.5, label=f'Test ROC (AUC={test_roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Random Forest (Bayesian Optimized)\")\n",
    "plt.legend(); plt.savefig(\"ROC_RF_Bayes.jpeg\", dpi=600); plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# ğŸŒ² ä¸ƒã€ç‰¹å¾é‡è¦æ€§ä¸è§£é‡Šæ€§åˆ†æï¼ˆSHAPï¼‰\n",
    "# ä¸ºäº†ç†è§£æ¨¡å‹å†³ç­–ä¾æ®ï¼Œæˆ‘ä»¬ä¸ä»…è®¡ç®—äº†éšæœºæ£®æ—è‡ªå¸¦çš„é‡è¦æ€§ï¼Œè¿˜ç»“åˆ SHAPå€¼ æä¾›æ›´ç»†è‡´çš„è§£é‡Šã€‚\n",
    "# è¿™ä¸€æ­¥ä¸ä»…æ­ç¤ºäº†æ¨¡å‹â€œå¦‚ä½•åˆ¤åˆ«æ»‘å¡â€ï¼Œè¿˜å¯ä»¥æŒ‡å¯¼æˆ‘ä»¬ä»åœ°è´¨æœºç†è§’åº¦éªŒè¯æ¨¡å‹åˆç†æ€§ã€‚\n",
    "# =============================================================================\n",
    "explainer = shap.TreeExplainer(rf_model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# è®¡ç®—SHAPç‰¹å¾é‡è¦æ€§\n",
    "shap_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'SHAP_Importance': np.abs(shap_values[1]).mean(axis=0)\n",
    "}).sort_values('SHAP_Importance', ascending=False)\n",
    "\n",
    "# å¯è§†åŒ–å‰20ä¸ªç‰¹å¾\n",
    "sns.barplot(x='SHAP_Importance', y='Feature', data=shap_importance.head(20))\n",
    "plt.xlabel(\"mean(|SHAP value|)\")\n",
    "plt.title(\"Top 20 SHAP Feature Importances\")\n",
    "plt.savefig(\"SHAP_RF_Bayes.jpeg\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# å…«ã€æ¨¡å‹é¢„æµ‹ï¼šç”Ÿæˆæ»‘å¡å‘ç”Ÿçš„æ¦‚ç‡å€¼\n",
    "# ç¬¬ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ©ç”¨è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹å¯¹ç ”ç©¶åŒºçš„æ‰€æœ‰æ …æ ¼æ ·æœ¬è¿›è¡Œé¢„æµ‹ã€‚è¿™é‡Œçš„é‡ç‚¹æ˜¯ è¯»å–å…¨åŒºç‰¹å¾æ•°æ® â†’ è°ƒç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹ â†’ è¾“å‡ºæ¯ä¸ªæ …æ ¼çš„æ»‘å¡æ¦‚ç‡ã€‚\n",
    "# =============================================================================\n",
    "def predict_probabilities():\n",
    "    \"\"\"ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹è¿›è¡Œé¢„æµ‹å¹¶ä¿å­˜æ¦‚ç‡ç»“æœ\"\"\"\n",
    "    print(\"æ­£åœ¨åŠ è½½æ¨¡å‹å’Œæ•°æ®è¿›è¡Œé¢„æµ‹...\")\n",
    "    try:\n",
    "        # åŠ è½½ä¼˜åŒ–åçš„éšæœºæ£®æ—æ¨¡å‹\n",
    "        RFmodel = joblib.load('RFmodel_anti_overfitting.pkl')\n",
    "        \n",
    "        # åŠ è½½å…¨åŒºæ ·æœ¬æ•°æ®\n",
    "        test_data = pd.read_csv('tifftocsvxishui.csv').values\n",
    "        # ç”¨å‡å€¼å¡«å…… NaNï¼Œä¿è¯é¢„æµ‹è¿‡ç¨‹ä¸å‡ºé”™\n",
    "        test_data = np.nan_to_num(test_data, nan=np.nanmean(test_data))\n",
    "        \n",
    "        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡\n",
    "        predicted_probabilities = RFmodel.predict_proba(test_data)\n",
    "        \n",
    "        # ä¿å­˜æ¦‚ç‡ç»“æœ\n",
    "        probability_df = pd.DataFrame(predicted_probabilities, \n",
    "                                    columns=['Class_0_Probability', 'Class_1_Probability'])\n",
    "        probability_df.to_csv('RFprob.csv', index=False)\n",
    "        print(\"é¢„æµ‹æ¦‚ç‡å·²ä¿å­˜ä¸º RFprob.csv\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"é¢„æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# =============================================================================\n",
    "# ä¹ã€ç”Ÿæˆæ˜“å‘æ€§æ …æ ¼ï¼šç©ºé—´åŒ–è¡¨è¾¾æ¦‚ç‡\n",
    "# æœ‰äº†æ¯ä¸ªæ ·æœ¬ç‚¹çš„æ¦‚ç‡å€¼åï¼Œæ¥ä¸‹æ¥éœ€è¦å°†å…¶æ˜ å°„å›æ …æ ¼ç©ºé—´ã€‚æˆ‘ä»¬ä»¥ä¸€å¼ å‚è€ƒæ …æ ¼ï¼ˆå¦‚å¡å‘ Aspect.tifï¼‰ä¸ºæ¨¡æ¿ï¼Œä¿è¯ç”Ÿæˆçš„æ˜“å‘æ€§å›¾ä¸ç ”ç©¶åŒºç©ºé—´èŒƒå›´å’Œåˆ†è¾¨ç‡å®Œå…¨ä¸€è‡´ã€‚\n",
    "# =============================================================================\n",
    "def create_susceptibility_raster():\n",
    "    \"\"\"å°†é¢„æµ‹æ¦‚ç‡ç»“æœè½¬æ¢ä¸ºæ˜“å‘æ€§æ …æ ¼\"\"\"\n",
    "    print(\"æ­£åœ¨ç”Ÿæˆæ˜“å‘æ€§æ …æ ¼...\")\n",
    "    try:\n",
    "        # è¯»å–é¢„æµ‹ç»“æœï¼Œåªå–æ­£ç±»æ¦‚ç‡\n",
    "        data = pd.read_csv('RFprob.csv')\n",
    "        values = data['Class_1_Probability'].values\n",
    "\n",
    "        # æ‰“å¼€å‚è€ƒæ …æ ¼ï¼Œè¯»å–ç©ºé—´ä¿¡æ¯\n",
    "        with rasterio.open(r\"J:\\å¾®ä¿¡å…¬ä¼—å·\\æ»‘å¡æ˜“å‘æ€§è¯„ä»·æ¨¡å‹\\model\\Aspect.tif\") as src:\n",
    "            profile = src.profile.copy()\n",
    "            num_cols = src.width\n",
    "            num_rows = src.height\n",
    "\n",
    "        # ç¡®è®¤æ•°æ®æ•°é‡ä¸æ …æ ¼åƒå…ƒæ•°ä¸€è‡´\n",
    "        if len(values) != num_rows * num_cols:\n",
    "            raise ValueError(f\"æ•°æ®å°ºå¯¸ä¸åŒ¹é…: CSVæ•°æ®ç‚¹({len(values)}) â‰  æ …æ ¼å°ºå¯¸({num_rows*num_cols})\")\n",
    "\n",
    "        # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„å¹¶ä¿å­˜\n",
    "        raster_data = values.reshape((num_rows, num_cols)).astype(np.float32)\n",
    "        profile.update(dtype=rasterio.float32, count=1)\n",
    "        \n",
    "        output_path = r\"J:\\å¾®ä¿¡å…¬ä¼—å·\\æ»‘å¡æ˜“å‘æ€§è¯„ä»·æ¨¡å‹è¿›é˜¶\\æ˜“å‘æ …æ ¼\\BoCV-RF.tif\"\n",
    "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "            dst.write(raster_data, 1)\n",
    "            \n",
    "        print(f\"æ˜“å‘æ€§æ …æ ¼å·²æˆåŠŸä¿å­˜è‡³ {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"ç”Ÿæˆæ …æ ¼æ—¶å‡ºé”™: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0075d",
   "metadata": {},
   "source": [
    "## æ–¹æ³•5:é€’å½’ç‰¹å¾ç­›é€‰\n",
    "é€’å½’ç‰¹å¾ç­›é€‰ï¼ˆRecursive Feature Elimination, RFEï¼‰æ˜¯ä¸€ç§å¸¸è§çš„ç‰¹å¾é€‰æ‹©æ–¹æ³•ï¼Œå…¶æ ¸å¿ƒæ€æƒ³æ˜¯åŸºäºæ¨¡å‹è‡ªå¸¦çš„ç‰¹å¾é‡è¦æ€§æ’åï¼Œç»“åˆè®¾å®šçš„æ­¥é•¿å’Œæœ€ä½ä¿ç•™ç‰¹å¾æ•°é‡ï¼Œå¯¹ç‰¹å¾é›†è¿›è¡Œé€æ­¥é€’å½’åœ°ç­›é€‰ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œæ ¹æ®æ¨¡å‹è®¡ç®—çš„ç‰¹å¾è´¡çŒ®åº¦ï¼ˆå¦‚åŸºäºéšæœºæ£®æ—çš„Ginié‡è¦æ€§æˆ–åŸºäºBoostingç®—æ³•çš„åˆ†è£‚å¢ç›Šç­‰ï¼‰ï¼Œå‰”é™¤ä¸€éƒ¨åˆ†æ’åæœ€ä½çš„ç‰¹å¾ï¼Œç›´åˆ°è¾¾åˆ°è®¾å®šçš„æœ€ä½ç‰¹å¾æ•°é‡æˆ–æ€§èƒ½å¹³è¡¡ç‚¹ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿåœ¨å‡å°‘ç‰¹å¾æ•°é‡çš„åŒæ—¶å°½é‡ä¿æŒæ¨¡å‹æ€§èƒ½çš„ç¨³å®šæ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f7a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# æ¨¡å—5.5â€”â€”â€”â€”é€’å½’ç‰¹å¾ç­›é€‰å®ç°ç‰¹å¾é€‰æ‹©\n",
    "# å‚è€ƒï¼š# https://mp.weixin.qq.com/s/gfh5Ocv4ATpcMRbwNnJAAg\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ä¸€ï¼šæ•°æ®å‡†å¤‡\n",
    "# =============================================================================\n",
    "# å°†æ•°æ®é›†æŒ‰ç‰¹å¾å˜é‡ (X) å’Œç›®æ ‡å˜é‡ (y) è¿›è¡Œåˆ’åˆ†åï¼Œè¿›ä¸€æ­¥å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç¡®ä¿ç›®æ ‡å˜é‡ y åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­ç±»åˆ«åˆ†å¸ƒæ¯”ä¾‹ä¸€è‡´ï¼ˆé€šè¿‡ stratify å‚æ•°ï¼‰\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "df = pd.read_excel(\"2024-12-21å…¬ä¼—å·Pythonæœºå™¨å­¦ä¹ AI.xlsx\")\n",
    "# åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡å˜é‡\n",
    "X = df.drop(['y'], axis=1)\n",
    "y = df['y']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡å˜é‡\n",
    "X = df.drop(['y'], axis=1)\n",
    "y = df['y']\n",
    "# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=df['y'])\n",
    "# =============================================================================\n",
    "# æ­¥éª¤äºŒï¼šé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼ˆRFEï¼‰ä¸å¤šæ¨¡å‹æ€§èƒ½è¾“å‡º\n",
    "# =============================================================================\n",
    "'''\n",
    "1.é€šè¿‡é€’å½’ç‰¹å¾æ¶ˆå‡ï¼ˆRFEï¼‰æ–¹æ³•ï¼Œä»¥æ­¥é•¿ä¸º1é€æ­¥å‡å°‘ç‰¹å¾ï¼Œè®¡ç®—ä¸åŒç‰¹å¾æ•°é‡ä¸‹å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆRandomForestã€GradientBoostingã€XGBoostã€LightGBMã€CatBoostã€AdaBoostï¼‰åœ¨æµ‹è¯•é›†ä¸Šçš„AUCåˆ†æ•°ï¼Œå¹¶è®°å½•æ¯ä¸ªæ¨¡å‹åœ¨ä¸åŒç‰¹å¾å­é›†ä¸Šçš„æ€§èƒ½è¡¨ç°ï¼ˆæœ€ä½ç‰¹å¾æ•°ä¸º1ï¼‰ã€‚æ­¤å¤–ï¼Œé™¤äº†ä½¿ç”¨æ­¤ç›´æ¥é€’å½’ç‰¹å¾æ¶ˆå‡å‡½æ•°å¤–ï¼Œè¿˜å­˜åœ¨ç»“åˆäº¤å‰éªŒè¯çš„é€’å½’ç‰¹å¾æ¶ˆå‡ï¼ˆRFECVï¼‰ï¼Œèƒ½å¤Ÿæ›´ç¨³å¥åœ°é€‰æ‹©æœ€ä½³ç‰¹å¾æ•°é‡ï¼Œ\n",
    "2.KæŠ˜äº¤å‰éªŒè¯ç»“åˆRFEä¸éšæœºæ£®æ—ï¼šç‰¹å¾é€‰æ‹©å…¨è¿‡ç¨‹å¯è§†åŒ–å‚è€ƒï¼šhttps://mp.weixin.qq.com/s?__biz=Mzk0NDM4OTYyOQ==&mid=2247486976&idx=1&sn=4096d77c1a9f704765bb8a7d3d290b2b&scene=21&poc_token=HMCBc2mj0lrZM0xfgLbTMbUgNmcx0JDyuZY0tsAH\n",
    "'''\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "def perform_rfe_single_model(model, X_train, y_train, X_test, y_test, model_name):\n",
    "    # åˆå§‹åŒ–RFEï¼Œè®¾ç½®æ‰€æœ‰ç‰¹å¾ä¸ºèµ·å§‹é€‰æ‹©\n",
    "    feature_count = len(X_train.columns)\n",
    "    step = 1\n",
    "    rfe = RFE(estimator=model, n_features_to_select=1, step=step)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    # æå–ç‰¹å¾æ•°é‡å’Œå¯¹åº”çš„ç‰¹å¾æ’å\n",
    "    rankings = rfe.ranking_\n",
    "    sorted_indices = sorted(range(len(rankings)), key=lambda k: rankings[k])\n",
    "    # è®¡ç®—æ¯ä¸ªç‰¹å¾å­é›†çš„ROC AUCåˆ†æ•°ï¼ˆåŸºäºæµ‹è¯•é›†ï¼‰\n",
    "    scores = []\n",
    "    feature_counts = []\n",
    "    for i in range(feature_count, 0, -step):\n",
    "        selected_features_train = X_train.iloc[:, sorted_indices[:i]]\n",
    "        selected_features_test = X_test.iloc[:, sorted_indices[:i]]\n",
    "        \n",
    "        # åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒæ¨¡å‹\n",
    "        model.fit(selected_features_train, y_train)\n",
    "        \n",
    "        # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹å¹¶è®¡ç®—AUC\n",
    "        y_pred = model.predict_proba(selected_features_test)[:, 1]\n",
    "        score = roc_auc_score(y_test, y_pred)\n",
    "        scores.append(score)\n",
    "        feature_counts.append(i)\n",
    "    results_df = pd.DataFrame({\n",
    "        \"Number_of_Features\": feature_counts[::-1],  \n",
    "        model_name: scores[::-1]                   \n",
    "    })\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42, verbose=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42, algorithm='SAMME')\n",
    "}\n",
    "\n",
    "\n",
    "results_df = None\n",
    "\n",
    "# éå†æ¨¡å‹å¹¶è¿è¡ŒRFE\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Running RFE for {model_name}...\")\n",
    "    model_results = perform_rfe_single_model(model, X_train, y_train, X_test, y_test, model_name)\n",
    "    if results_df is None:\n",
    "        results_df = model_results\n",
    "    else:\n",
    "        results_df = results_df.merge(model_results, on=\"Number_of_Features\", how=\"outer\")\n",
    "\n",
    "results_df.reset_index(drop=True, inplace=True)\n",
    "results_df\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤ä¸‰ï¼šåŸºäºé€’å½’ç‰¹å¾æ¶ˆé™¤çš„å¤šæ¨¡å‹AUCæ€§èƒ½å¯è§†åŒ–ä¸æœ€ä½³ç‰¹å¾é€‰æ‹©\n",
    "# =============================================================================\n",
    "'''\n",
    "1.é€šè¿‡ç­›é€‰ç‰¹å¾æ•°é‡é€’å‡çš„æ¨¡å‹æ€§èƒ½æ•°æ®ï¼Œç»“åˆæ­¥é•¿å’Œåˆå§‹å€¼ï¼ˆå½“ç„¶è¿™é‡Œçš„æ­¥é•¿å’Œåˆå§‹å€¼åœ¨æ‰§è¡ŒRFEæ—¶å°±å¯ä»¥è¿›è¡Œå®šä¹‰ä½†æ˜¯é€šè¿‡è¿™ç§æ‰‹æ®µæ›´åŠ çµæ´»è€Œå·²ï¼‰ï¼Œå¯¹å¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ç‰¹å¾æ•°é‡å‡å°‘è¿‡ç¨‹ä¸­çš„AUCå˜åŒ–è¿›è¡Œå¯è§†åŒ–ï¼ŒåŒæ—¶ç»˜åˆ¶ä¸€æ¡è‡ªå®šä¹‰çš„å‚ç›´çº¿ï¼ˆoptimal_features = 8ï¼‰ä»¥æ ‡æ³¨è‡ªå®šä¹‰çš„æœ€ä½³ç‰¹å¾æ•°é‡\n",
    "2.éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„ optimal_features = 8 å¹¶ä¸æ˜¯æ¨¡å‹è¯„ä»·æŒ‡æ ‡æ„ä¹‰ä¸Šçš„æœ€ä¼˜ç‰¹å¾æ•°é‡ï¼Œè€Œæ˜¯å‡ºäºå®é™…éœ€æ±‚ï¼ˆå¦‚ç‰¹å¾æ•°é‡è¾ƒå°‘ä½†æ€§èƒ½æ¥è¿‘æœ€ä¼˜çš„å¯æ¥å—èŒƒå›´ï¼‰çš„æƒè¡¡ç»“æœã€‚ä¸ºäº†ä½“ç°è¿™ä¸€ç‚¹ï¼Œç»˜å›¾æ—¶ä½¿ç”¨è‡ªå®šä¹‰çš„æœ€ä½³ç‰¹å¾æ•°é‡ï¼Œè€Œä¸æ˜¯ä¸¥æ ¼ä¾æ®AUCæœ€é«˜ç‚¹çš„ç‰¹å¾æ•°é‡\n",
    "'''\n",
    "# ç­›é€‰å‡ºæ»¡è¶³æ­¥é•¿ä¸º1ã€åˆå§‹å€¼ä¸º1çš„è¡Œ\n",
    "step_size = 1  # æ­¥é•¿è®¾ç½®ä¸º1\n",
    "initial_value = 1  # åˆå§‹å€¼è®¾ç½®ä¸º1\n",
    "\n",
    "# ç­›é€‰æ»¡è¶³æ¡ä»¶çš„æ•°æ®è¡Œï¼šç‰¹å¾æ•°é‡ >= åˆå§‹å€¼ï¼Œå¹¶æŒ‰æ­¥é•¿å–æ ·\n",
    "filtered_results_df = results_df[\n",
    "    results_df[\"Number_of_Features\"] >= initial_value  # ç­›é€‰ç‰¹å¾æ•°é‡å¤§äºç­‰äºåˆå§‹å€¼çš„è¡Œ\n",
    "].iloc[::step_size, :].reset_index(drop=True)  # æŒ‰æ­¥é•¿å–æ ·ï¼Œå¹¶é‡ç½®ç´¢å¼•\n",
    "\n",
    "# æŒ‰ \"Number_of_Features\" åˆ—ä»å¤§åˆ°å°æ’åº\n",
    "filtered_results_df.sort_values(by=\"Number_of_Features\", ascending=False, inplace=True)\n",
    "\n",
    "# ç»˜åˆ¶å›¾å½¢\n",
    "plt.figure(figsize=(8, 6))  # è®¾ç½®ç”»å¸ƒå¤§å°\n",
    "for column in filtered_results_df.columns[1:]:  # éå†æ‰€æœ‰æ¨¡å‹çš„åˆ—ï¼ˆä»ç¬¬2åˆ—å¼€å§‹ï¼‰\n",
    "    plt.plot(\n",
    "        filtered_results_df[\"Number_of_Features\"],  # Xè½´ï¼šç‰¹å¾æ•°é‡\n",
    "        filtered_results_df[column],               # Yè½´ï¼šå¯¹åº”æ¨¡å‹çš„AUCåˆ†æ•°\n",
    "        label=column,                              # è®¾ç½®å›¾ä¾‹ä¸ºæ¨¡å‹åç§°\n",
    "        marker='o',                                # åœ¨æ›²çº¿ä¸Šæ ‡è®°ç‚¹\n",
    "        linewidth=1.5                              # è®¾ç½®çº¿æ¡å®½åº¦\n",
    "    )\n",
    "\n",
    "# ç»˜åˆ¶æœ€ä½³ç‰¹å¾æ•°é‡çš„å‚ç›´è™šçº¿\n",
    "optimal_features = 8  # æœ€ä½³ç‰¹å¾æ•°é‡\n",
    "plt.axvline(\n",
    "    x=optimal_features,  # å‚ç›´çº¿çš„ä½ç½®\n",
    "    color='black',       # è®¾ç½®çº¿çš„é¢œè‰²ä¸ºé»‘è‰²\n",
    "    linestyle='--',      # è®¾ç½®çº¿å‹ä¸ºè™šçº¿\n",
    "    label='Optimal Features'  # å›¾ä¾‹è¯´æ˜\n",
    ")\n",
    "\n",
    "# è®¾ç½®å›¾è¡¨æ ‡é¢˜å’Œåæ ‡è½´æ ‡ç­¾\n",
    "plt.title('Feature Reduction (Step=7, Initial=1)', fontsize=14)  # å›¾è¡¨æ ‡é¢˜\n",
    "plt.xlabel('Number of Features', fontsize=12)  # Xè½´æ ‡ç­¾\n",
    "plt.ylabel('Area Under the ROC Curve (AUC)', fontsize=12)  # Yè½´æ ‡ç­¾\n",
    "# è®¾ç½®Xè½´çš„åˆ»åº¦å€¼å’Œå­—ä½“å¤§å°\n",
    "plt.xticks(\n",
    "    ticks=filtered_results_df[\"Number_of_Features\"],  # è®¾ç½®åˆ»åº¦å€¼ä¸ºç­›é€‰åçš„ç‰¹å¾æ•°é‡\n",
    "    fontsize=10  # è®¾ç½®å­—ä½“å¤§å°\n",
    ")\n",
    "plt.yticks(fontsize=10)  # è®¾ç½®Yè½´å­—ä½“å¤§å°\n",
    "plt.legend(title=\"Models\", fontsize=10, loc=\"best\")  # å›¾ä¾‹æ ‡é¢˜ã€å­—ä½“å¤§å°åŠä½ç½®\n",
    "plt.grid(axis='y', alpha=0.5)  # æ·»åŠ Yè½´æ–¹å‘çš„ç½‘æ ¼çº¿ï¼Œå¹¶è®¾ç½®é€æ˜åº¦\n",
    "plt.gca().spines['top'].set_visible(False)  # éšè—é¡¶éƒ¨è¾¹æ¡†\n",
    "plt.gca().spines['right'].set_visible(False)  # éšè—å³ä¾§è¾¹æ¡†\n",
    "plt.savefig('1.pdf', format='pdf', bbox_inches='tight', dpi=1200)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# æ­¥éª¤å››ï¼šé€’å½’ç‰¹å¾æ¶ˆé™¤ï¼šç¡®å®šæœ€ä½³æ¨¡å‹ä¸è‡ªå®šä¹‰ç‰¹å¾é€‰æ‹©\n",
    "# =============================================================================\n",
    "'''\n",
    "# å½“ç¡®å®šäº†æœ€ä½³æ¨¡å‹ï¼ˆå¦‚éšæœºæ£®æ—ï¼‰ä»¥åŠè‡ªå®šä¹‰çš„æœ€ä½³ç‰¹å¾æ•°é‡ï¼ˆå¦‚ feature_target = 8ï¼‰æ—¶ï¼Œç”±äºè¯¥ç‰¹å¾æ•°é‡å¹¶éæ¨¡å‹è¯„ä»·æŒ‡æ ‡æ„ä¹‰ä¸Šçš„æœ€ä¼˜ï¼Œè€Œæ˜¯æ ¹æ®å®é™…éœ€æ±‚è‡ªå®šä¹‰çš„ç»“æœï¼Œå› æ­¤éœ€è¦é€šè¿‡é€’å½’ç‰¹å¾ç­›é€‰çš„åŸç†è¿”å›å¯¹åº”çš„ç‰¹å¾é›†åˆï¼Œæ‰€ä»¥è¿™é‡Œçš„perform_rfe_single_modelä¸ºè‡ªå®šä¹‰å‡½æ•°ç”¨äºè¿”å›æ¨¡å‹æ‰€é€‰æ‹©çš„ç‰¹å¾æ•°é‡\n",
    "'''\n",
    "# Random Forest æ¨¡å‹\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# æ‰§è¡Œ RFEï¼Œä»…å¯¹ Random Forest\n",
    "model_name = \"RandomForest\"\n",
    "feature_target = 8\n",
    "print(f\"Running RFE for {model_name}...\")\n",
    "\n",
    "results_df, selected_features = perform_rfe_single_model(\n",
    "    random_forest_model, X_train, y_train, X_test, y_test, model_name, feature_target\n",
    ")\n",
    "\n",
    "# æŸ¥çœ‹ Random Forest çš„ç‰¹å¾é€‰æ‹©\n",
    "print(f\"{model_name} selected features when using {feature_target} features:\")\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
