{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e62e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75ac2c47",
   "metadata": {},
   "source": [
    "# 1、数据预处理与数据集划分\n",
    "\n",
    "主要负责数据的加载、清洗、特征转换以及最终的数据集划分。我们引入了自定义的数据处理管道模块，对原始的静态临床数据进行一系列预处理操作，并将其划分为训练集和测试集。\n",
    "\n",
    "- 导入必要的库和自定义模块,包括用于删除常量列和二元化毒性等级的工具类。\n",
    "- 读取静态数据 CSV 文件，并使用 ConstantColumnDropper 删除数值完全相同的列（即所有样本在该列上的值都一样，不具备区分度）。\n",
    "- 使用 ToxicityBinarizer 将多个毒性指标（如 CRS、ICANS、Infection）根据阈值转换为二分类变量（例如：等级 > 2 视为严重，标记为 1；否则为 0）。\n",
    "- 清洗目标变量列（Infection）中的缺失值，并重置索引以确保数据连续性。\n",
    "- 使用 PatientLevelStratifiedSplitterWithCV 进行\n",
    "\n",
    "基于患者层面的分层划分，生成训练集、测试集以及交叉验证折，确保同一患者的数据不会同时出现在训练集和测试集中，避免数据泄露。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d34358",
   "metadata": {},
   "source": [
    "### 关于字体BUGfindfont: Generic family 'serif' not found because none of the following families were found: Times New Roman\n",
    "参考：https://zhuanlan.zhihu.com/p/509574840\n",
    "\n",
    "把windows下的Times New Roman字体上传到本地虚拟环境中的字体文件夹中\n",
    "/opt/anaconda3/envs/pl/lib/python3.10/site-packages/matplotlib/mpl-data/fonts/ttf/times.ttf\n",
    "然后删除matplotlib的缓存\n",
    "rm -rf /home/phl/.cache/matplotlib\n",
    "最后重启vscode，重新运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3e1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置工作路径\n",
    "import os\n",
    "os.chdir(\"/home/phl/PHL/Car-T/model_v1\")\n",
    "\n",
    "# 获取并打印当前工作路径\n",
    "current_path = os.getcwd()\n",
    "print(\"当前工作路径：\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Pandas库，用于数据处理\n",
    "import pandas as pd\n",
    "\n",
    "# 导入NumPy库，用于数值计算\n",
    "import numpy as np\n",
    "\n",
    "# 导入模型选择模块中的train_test_split，用于拆分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 导入预处理模块中的StandardScaler 和OneHotEncoder，用于数据标准化和独热编码\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# 导入逻辑回归模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 导入随机森林分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 导入多层感知器分类器\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 导入支持向量机分类器\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 导入XGBoost库\n",
    "# !pip install xgboost -i https://mirrors.aliyun.com/pypi/simple/\n",
    "import xgboost as xgb\n",
    "\n",
    "#导入LightGBM库\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "# 清除缓存并重新加载模块\n",
    "if 'pipeline.perfect_pipeline' in sys.modules:\n",
    "    importlib.reload(sys.modules['pipeline.perfect_pipeline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b6f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 你需要确保下游的特征工程步骤（如读取动态数据）能够接受原始动态数据目录作为输入，而不是寻找 train_dynamic 文件夹。\n",
    "# 要在特征提取或数据加载函数中，不要硬编码读取 ./train_dynamic/，代码实现如下：\n",
    "\n",
    "def build_features_from_df(target_df, dynamic_source_dir):\n",
    "    \"\"\"\n",
    "    target_df: 也就是 split 出来的 train_df 或 test_df\n",
    "    dynamic_source_dir: 原始的 processed_standardized 文件夹路径\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    # 只遍历当前分割 (train 或 test) 中的患者 ID\n",
    "    for patient_id in target_df['ID']: \n",
    "        file_path = os.path.join(dynamic_source_dir, f\"{patient_id}.csv\")\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            # 读取数据并处理\n",
    "            df_dyn = pd.read_csv(file_path)\n",
    "            # ... 特征工程逻辑 ...\n",
    "        else:\n",
    "            # 处理缺失情况\n",
    "            pass\n",
    "            \n",
    "    return pd.DataFrame(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62749210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm  # 用于显示进度条\n",
    "\n",
    "def extract_baseline_features(patient_ids, dynamic_dir, time_col='Day', cutoff_day=0):\n",
    "    \"\"\"\n",
    "    从动态文件中提取 Day < cutoff_day 的最后一次观测值作为静态基线特征。\n",
    "    \n",
    "    参数:\n",
    "        patient_ids: 患者 ID 列表 (来自清洗好的数据 df_clean?['ID'])\n",
    "        dynamic_dir: 动态数据文件夹路径\n",
    "        time_col: 动态数据中表示时间的列名，默认为 'Day'\n",
    "        cutoff_day: 截断时间点，默认为 0\n",
    "        \n",
    "    返回:\n",
    "        pd.DataFrame: 包含提取特征的 DataFrame，索引为 ID\n",
    "    \"\"\"\n",
    "    extracted_features = []\n",
    "    \n",
    "    print(f\"正在从 {len(patient_ids)} 个动态文件中提取基线特征 (Day < {cutoff_day})...\")\n",
    "    \n",
    "    for pid in tqdm(patient_ids):\n",
    "        file_path = os.path.join(dynamic_dir, f\"{pid}.csv\")\n",
    "        \n",
    "        # 初始化字典，包含 ID\n",
    "        patient_data = {'ID': pid}\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # 读取动态数据\n",
    "                df_dyn = pd.read_csv(file_path)\n",
    "                \n",
    "                # 确保时间列存在且为数值型\n",
    "                if time_col in df_dyn.columns:\n",
    "                    # 1. 筛选：仅保留 Day < 0 的数据\n",
    "                    # 注意：这里使用 < 0，不包含 Day 0。如果 Day 0 是输注前采集，可改为 <= 0\n",
    "                    mask = df_dyn[time_col] < cutoff_day\n",
    "                    df_baseline_window = df_dyn[mask]\n",
    "                    \n",
    "                    if not df_baseline_window.empty:\n",
    "                        # 2. 排序：按时间升序排列\n",
    "                        df_sorted = df_baseline_window.sort_values(by=time_col, ascending=True)\n",
    "                        \n",
    "                        # 3. 提取：取最后一行（即离 Day 0 最近的一次观测）\n",
    "                        last_observation = df_sorted.iloc[-1]\n",
    "                        \n",
    "                        # 4. 转换：将 Series 转为字典，排除时间列\n",
    "                        # 添加前缀 'baseline_' 以区分原始静态变量\n",
    "                        for col in last_observation.index:\n",
    "                            if col != time_col and col != 'ID': # 排除 ID 和 Day\n",
    "                                patient_data[f\"baseline_{col}\"] = last_observation[col]\n",
    "                                \n",
    "                        # 特殊情况处理：记录最后一次观测具体是第几天（可选，用于质量控制）\n",
    "                        # 含义：它表示提取的基线数据距离治疗开始（Day 0）有多少天。例如：如果一个患者在Day-1有数据，cutoff_day是0，那么gap=0-(-1)=1天。\n",
    "                        # 如果一个患者最近一次检查是在 Day -20，那么 gap = 20 天。\n",
    "                        # 作用：作为一个非常重要的特征，部分模型可能会利用这个信息，给近期数据更高的权重。\n",
    "                        # 数据质量评估：Gap 越小（如 1-3 天），说明数据越能代表患者接受治疗前的即时状态。Gap很大（如>30天），说明数据可能已经“过期”，参考价值降低。\n",
    "                        patient_data['baseline_gap_days'] = cutoff_day - last_observation[time_col]\n",
    "            except Exception as e:\n",
    "                print(f\"处理 ID {pid} 时出错: {e}\")\n",
    "        \n",
    "        extracted_features.append(patient_data)\n",
    "    \n",
    "    # 转换为 DataFrame\n",
    "    df_features = pd.DataFrame(extracted_features)\n",
    "    return df_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "# 参考：https://sklearn.apachecn.org/master/30/————3.1.2.2.2. 分层随机 Split（StratifiedShuffleSplit）\n",
    "# 方案1:70/30 患者级分层划分；\n",
    "# 方案2:70/30 患者级分层划分 + 5折交叉验证\n",
    "from pipeline.perfect_pipeline import ConstantColumnDropper, ToxicityBinarizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pipeline.data_splitters import (\n",
    "    PatientLevelStratifiedSplitter,\n",
    "    PatientLevelStratifiedSplitterWithCV,\n",
    "    patient_level_train_test_split\n",
    ")\n",
    "\n",
    "# 读入静态数据和动态数据\n",
    "df = pd.read_csv(\"/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/csv/B-NHL_static_data.csv\")\n",
    "dynamic_data_dir = \"/home/phl/PHL/Car-T/model_v1/B-NHL_reindexed_example/processed\"\n",
    "print(f\"原始分布: {df.shape}\")\n",
    "# print (df)\n",
    "\n",
    "# =============================================================================\n",
    "#  1. 行处理: 剔除 Ann Arbor分期（Ann Arbor stage即AAS） 为空的患者（即白血病或者原发中枢的患者） \n",
    "# 原理：后续读取动态数据时（如 build_features_from_df 函数），是基于 df['ID'] 列表进行遍历的。因此只需要在内存中的静态数据DataFrame即 df 里剔除这些患者对应的行。\n",
    "# 这样，后续的程序逻辑自然就会跳过这些 ID，从而达到“逻辑删除数据”的效果，而不会在物理上删除磁盘上的原始动态数据文件。\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "# 找出 AAS为 NaN 的行\n",
    "if 'AAS' in df.columns:\n",
    "    missing_AAS_mask = df['AAS'].isna()\n",
    "    missing_ids = df.loc[missing_AAS_mask, 'ID'].values\n",
    "    \n",
    "    if len(missing_ids) > 0:\n",
    "        print(f\"步骤1(行处理): 发现 {len(missing_ids)} 例患者 AAS 信息缺失(即白血病或原发中枢患者), 患者ID: {missing_ids}, 正在执行剔除操作...\")\n",
    "        \n",
    "        # 仅从静态数据 DataFrame 中剔除这些行, 只要这里剔除了 ID，后续步骤自然会跳过读取对应的动态数据文件。\n",
    "        df_clean1 = df[~missing_AAS_mask].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"已剔除变量 AAS 为空的患者\")\n",
    "        print(f\"处理后分布: {df_clean1.shape}\")\n",
    "    else:\n",
    "        print(\"检查通过：未发现 AAS 为空的患者。\")\n",
    "else:\n",
    "    print(\"警告：静态数据中未找到 'AAS' 列，跳过此检查。\")\n",
    "# print (df_clean1)\n",
    "\n",
    "# =============================================================================\n",
    "#  2. 行处理: 剔除目标变量为空的患者(仅逻辑剔除静态变量中的数据，不删除物理文件)\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)   \n",
    "# 步骤1: 处理目标变量列中的缺失值\n",
    "if 'Infection' in df.columns:\n",
    "\n",
    "    # 找出目标变量为空的患者ID并打印\n",
    "    missing_label_mask = df_clean1['Infection'].isna()\n",
    "    if missing_label_mask.sum() > 0:\n",
    "        missing_ids_infection = df_clean1.loc[missing_label_mask, 'ID'].values\n",
    "        print(f\"步骤2(行处理): 发现 {df_clean1['Infection'].isna().sum()} 例患者目标变量列 'Infection' 缺失, 患者ID: {missing_ids_infection}, 正在执行剔除操作...\")\n",
    "\n",
    "    # 删除目标变量列中包含 NaN 的行，会导致索引ID不连续，但是不影响后续数据划分\n",
    "    # 分割器返回的是索引位置(iloc），不依赖原始索引值\n",
    "    df_clean2 = df_clean1[df_clean1['Infection'].notna()].copy()\n",
    "    print(f\"目标变量分布: {df_clean2['Infection'].value_counts().to_dict()}\")\n",
    "    print(f\"处理后分布: {df_clean2.shape}\")\n",
    "\n",
    "    # 可选：重置索引使其连续（推荐用于后续处理），虽然索引不连续不影响划分，但如果后续代码依赖连续索引，可能会出问题\n",
    "    df_clean2 = df_clean2.reset_index(drop=True)\n",
    "# print (df_clean2)\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 行处理: 执行动态特征提取并合并\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"步骤3(行处理): 动态特征提取与合并\")\n",
    "# 1. 提取特征, df_clean2 是当前清洗好的静态数据框\n",
    "baseline_features_df = extract_baseline_features(\n",
    "    patient_ids=df_clean2['ID'].values,\n",
    "    dynamic_dir=dynamic_data_dir, # 使用动态数据定义路径\n",
    "    time_col='day',               # 请确认动态CSV中时间列名为 'day' 还是 'time'\n",
    "    cutoff_day=0\n",
    ")\n",
    "\n",
    "# 2. 合并到主数据框\n",
    "# 使用左连接 (Left Join) 确保不会丢失 df_clean2 中的患者\n",
    "# 如果某个患者没有动态数据，新列将自动填充为 NaN\n",
    "print(f\"\\n合并前 df_clean2 形状: {df_clean2.shape}\")\n",
    "df_clean3 = pd.merge(df_clean2, baseline_features_df, on='ID', how='left')\n",
    "print(f\"合并后 df_clean3 形状: {df_clean3.shape}\")\n",
    "\n",
    "# 3. 简单的缺失值检查\n",
    "new_cols = [c for c in df_clean3.columns if c.startswith('baseline_')]\n",
    "missing_counts = df_clean3[new_cols].isna().sum()\n",
    "print(f\"\\n新特征缺失值情况 (Top 5): \\n{missing_counts.sort_values(ascending=False).head()}\")\n",
    "print(f\"处理后分布: {df_clean3.shape}\")\n",
    "# print (df_clean3)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 值处理: 二元化毒性等级\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"步骤4(值处理): 二元化毒性等级\")\n",
    "# 删除常量列 & 缺失值处理\n",
    "# dropper = ConstantColumnDropper()\n",
    "# df_clean4 = dropper.fit_transform(df_clean3)\n",
    "# 检查缺失值并删除包含缺失值的行\n",
    "#df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# binarizer = ToxicityBinarizer(col=\"CRS\", threshold=2)   # 单个指标二元化\n",
    "binarizer = ToxicityBinarizer(      # 多个毒性指标使用相同阈值\n",
    "    columns=[\"CRS\", \"ICANS\", \"E_ICAHT\", \"L_ICAHT\", \"Infection\"],\n",
    "    threshold=2)\n",
    "df_clean4 = binarizer.fit_transform(df_clean3)\n",
    "\n",
    "# =============================================================================\n",
    "#  5. 列处理: 手动删除不需要的冗余列（仅删除静态变量中的部分列）\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "# 根据需要修改冗余列列表\n",
    "cols_to_drop = ['CCID', 'Disease']\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"步骤5(列处理): 正在删除冗余列: {cols_to_drop}\")\n",
    "    df_clean5 = df_clean4.drop(columns=cols_to_drop, errors='ignore') # errors='ignore' 防止列不存在时报错\n",
    "    print(f\"处理后分布: {df_clean5.shape}\")\n",
    "# print (df_clean5)\n",
    "df_final = df_clean5\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 数据集划分，划分为70%训练集和30%测试集，使用基于类的分割器进行患者级分层分割\n",
    "# =============================================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"步骤6(数据集划分): 基础患者级分层划分\")\n",
    "\n",
    "# a.基础分割器\n",
    "# splitter = PatientLevelStratifiedSplitter(test_size=0.3, random_state=42)\n",
    "# train_df, test_df = splitter.split(\n",
    "#     df_final, \n",
    "#     label_col=\"Infection\", \n",
    "#     patient_id_col=\"ID\"  # 确保患者级独立性\n",
    "# )\n",
    "# print(f\"\\n训练集大小: {len(train_df)}\")\n",
    "# print(f\"测试集大小: {len(test_df)}\")\n",
    "\n",
    "# b.带交叉验证的分割器（5折交叉验证）\n",
    "# 注意：PatientLevelStratifiedSplitterWithCV 返回 3 个值：train_df, test_df, cv_folds\n",
    "splitter = PatientLevelStratifiedSplitterWithCV(test_size=0.3, n_folds=5, random_state=42)    \n",
    "train_df, test_df, cv_folds = splitter.split(\n",
    "    df_final, \n",
    "    label_col=\"Infection\", \n",
    "    patient_id_col=\"ID\"  # 确保患者级独立性\n",
    ")\n",
    "print(f\"\\n训练集大小: {len(train_df)}\")\n",
    "print(f\"测试集大小: {len(test_df)}\")\n",
    "print(f\"交叉验证折数: {len(cv_folds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37556882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 数据预览\n",
    "print(f\"\\n数据预览:{df_final.shape}\")\n",
    "print(f\"行数:{df_final.shape[0]}，列数:{df_final.shape[1]}\")\n",
    "# 显示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 显示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "#不换行显示\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "# 显示处理后的数据框，head() 方法本身就只返回前5行数据\n",
    "# print (df_final.head())\n",
    "print(df_final)\n",
    "\n",
    "# 查看训练集和测试集分布\n",
    "print(f\"\\n训练集分布\")\n",
    "print(train_df)\n",
    "\n",
    "print(f\"\\n测试集分布\")\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e9b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer  # 导入缺失值填充器\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X_train = train_df.drop(columns=['Infection', 'ID'])  # 特征\n",
    "y_train = train_df['Infection']                 # 目标变量\n",
    "\n",
    "X_test = test_df.drop(columns=['Infection', 'ID'])  # 特征\n",
    "y_test = test_df['Infection']  \n",
    "\n",
    "# X = df[['area', 'rooms', 'floor', 'year_built', 'location']] \n",
    "# y = df['price']\n",
    "\n",
    "# print(\"\\n特征数据预览:\")\n",
    "# print(X_train.head())\n",
    "# print(\"\\n目标变量数据预览:\")\n",
    "# print(y_train.head())\n",
    "\n",
    "# # 构建预处理步骤--方法一\n",
    "# # 将分类变量转换为独热编码\n",
    "# categorical_cols = ['X66', 'X77']\n",
    "# encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "# X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_cols]), columns=encoder.get_feature_names_out(categorical_cols))\n",
    "# X = pd.concat([X.drop(columns=categorical_cols), X_encoded], axis=1)\n",
    "\n",
    "# # 标准化数值特征\n",
    "# val_cols = ['X1', 'X2', 'X3', 'X4', 'X5']\n",
    "# scaler = StandardScaler()\n",
    "# X[val_cols] = scaler.fit_transform(X[val_cols])\n",
    "\n",
    "# # 将数据转换为 fLoat32 类型\n",
    "# X = X.astype(np.float32)\n",
    "\n",
    "# 构建预处理步骤--方法二\n",
    "# 1. 使用您数据中真实的列名\n",
    "numeric_features = ['Age', 'BMDB',]                                                                          # 数值型特征\n",
    "# categorical_features_2 = ['Sex', 'EM', 'B_symptoms', 'BT', 'CTFA', 'CRS', 'ICANS', 'E_ICAHT', 'L_ICAHT']   # 类别型特征\n",
    "categorical_features = ['BMC', 'EI', 'AAS', 'NL', 'PHSC', 'CM', 'TYPE',\n",
    "                        'Sex', 'EM', 'B_symptoms', 'BT', 'CTFA', 'PCT', 'CRS', 'ICANS', 'E_ICAHT', 'L_ICAHT']\n",
    "# 2. 定义转换器\n",
    "# 类别型特征的预处理\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # 用中位数填充缺失值\n",
    "    ('scaler', StandardScaler())  # 数值特征标准化\n",
    "])\n",
    "\n",
    "# 类别型特征的预处理\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "#    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # 用常数'missing'填充缺失值\n",
    "    # drop='first' 适合线性模型(逻辑回归)，handle_unknown=PCgnore' 适合树模型\n",
    "    # 如果主要用 XGBoost/RF，建议去掉 drop='first'，保留 handle_unknown='ignore'\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))   # 处理测试集中的新类别\n",
    "])\n",
    "\n",
    "# 3. 组合成 ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # 重要：保留未在上面列出的其他列，否则会被丢弃\n",
    ")\n",
    "\n",
    "# 4. 应用 (注意：要在拆分训练/测试集之后 fit)\n",
    "# 假设您已经有了 X_train, X_test\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# 5. (可选) 如果想看列名，可以将结果转回 DataFrame\n",
    "feature_names_train = (numeric_features + \n",
    "                 list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)) + \n",
    "                 [col for col in X_train.columns if col not in numeric_features + categorical_features])\n",
    "X_train = pd.DataFrame(X_train_transformed, columns=feature_names_train)\n",
    "\n",
    "feature_names_test = (numeric_features + \n",
    "                 list(preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)) + \n",
    "                 [col for col in X_test.columns if col not in numeric_features + categorical_features])\n",
    "X_test = pd.DataFrame(X_test_transformed, columns=feature_names_test)\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# === 增强版修复代码（针对本地数据缺失值过多）：强制数值转换 + 多重填充策略 ===\n",
    "\n",
    "# 输入数据中包含缺失值 (NaN)，而 Scikit-Learn 的模型（如 SVM、逻辑回归、MLP、随机森林）不支持缺失值。\n",
    "\n",
    "# 具体原因分析\n",
    "# 在前面的步骤（单元格 9）中，静态数据与动态数据合并时使用了 how='left'。代码输出显示 新特征缺失值情况，说明 baseline_ 开头的动态特征列中存在大量 NaN（因为部分患者没有动态数据）。\n",
    "\n",
    "# 预处理未覆盖：\n",
    "# 在单元格 11 中，你定义了 numeric_transformer 来填充 numeric_features 中的缺失值。但是，对于 baseline_ 等动态特征，你使用了 remainder='passthrough'。这意味着这些列原样保留，其中的 NaN 并没有被填充。\n",
    "\n",
    "# 导致的问题：\n",
    "# SVM、逻辑回归、MLP、随机森林：这些 Scikit-Learn 模型无法处理 NaN，会直接报错 ValueError: Input contains NaN, infinity or a value too large for dtype('float64')。\n",
    "# XGBoost、LightGBM：这些模型可以自动处理 NaN，但代码在运行到它们之前就已经在 SVM 处崩溃了。\n",
    "# 需要处理那些“passthrough”列的缺失值。最简单的方法是在数据进入 ColumnTransformer 之前，或者在生成 X_train 后立即填充缺失值。\n",
    "\n",
    "# 常见填充策略：\n",
    "# 中位数填充：对于数值型列，使用该列的中位数填充缺失值。\n",
    "# 众数填充：对于类别型列，使用该列的众数填充缺失值。\n",
    "# 常数填充：对于某些列，可以使用固定值（如 0 或 'missing'）填充缺失值。\n",
    "\n",
    "# 以下是增强版的缺失值填充代码，专门处理那些未经过预处理器的列，确保所有列都没有缺失值，防止模型报错。\n",
    "# 通常是因为 某些列（特别是 baseline_ 开头的动态特征）可能整列都是缺失值（NaN），或者包含非数值型数据（如字符串）。\n",
    "\n",
    "# 如果某列全是 NaN，median() 计算结果也是 NaN，导致 fillna 无效，SVM 等模型依然会报错。\n",
    "# 如果 passthrough 保留了非数值列，模型也会报错。\n",
    "# 请使用下面的增强版修复代码替换原来的修复部分。它会强制将数据转为数值型，并对全空的列填充 0。\n",
    "\n",
    "# 1. 强制转换为数值类型 (将可能混入的字符串/Object类型转为NaN，防止模型报错)\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 2. 计算训练集的中位数\n",
    "train_medians = X_train.median()\n",
    "\n",
    "# 3. 填充缺失值：\n",
    "# 第一步：用中位数填充\n",
    "# 第二步：如果中位数也是NaN (说明整列都是NaN)，则填 0 (代表无数据)\n",
    "X_train = X_train.fillna(train_medians).fillna(0)\n",
    "X_test = X_test.fillna(train_medians).fillna(0)\n",
    "\n",
    "# 4. 最终检查 (打印剩余 NaN 数量，应为 0)\n",
    "print(f\"预处理后 X_train 缺失值数量: {X_train.isna().sum().sum()}\")\n",
    "print(f\"预处理后 X_test 缺失值数量: {X_test.isna().sum().sum()}\")\n",
    "# ===================================\n",
    "\n",
    "\n",
    "# # 查看数据预处理后的结构\n",
    "# print(\"预处理后的训练数据：\")\n",
    "# print(X_train)\n",
    "# print(\"预处理后的测试数据：\")\n",
    "# print(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51198468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 代码参考：https://www.bilibili.com/video/BV1j2421Z7ti?spm_id_from=333.788.player.switch&vd_source=fe18fc7331f8146ed53da453b38c3226\n",
    "\n",
    "\n",
    "# # 分割数据集\n",
    "# # 分割数据集为训练集和测试集，测试集占20%\n",
    "# X_train_val,X_test,y_train_val,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# # 再次分割训练集以获取验证集，验证集占训练集的25%，即0.25 × 0.8 = 0.2\n",
    "# X_train,X_val,y_train,y_val = train_test_split(X_train_val,y_train_val,test_size=0.25,random_state=42)\n",
    "\n",
    "\n",
    "# 构建SVM模型\n",
    "svm = SVC(probability=True, random_state=42)\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "# 构建XGBoost模型\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic', \n",
    "    eval_metric='logloss', \n",
    "    use_label_encoder=False, \n",
    "    learning_rate=0.1, \n",
    "    max_depth=5, \n",
    "    n_estimators=100, \n",
    "    random_state=42)\n",
    "xgb_model.fit(X_train,y_train)\n",
    "\n",
    "# 构建GBM模型\n",
    "params ={\n",
    "    'objective':'binary',\n",
    "    'metric':'binary_logloss',\n",
    "    'boosting_type':'gbdt',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves':31,\n",
    "    'max_depth':-1,\n",
    "    'min_data_in_leaf':20,\n",
    "    'num_threads': 4,\n",
    "    'verbose':-1}\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_model = lgb.train(params,lgb_train, num_boost_round=109)\n",
    "\n",
    "\n",
    "# 构建逻辑回归模型\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train,y_train)\n",
    "\n",
    "# 构建随机森林模型\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "# 构建多层感知器（MLP）模型\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于您当前项目的实际代码结构（单元格 11 已完成预处理并生成了带列名的 X_train DataFrame，单元格 12 单独训练了 rf 模型），实现方式比单元格 13 的官方示例要简单得多。\n",
    "\n",
    "# 主要区别：\n",
    "# 官方示例 (Cell 13)：使用了 Pipeline 包含预处理和模型，所以需要深入 named_steps 去挖掘特征名称。\n",
    "# 您的项目：预处理和训练是分开的。X_train 已经是包含所有特征名称的 DataFrame，rf 是独立的分类器。\n",
    "\n",
    "# 随机森林特征重要性排序\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. 获取特征重要性和特征名称\n",
    "# 您的 rf 模型是在 Cell 12 中单独定义的，不是 Pipeline\n",
    "# 您的 X_train 在 Cell 11 中已经处理为带有列名的 DataFrame，可以直接使用\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# 2. 数据整合与排序\n",
    "# 创建一个 DataFrame 方便处理\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importances\n",
    "})\n",
    "\n",
    "# 按重要性降序排序\n",
    "feature_imp_df = feature_imp_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# 3. 绘图配置\n",
    "# 考虑到 OneHot 编码后特征可能很多，我们只展示 Top 20\n",
    "top_n = 20\n",
    "top_features = feature_imp_df.head(top_n)\n",
    "\n",
    "# 为了让 barh 图中最重要的特征显示在最上方，绘图数据需要反转顺序（升序）\n",
    "plot_data = top_features.sort_values(by='importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "y_ticks = np.arange(len(plot_data))\n",
    "\n",
    "# 绘制水平条形图\n",
    "ax.barh(y_ticks, plot_data['importance'], color='#1f77b4')\n",
    "ax.set_yticks(y_ticks)\n",
    "ax.set_yticklabels(plot_data['feature'])\n",
    "\n",
    "ax.set_title(f\"Random Forest Feature Importances (Top {top_n})\", fontsize=15)\n",
    "ax.set_xlabel(\"Importance Score (MDI)\", fontsize=12)\n",
    "\n",
    "# 添加网格线方便阅读\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图片\n",
    "output = \"/home/phl/PHL/Car-T/model_v1/output\"\n",
    "output_path = os.path.join(output, \"rf_feature_importance.pdf\")\n",
    "if not os.path.exists(output):\n",
    "    os.makedirs(output)\n",
    "    \n",
    "plt.savefig(output_path)\n",
    "print(f\"特征重要性图已保存至: {output_path}\")\n",
    "plt.show()\n",
    "print(\"\\nTop 10 Feature Importances:\")      # 打印 Top 10 特征的具体数值\n",
    "print(feature_imp_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1410667",
   "metadata": {},
   "source": [
    "# 多重共线性分析 (基于 Spearman 相关性与层次聚类)\n",
    "\n",
    "本部分基于参考笔记本的第15个单元格，使用 Spearman 相关性和层次聚类来分析当前项目数据的多重共线性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a660e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的统计和聚类库\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 计算 Spearman 相关性矩阵\n",
    "# 使用训练集 X_train 进行计算\n",
    "# spearmanr 返回值的属性是 statistic，或者它是一个元组(旧版本)\n",
    "res = spearmanr(X_train)\n",
    "if hasattr(res, 'statistic'):\n",
    "    corr = res.statistic\n",
    "else:\n",
    "    corr = res[0]\n",
    "\n",
    "# 处理可能出现的 NaN 值（常量列会导致相关性为 NaN，进而导致 hierarchy.ward 报错）\n",
    "# 将 NaN 替换为 0（表示无相关性）\n",
    "corr = np.nan_to_num(corr)\n",
    "\n",
    "# 确保相关性矩阵是对称的（处理可能的数值误差）\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# 执行层次聚类 (Ward Linkage)\n",
    "# 将相关性矩阵转换为距离矩阵进行聚类处理\n",
    "# hierarchy.ward 期望输入是压缩的距离矩阵或原始观测值，这里直接对相关性矩阵操作\n",
    "corr_linkage = hierarchy.ward(corr)\n",
    "\n",
    "# 绘制树状图和相关性热图\n",
    "fig_tree, ax1 = plt.subplots(figsize=(20, 12))           # 创建独立的画布用于树状图\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))   # 创建一个画布，包含两个子图\n",
    "\n",
    "# 绘制树状图\n",
    "dendro = hierarchy.dendrogram(\n",
    "    corr_linkage, \n",
    "    labels=X_train.columns, \n",
    "    ax=ax1,\n",
    "    leaf_rotation=90,\n",
    "    leaf_font_size=12   # 设置字号\n",
    ")\n",
    "\n",
    "# 调整树状图标签字体样式\n",
    "# 遍历 x 轴标签，设置为加粗和纯黑色\n",
    "for label in ax1.get_xticklabels():\n",
    "    label.set_fontweight('bold')\n",
    "    label.set_color('black')\n",
    "ax1.set_title(\"Feature Clustering Dendrogram\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存树状图\n",
    "tree_output_path = \"/home/phl/PHL/Car-T/model_v1/output/feature_clustering_dendrogram.pdf\"\n",
    "plt.savefig(tree_output_path)\n",
    "print(f\"树状图已保存至: {tree_output_path}\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 2. 单独绘制并保存重排后的相关性热图\n",
    "fig_heat, ax2 = plt.subplots(figsize=(20, 16))  # 创建独立的画布用于热图\n",
    "label_fontsize = 12  # 定义标签字号大小\n",
    "\n",
    "# 使用 dendro['leaves'] 来重新排序相关性矩阵，使其与树状图对齐\n",
    "# cmap='coolwarm': 蓝红配色 (蓝色负相关, 红色正相关)；camp='viridis': 绿色渐变\n",
    "# vmin=-1, vmax=1: 确保颜色映射范围覆盖 -1 到 1\n",
    "dendro_idx = np.arange(0, len(dendro['ivl']))   # 获取聚类后的索引顺序\n",
    "im = ax2.imshow(corr[dendro['leaves'], :][:, dendro['leaves']], cmap='coolwarm', vmin=-1, vmax=1)\n",
    "ax2.set_xticks(dendro_idx)\n",
    "ax2.set_yticks(dendro_idx)\n",
    "\n",
    "# 应用字号设置，并加粗字体、设置为纯黑色以提高清晰度\n",
    "ax2.set_xticklabels(dendro['ivl'], rotation='vertical', fontsize=label_fontsize, color='black', fontweight='bold')\n",
    "ax2.set_yticklabels(dendro['ivl'], fontsize=label_fontsize, color='black', fontweight='bold')\n",
    "ax2.set_title(\"Feature Correlation Heatmap (Sorted)\")\n",
    "\n",
    "fig.colorbar(im, ax=ax2)\n",
    "fig.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/feature_collinearity_spearman.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a143986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考：https://mp.weixin.qq.com/s/WyLyVmWmiWCwjfStkeVobA\n",
    "# 计算训练集各类模型评估指标\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def calculate_metrics(model, X, y, label, is_lgb=False):\n",
    "    \"\"\"仅计算指标，返回字典\"\"\"\n",
    "    if is_lgb:\n",
    "        y_pred_proba = model.predict(X, num_iteration=model.best_iteration)\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "        \n",
    "    y_pred_label = model.predict(X)     # 预测测试集的类别\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': label,                                         # 添加模型名称\n",
    "        'AUC': roc_auc_score(y, y_pred_proba),                  # 计算AUC\n",
    "        'Accuracy': accuracy_score(y, y_pred_label),            # 计算准确率\n",
    "        'Precision': precision_score(y, y_pred_label, zero_division = \"warn\"),          # 计算精确率\n",
    "        'Recall': recall_score(y, y_pred_label, zero_division = \"warn\"),                # 计算召回\n",
    "        'F1_score': f1_score(y, y_pred_label, zero_division = \"warn\"),                        # 计算F1分数\n",
    "        'Confusion_Matrix': confusion_matrix(y, y_pred_label),  # 计算混淆矩阵\n",
    "\n",
    "    }\n",
    "    \n",
    "    # print(f\"\\n{label} 指标:\")\n",
    "    # print(f\"  AUC: {metrics['auc']:.2f}\")\n",
    "    # print(f\"  Accuracy: {metrics['accuracy']:.2f}\")\n",
    "    # print(f\"  Precision: {metrics['precision']:.2f}\")\n",
    "    # print(f\"  Recall: {metrics['recall']:.2f}\")\n",
    "    # print(f\"  F1 Score: {metrics['f1']:.2f}\")\n",
    "    # print(f\"  Confusion Matrix:\\n{metrics['confusion_matrix']}\")\n",
    "\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, label, subdir=\"confusion_matrix_train\"):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Class 0', 'Class 1'], \n",
    "                yticklabels=['Class 0', 'Class 1'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {label}')\n",
    "    \n",
    "    # 确保目录存在\n",
    "    output_dir = os.path.join(\"/home/phl/PHL/Car-T/model_v1/output\", subdir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 处理文件名中的空格\n",
    "    safe_label = label.replace(' ', '_')\n",
    "    filepath = os.path.join(output_dir, f\"confusion_matrix_{safe_label}.pdf\")\n",
    "    \n",
    "    plt.savefig(filepath, bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    plt.close()  # 关闭图形,避免内存占用\n",
    "    \n",
    "# ========== 计算各模型指标并绘制混淆矩阵 ==========\n",
    "all_metrics_train = []\n",
    "\n",
    "# SVM\n",
    "metrics_train = calculate_metrics(svm, X_train, y_train, label='SVM')\n",
    "all_metrics_train.append(metrics_train)\n",
    "plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'SVM')\n",
    "\n",
    "# XGBoost\n",
    "metrics_train = calculate_metrics(xgb_model, X_train, y_train, label='XGBoost')\n",
    "all_metrics_train.append(metrics_train)\n",
    "plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'XGBoost')\n",
    "\n",
    "# LightGBM (如果需要)\n",
    "# metrics_train = calculate_metrics(lgb_model, X_train, y_train, label='LightGBM', is_lgb=True)\n",
    "# all_metrics_train.append(metrics_train)\n",
    "# plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'LightGBM')\n",
    "\n",
    "# Logistic Regression\n",
    "metrics_train = calculate_metrics(log_reg, X_train, y_train, label='Logistic Regression')\n",
    "all_metrics_train.append(metrics_train)\n",
    "plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'Logistic Regression')\n",
    "\n",
    "# Random Forest\n",
    "metrics_train = calculate_metrics(rf, X_train, y_train, label='Random Forest')\n",
    "all_metrics_train.append(metrics_train)\n",
    "plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'Random Forest')\n",
    "\n",
    "# MLP\n",
    "metrics_train = calculate_metrics(mlp, X_train, y_train, label='MLP')\n",
    "all_metrics_train.append(metrics_train)\n",
    "plot_confusion_matrix(metrics_train['Confusion_Matrix'], 'MLP')\n",
    "\n",
    "# ========== 创建汇总表格 ==========\n",
    "# 提取混淆矩阵之外的所有指标\n",
    "df_metrics_train = pd.DataFrame([\n",
    "    {k: v for k, v in m.items() if k != 'Confusion_Matrix'} \n",
    "    for m in all_metrics_train\n",
    "])\n",
    "\n",
    "# 设置Model为索引\n",
    "df_metrics_train = df_metrics_train.set_index('Model')\n",
    "\n",
    "# ========== 美化输出 ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model performance metrics (Train Set)\")\n",
    "print(\"=\"*80)\n",
    "print(df_metrics_train.to_string(float_format=lambda x: f'{x:.4f}'))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 保存到CSV ==========\n",
    "output_csv = \"/home/phl/PHL/Car-T/model_v1/output/model_metrics_summary_train.csv\"\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_metrics_train.to_csv(output_csv)\n",
    "print(f\"\\n 指标表格已保存到: {output_csv}\")\n",
    "\n",
    "# ========== 输出详细混淆矩阵 ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"详细混淆矩阵\")\n",
    "print(\"=\"*80)\n",
    "for m in all_metrics_train:\n",
    "    print(f\"\\n{m['Model']}:\")\n",
    "    cm = m['Confusion_Matrix']\n",
    "    print(f\"  TN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "    print(f\"  FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    print(f\"  Sensitivity (TPR): {cm[1,1]/(cm[1,0]+cm[1,1]):.4f}\")\n",
    "    print(f\"  Specificity (TNR): {cm[0,0]/(cm[0,0]+cm[0,1]):.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9276328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算测试集各类模型评估指标\n",
    "    \n",
    "# ========== 计算各模型指标并绘制混淆矩阵 ==========\n",
    "all_metrics_test = []\n",
    "\n",
    "# SVM\n",
    "metrics_test = calculate_metrics(svm, X_test, y_test, label='SVM')\n",
    "all_metrics_test.append(metrics_test)\n",
    "plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'SVM', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# XGBoost\n",
    "metrics_test = calculate_metrics(xgb_model, X_test, y_test, label='XGBoost')\n",
    "all_metrics_test.append(metrics_test)\n",
    "plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'XGBoost', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# LightGBM (如果需要)\n",
    "# metrics_test = calculate_metrics(lgb_model, X_test, y_test, label='LightGBM', is_lgb=True)\n",
    "# all_metrics_test.append(metrics_test)\n",
    "# plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'LightGBM', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# Logistic Regression\n",
    "metrics_test = calculate_metrics(log_reg, X_test, y_test, label='Logistic Regression')\n",
    "all_metrics_test.append(metrics_test)\n",
    "plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'Logistic Regression', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# Random Forest\n",
    "metrics_test = calculate_metrics(rf, X_test, y_test, label='Random Forest')\n",
    "all_metrics_test.append(metrics_test)\n",
    "plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'Random Forest', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# MLP\n",
    "metrics_test = calculate_metrics(mlp, X_test, y_test, label='MLP')\n",
    "all_metrics_test.append(metrics_test)\n",
    "plot_confusion_matrix(metrics_test['Confusion_Matrix'], 'MLP', subdir=\"confusion_matrix_test\")\n",
    "\n",
    "# ========== 创建汇总表格 ==========\n",
    "# 提取混淆矩阵之外的所有指标\n",
    "df_metrics = pd.DataFrame([\n",
    "    {k: v for k, v in m.items() if k != 'Confusion_Matrix'} \n",
    "    for m in all_metrics_test\n",
    "])\n",
    "\n",
    "# 设置Model为索引\n",
    "df_metrics = df_metrics.set_index('Model')\n",
    "\n",
    "# ========== 美化输出 ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model performance metrics (Test Set)\")\n",
    "print(\"=\"*80)\n",
    "print(df_metrics.to_string(float_format=lambda x: f'{x:.4f}'))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ========== 保存到CSV ==========\n",
    "output_csv = \"/home/phl/PHL/Car-T/model_v1/output/model_metrics_summary_test.csv\"\n",
    "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
    "df_metrics.to_csv(output_csv)\n",
    "print(f\"\\n 指标表格已保存到: {output_csv}\")\n",
    "\n",
    "# ========== 输出详细混淆矩阵 ==========\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"详细混淆矩阵\")\n",
    "print(\"=\"*80)\n",
    "for m in all_metrics_test:\n",
    "    print(f\"\\n{m['Model']}:\")\n",
    "    cm = m['Confusion_Matrix']\n",
    "    print(f\"  TN={cm[0,0]}, FP={cm[0,1]}\")\n",
    "    print(f\"  FN={cm[1,0]}, TP={cm[1,1]}\")\n",
    "    print(f\"  Sensitivity (TPR): {cm[1,1]/(cm[1,0]+cm[1,1]):.4f}\")\n",
    "    print(f\"  Specificity (TNR): {cm[0,0]/(cm[0,0]+cm[0,1]):.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc83360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练集的ROC曲线和AUC值\n",
    "def plot_roc_curve(model, X, y, label, is_lgb=False):\n",
    "    if is_lgb:\n",
    "        y_pred = model.predict(X, num_iteration=model.best_iteration)\n",
    "    else:\n",
    "        y_pred = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "# 绘制训练集的ROC曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 逻辑回归\n",
    "plot_roc_curve(log_reg, X_train, y_train, 'Logistic Regression')\n",
    "\n",
    "# 随机森林\n",
    "plot_roc_curve(rf, X_train, y_train, 'Random Forest')\n",
    "\n",
    "# 多层感知器 （MLP）\n",
    "plot_roc_curve(mlp, X_train, y_train, 'MLP')\n",
    "\n",
    "# SVM\n",
    "plot_roc_curve(svm, X_train, y_train, 'SVM')\n",
    "\n",
    "# XGBoost\n",
    "plot_roc_curve(xgb_model, X_train, y_train, 'XGBoost')\n",
    "\n",
    "# GBM\n",
    "plot_roc_curve(lgb_model, X_train, y_train, 'GBM', is_lgb=True)\n",
    "\n",
    "# 添加对角线（随机分类器的参考线）\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (Diagonal)')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (Train)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/roc_curves_train.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459bdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制测试集的ROC曲线\n",
    "plt.figure(figsize=(10, 8))\n",
    "    \n",
    "# 逻辑回归\n",
    "plot_roc_curve(log_reg, X_test, y_test, 'Logistic Regression')\n",
    "\n",
    "# 随机森林\n",
    "plot_roc_curve(rf, X_test, y_test, 'Random Forest')\n",
    "\n",
    "# 多层感知器 （MLP）\n",
    "plot_roc_curve(mlp, X_test, y_test, 'MLP')\n",
    "\n",
    "# SVM\n",
    "plot_roc_curve(svm, X_test, y_test, 'SVM')\n",
    "\n",
    "# XGBoost\n",
    "plot_roc_curve(xgb_model, X_test, y_test, 'XGBoost')\n",
    "\n",
    "# GBM\n",
    "plot_roc_curve(lgb_model, X_test, y_test, 'GBM', is_lgb=True)\n",
    "\n",
    "# 添加对角线（随机分类器的参考线）\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (Diagonal)')\n",
    "\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (Test)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/roc_curves_test.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 绘制测试集数据类森林图\n",
    "from scipy.stats import sem\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 计算 AUC 及其置信区间的函数\n",
    "def compute_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)         # 数据样本数\n",
    "    mean = np.mean(data)  # 计算均值\n",
    "    std_err = sem(data)   # 计算标准误差\n",
    "    h= std_err * 1.96      # 对于95%的置信区间\n",
    "    return mean,mean -h, mean + h\n",
    "\n",
    "# 定义模型字典\n",
    "models ={\n",
    "    'SVM': svm,\n",
    "    'XGBoost': xgb_model,\n",
    "    'GBM': lgb_model,\n",
    "    'Logistic Regression': log_reg,\n",
    "    'Random Forest': rf,\n",
    "    'MLP': mlp\n",
    "}\n",
    "\n",
    "# 计算每个模型的 AUC 及其置信区间\n",
    "results = []\n",
    "for name,model in models.items():\n",
    "    if name =='XGBoost':\n",
    "        y_pred = model.predict_proba(X_test)[:, 1] # 使用验证集数据\n",
    "    elif name =='GBM':\n",
    "        y_pred = model.predict(X_test) # 对GBM模型直接使用predict方法\n",
    "    else:\n",
    "        y_pred = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "    auc_score = roc_auc_score(y_test, y_pred) # 计算AUC得分\n",
    "    auc_std = sem(y_pred) # 计算AUC的标准误差\n",
    "    ci = 1.96 * auc_std # 95% 置信区间\n",
    "    results.append((name, auc_score, auc_score - ci, auc_score + ci)) # 添加结果到结果列表\n",
    "        \n",
    "        \n",
    "# 创建 DataFrame 并展示 AUC 数据\n",
    "df_results = pd.DataFrame(results, columns=['Model', 'AUC Score', 'CI Lower Bound', 'CI Upper Bound'])\n",
    "print(df_results)\n",
    "\n",
    "# 可选：保存为CSV 文件\n",
    "df_results.to_csv('/home/phl/PHL/Car-T/model_v1/output//auc_results_test.csv', index=False)\n",
    "\n",
    "\n",
    "# ========== 绘制森林图（纵向布局）==========\n",
    "# 提取数据用于绘图\n",
    "names = df_results['Model'].tolist()\n",
    "means = df_results['AUC Score'].tolist()\n",
    "lower_bounds = df_results['CI Lower Bound'].tolist()\n",
    "upper_bounds = df_results['CI Upper Bound'].tolist()\n",
    "\n",
    "# 定义颜色（为每个模型分配不同颜色）\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# 绘制纵向误差条（森林图 - 纵向布局）\n",
    "for i, (mean, lower, upper, color) in enumerate(zip(means, lower_bounds, upper_bounds, colors)):\n",
    "    # 计算误差条的长度\n",
    "    yerr_lower = mean - lower\n",
    "    yerr_upper = upper - mean\n",
    "    \n",
    "    # 绘制误差条（注意：这里交换了x和y的位置）\n",
    "    ax.errorbar(\n",
    "        i,              # x位置：模型索引\n",
    "        mean,           # y位置：AUC均值\n",
    "        yerr=[[yerr_lower], [yerr_upper]],  # 误差范围（现在是纵向的）\n",
    "        fmt='o',        # 点的样式\n",
    "        color=color,    # 颜色\n",
    "        ecolor=color,   # 误差条颜色\n",
    "        elinewidth=3,   # 误差条线宽\n",
    "        capsize=5,      # 误差条端点长度\n",
    "        markersize=8    # 点的大小\n",
    "    )\n",
    "\n",
    "# 添加参考线（AUC = 0.5，随机分类器）- 现在是水平线\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='Random Classifier (AUC=0.5)')\n",
    "\n",
    "# 设置图形参数（X轴和Y轴互换）\n",
    "ax.set_xticks(range(len(names)))  # 设置x轴刻度\n",
    "ax.set_xticklabels(names, rotation=45, ha='right')  # 设置x轴标签，旋转45度便于阅读\n",
    "ax.set_ylim(0, 1.0)  # 设置Y轴范围\n",
    "ax.set_xlabel('Model', fontdict={'family': 'Times New Roman', 'fontsize': 15})  # X轴标签\n",
    "ax.set_ylabel('AUC Score', fontdict={'family': 'Times New Roman', 'fontsize': 15})  # Y轴标签\n",
    "ax.set_title('AUC Scores with 95% Confidence Intervals for Different Models (Test Set)', \n",
    "             fontdict={'family': 'Times New Roman', 'fontsize': 16})  # 设置图表标题\n",
    "\n",
    "# 添加网格（现在是水平网格线）\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# 美化边框\n",
    "ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图表为PDF文件\n",
    "plt.savefig('/home/phl/PHL/Car-T/model_v1/output/forestplot_test.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()  # 显示图表\n",
    "\n",
    "# 打印统计摘要\n",
    "print(\"\\n========== 模型性能统计摘要 ==========\")\n",
    "for i, row in df_results.iterrows():\n",
    "    print(f\"{row['Model']:20s} | AUC: {row['AUC Score']:.4f} | 95% CI: [{row['CI Lower Bound']:.4f}, {row['CI Upper Bound']:.4f}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d964caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 绘制测试集校准曲线\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 计算每个模型的预测概率的函数\n",
    "def get_predictions(model, X, model_name):\n",
    "    if model_name == 'GBM':\n",
    "        return model.predict(X)  # 对GBM模型使用predict方法\n",
    "    else:\n",
    "        return model.predict_proba(X)[:, 1]  # 其他模型使用predict_proba方法并选择概率列\n",
    "\n",
    "# 创建图形\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 定义颜色\n",
    "colors = {'SVM': '#1f77b4', 'XGBoost': '#ff7f0e', 'GBM': '#2ca02c', \n",
    "          'Logistic Regression': '#d62728', 'Random Forest': '#9467bd', 'MLP': '#8c564b'}\n",
    "\n",
    "# 遍历每个模型并绘制校准曲线\n",
    "for name, model in models.items():\n",
    "    y_pred = get_predictions(model, X_test, name) # 获取模型的预测概率\n",
    "    y_pred = np.clip(y_pred, 0, 1) # 将预测值限制在［0，1］范围内\n",
    "\n",
    "#     # 进行Isotonic Regression校准\n",
    "#     iso_reg = IsotonicRegression(out_of_bounds='clip')\n",
    "#     y_pred_calibrated = iso_reg.fit_transform(y_pred, y_test) # 使用Isotonic Regression 进行校准\n",
    "\n",
    "    # 原始模型的校准曲线\n",
    "    # 计算校准曲线\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10, strategy='uniform')\n",
    "    \n",
    "    # 绘制校准曲线\n",
    "    ax.plot(prob_pred, prob_true, marker='o', linewidth=2, \n",
    "            label=name, color=colors.get(name, 'gray'))\n",
    "\n",
    "# 绘制完美校准线（对角线）\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfectly Calibrated')\n",
    "\n",
    "# 设置图形参数\n",
    "ax.set_xlabel('Mean Predicted Probability', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "ax.set_ylabel('Fraction of Positives', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "ax.set_title('Calibration Curves (Test Set)', fontdict={'family': 'Times New Roman', 'fontsize': 16})\n",
    "\n",
    "# 设置坐标轴范围\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# 添加网格\n",
    "ax.grid(alpha=0.3, linestyle='--')\n",
    "\n",
    "# 美化边框\n",
    "ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "\n",
    "# 添加图例\n",
    "ax.legend(loc='upper left', fontsize=10)\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图形\n",
    "plt.savefig('/home/phl/PHL/Car-T/model_v1/output/calibration_curves_test.pdf', \n",
    "            bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# 可选：打印每个模型的校准统计信息\n",
    "print(\"\\n========== 校准曲线统计信息 ==========\")\n",
    "for name, model in models.items():\n",
    "    y_pred = get_predictions(model, X_test, name)\n",
    "    y_pred = np.clip(y_pred, 0, 1)\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_pred, n_bins=10)\n",
    "    \n",
    "    # 计算校准误差（Calibration Error）\n",
    "    calibration_error = np.mean(np.abs(prob_true - prob_pred))\n",
    "    print(f\"{name:20s} | 平均校准误差: {calibration_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe39aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单元格16的修复版本（建议直接删除，使用单元格15）\n",
    "\n",
    "# =========================================================================================\n",
    "# ====================================== 1. 库的导入 =========================================\n",
    "# =========================================================================================\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "# =========================================================================================\n",
    "# ====================================== 2. 绘图函数（完整版）=====================================\n",
    "# =========================================================================================\n",
    "def plot_calibration_curves(y_true, all_scores, plot_title, n_bins=10, filename=None):\n",
    "    \"\"\"\n",
    "    绘制校准曲线\n",
    "    \n",
    "    参数:\n",
    "        y_true: 真实标签\n",
    "        all_scores: 字典，格式为 {模型名: 预测概率数组}\n",
    "        plot_title: 图表标题\n",
    "        n_bins: 分箱数量\n",
    "        filename: 保存文件名（可选）\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.set_facecolor('white')  # 背景颜色\n",
    "    \n",
    "    # 定义颜色\n",
    "    colors = {'SVM': '#1f77b4', 'XGBoost': '#ff7f0e', 'GBM': '#2ca02c', \n",
    "              'Logistic Regression': '#d62728', 'Random Forest': '#9467bd', 'MLP': '#8c564b'}\n",
    "    \n",
    "    # 遍历所有模型并绘制校准曲线\n",
    "    for model_name, y_scores in all_scores.items():\n",
    "        y_scores_clipped = np.clip(y_scores, 0, 1)\n",
    "        prob_true, prob_pred = calibration_curve(y_true, y_scores_clipped, \n",
    "                                                  n_bins=n_bins, strategy='uniform')\n",
    "        \n",
    "        ax.plot(prob_pred, prob_true, marker='o', linewidth=2, \n",
    "                label=model_name, color=colors.get(model_name, 'gray'))\n",
    "    \n",
    "    # 绘制对角虚线/参考线\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=2, linestyle='--', label='Perfectly Calibrated')\n",
    "    \n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_ylim([0, 1])\n",
    "    \n",
    "    ax.set_xlabel('Mean Predicted Probability', fontsize=15, \n",
    "                  fontdict={'family': 'Times New Roman'})\n",
    "    ax.set_ylabel('Fraction of Positives', fontsize=15, \n",
    "                  fontdict={'family': 'Times New Roman'})\n",
    "    ax.set_title(plot_title, fontsize=16, fontdict={'family': 'Times New Roman'})\n",
    "    \n",
    "    # 添加网格\n",
    "    ax.grid(True, linestyle='--', linewidth=1, alpha=0.3)\n",
    "    \n",
    "    # 添加图例\n",
    "    ax.legend(loc='upper left', fontsize=10, frameon=True, edgecolor='black')\n",
    "    \n",
    "    # 美化边框\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图形\n",
    "    if filename:\n",
    "        plt.savefig(f'/home/phl/PHL/Car-T/model_v1/output/{filename}.pdf', \n",
    "                    bbox_inches='tight', dpi=300)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================================================================================\n",
    "# ====================================== 3. 使用您的实际数据绘制校准曲线 ======================\n",
    "# =========================================================================================\n",
    "\n",
    "# 定义获取预测概率的函数\n",
    "def get_predictions(model, X, model_name):\n",
    "    if model_name == 'GBM':\n",
    "        return model.predict(X)\n",
    "    else:\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "\n",
    "# 收集所有模型在测试集上的预测分数\n",
    "test_scores_dict = {}\n",
    "for name, model in models.items():\n",
    "    test_scores_dict[name] = get_predictions(model, X_test, name)\n",
    "\n",
    "# 绘制测试集的校准曲线\n",
    "plot_calibration_curves(\n",
    "    y_test,\n",
    "    test_scores_dict,\n",
    "    \"Calibration Curves on Test Set (Model Comparison)\",\n",
    "    n_bins=10,\n",
    "    filename='calibration_curves_test_v2'\n",
    ")\n",
    "\n",
    "# 可选：同样绘制训练集的校准曲线\n",
    "train_scores_dict = {}\n",
    "for name, model in models.items():\n",
    "    train_scores_dict[name] = get_predictions(model, X_train, name)\n",
    "\n",
    "plot_calibration_curves(\n",
    "    y_train,\n",
    "    train_scores_dict,\n",
    "    \"Calibration Curves on Training Set (Model Comparison)\",\n",
    "    n_bins=10,\n",
    "    filename='calibration_curves_train_v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f649c",
   "metadata": {},
   "source": [
    "### lgb的模型名称为lgb_model，且lgb只能用predict函数，而不是predict_proba函数\n",
    "gbm_preds_test = lgb_model.predict(X_test)   # GBM模型不需要[:, 1]\n",
    "\n",
    "- 如果 is_lgb=True，它调用 model.predict()（因为 LightGBM 原生 Booster 对象直接输出概率）。\n",
    "- 如果 is_lgb=False，它调用 model.predict_proba()[:, 1]（这是 Scikit-learn 标准模型的用法）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2107be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 绘制训练集的DCA曲线\n",
    "from sklearn.metrics import roc_curve,auc,confusion_matrix\n",
    "\n",
    "# 计算模型的净收益的函数\n",
    "def calculate_net_benefit_model(thresh_group,y_pred_score,y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score >thresh #根据阈值分类\n",
    "        tn,fp,fn,tp = confusion_matrix(y_label, y_pred_label).ravel() # 计算混淆矩障的四个值\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n)- (fp / n)*(thresh / (1 - thresh)) # 计算净收益\n",
    "        net_benefit_model = np.append(net_benefit_model,net_benefit)# 将净收益添加到数组\n",
    "    return net_benefit_model\n",
    "    \n",
    "# 计算所有样本净收益的函数\n",
    "def calculate_net_benefit_all(thresh_group,y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp,fn,tp = confusion_matrix(y_label,y_label).ravel() # 计算混淆矩阵的四个值（所有样本）\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        net_benefit = (tp / total)- (tn / total)*(thresh /(1 -thresh)) # 计算净收益\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)# 将净收益添加到数组\n",
    "    return net_benefit_all\n",
    "\n",
    "# 绘制决策曲线分析（DCA）图的函数\n",
    "def plot_DCA(ax,thresh_group,net_benefit_model,label):\n",
    "    # 绘制净收益曲线\n",
    "    ax.plot(thresh_group,net_benefit_model, label=f'{label}')\n",
    "\n",
    "    # 设置图形参数\n",
    "    ax.set_xlim(0, 1)  # 设置X轴范围\n",
    "    ax.set_ylim(net_benefit_model.min() - 0.15, net_benefit_model.max() + 0.15)  # 设置Y轴范围\n",
    "    ax.set_xlabel('Threshold Probability', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.set_ylabel('Net Benefit', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.grid('major')  # 在图形设置中启用网格，只需删除 ax.grid('major') 行\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.legend(loc='upper right')  # 显示图例\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# 绘制测试集的DCA曲线\n",
    "fig,ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# 定义阈值组\n",
    "thresh_group = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# 计算net_benefit_all\n",
    "net_benefit_all = calculate_net_benefit_all(thresh_group, y_test)\n",
    "\n",
    "# 绘制 “Treat all\" 和 “Treat none”\n",
    "ax.plot(thresh_group,net_benefit_all, color=\"black\",label='Treat all')\n",
    "ax.plot ([0,1],[0,0],color='black',linestyle=\":\",label='Treat none')\n",
    "\n",
    "# 填充 Treat all 和 Treat none 之间的区域\n",
    "y2 = np.maximum(net_benefit_all, 0)\n",
    "y1 = np.maximum(0, y2)\n",
    "ax.fill_between(thresh_group, y1, y2, alpha=0.2)\n",
    "\n",
    "# SVM模型\n",
    "svm_preds_train = svm.predict_proba(X_train)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group, svm_preds_train, y_train)\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, 'SVM')\n",
    "\n",
    "# XGBoost模型\n",
    "xgb_preds_train = xgb_model.predict_proba(X_train)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,xgb_preds_train,y_train)\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, 'XGBoost')\n",
    "\n",
    "# GBM模型\n",
    "gbm_preds_test = lgb_model.predict(X_test)   # GBM模型不需要[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,gbm_preds_test,y_test)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'GBM')  \n",
    "\n",
    "\n",
    "# 逻辑回归模型\n",
    "log_reg_preds_train = log_reg.predict_proba(X_train)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,log_reg_preds_train, y_train)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'Logistic Regression')\n",
    "\n",
    "# 随机森林模型\n",
    "rf_preds_train = rf.predict_proba(X_train)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,rf_preds_train,y_train)\n",
    "ax = plot_DCA(ax, thresh_group,net_benefit_model,'Random Forest')\n",
    "\n",
    "# MLP模型\n",
    "mlp_preds_train = mlp.predict_proba(X_train)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,mlp_preds_train,y_train)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'MLP')\n",
    "\n",
    "# 最后保存图像并显示\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/dca_curves_train.pdf\")# 保存DCA图像为PDF文件\n",
    "plt.show() # 显示DCA图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ea7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 绘制测试集的DCA曲线\n",
    "from sklearn.metrics import roc_curve,auc,confusion_matrix\n",
    "\n",
    "# 计算模型的净收益的函数\n",
    "def calculate_net_benefit_model(thresh_group,y_pred_score,y_label):\n",
    "    net_benefit_model = np.array([])\n",
    "    for thresh in thresh_group:\n",
    "        y_pred_label = y_pred_score >thresh #根据阈值分类\n",
    "        tn,fp,fn,tp = confusion_matrix(y_label, y_pred_label).ravel() # 计算混淆矩障的四个值\n",
    "        n = len(y_label)\n",
    "        net_benefit = (tp / n)- (fp / n)*(thresh / (1 - thresh)) # 计算净收益\n",
    "        net_benefit_model = np.append(net_benefit_model,net_benefit)# 将净收益添加到数组\n",
    "    return net_benefit_model\n",
    "    \n",
    "# 计算所有样本净收益的函数\n",
    "def calculate_net_benefit_all(thresh_group,y_label):\n",
    "    net_benefit_all = np.array([])\n",
    "    tn, fp,fn,tp = confusion_matrix(y_label,y_label).ravel() # 计算混淆矩阵的四个值（所有样本）\n",
    "    total = tp + tn\n",
    "    for thresh in thresh_group:\n",
    "        net_benefit = (tp / total)- (tn / total)*(thresh /(1 -thresh)) # 计算净收益\n",
    "        net_benefit_all = np.append(net_benefit_all, net_benefit)# 将净收益添加到数组\n",
    "    return net_benefit_all\n",
    "\n",
    "# 绘制决策曲线分析（DCA）图的函数\n",
    "def plot_DCA(ax,thresh_group,net_benefit_model,label):\n",
    "    # 绘制净收益曲线\n",
    "    ax.plot(thresh_group,net_benefit_model, label=f'{label}')\n",
    "\n",
    "    # 设置图形参数\n",
    "    ax.set_xlim(0, 1)  # 设置X轴范围\n",
    "    ax.set_ylim(net_benefit_model.min() - 0.15, net_benefit_model.max() + 0.15)  # 设置Y轴范围\n",
    "    ax.set_xlabel('Threshold Probability', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.set_ylabel('Net Benefit', fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.grid('major')  # 在图形设置中启用网格，只需删除 ax.grid('major') 行\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8))\n",
    "    ax.legend(loc='upper right')  # 显示图例\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# 绘制测试集的DCA曲线\n",
    "fig,ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# 定义阈值组\n",
    "thresh_group = np.linspace(0.01, 0.99, 100)\n",
    "\n",
    "# 计算net_benefit_all\n",
    "net_benefit_all = calculate_net_benefit_all(thresh_group, y_test)\n",
    "\n",
    "# 绘制 “Treat all\" 和 “Treat none”\n",
    "ax.plot(thresh_group,net_benefit_all, color=\"black\",label='Treat all')\n",
    "ax.plot ([0,1],[0,0],color='black',linestyle=\":\",label='Treat none')\n",
    "\n",
    "# 填充 Treat all 和 Treat none 之间的区域\n",
    "y2 = np.maximum(net_benefit_all, 0)\n",
    "y1 = np.maximum(0, y2)\n",
    "ax.fill_between(thresh_group, y1, y2, alpha=0.2)\n",
    "\n",
    "# SVM模型\n",
    "svm_preds_test = svm.predict_proba(X_test)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group, svm_preds_test, y_test)\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, 'SVM')\n",
    "\n",
    "# XGBoost模型\n",
    "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,xgb_preds_test,y_test)\n",
    "ax = plot_DCA(ax, thresh_group, net_benefit_model, 'XGBoost')\n",
    "\n",
    "# GBM模型\n",
    "gbm_preds_test = lgb_model.predict(X_test)   # GBM模型不需要[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,gbm_preds_test,y_test)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'GBM')  \n",
    "\n",
    "\n",
    "# 逻辑回归模型\n",
    "log_reg_preds_test = log_reg.predict_proba(X_test)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,log_reg_preds_test, y_test)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'Logistic Regression')\n",
    "\n",
    "# 随机森林模型\n",
    "rf_preds_test = rf.predict_proba(X_test)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,rf_preds_test,y_test)\n",
    "ax = plot_DCA(ax, thresh_group,net_benefit_model,'Random Forest')\n",
    "\n",
    "# MLP模型\n",
    "mlp_preds_test = mlp.predict_proba(X_test)[:, 1]\n",
    "net_benefit_model = calculate_net_benefit_model(thresh_group,mlp_preds_test,y_test)\n",
    "ax = plot_DCA(ax,thresh_group,net_benefit_model,'MLP')\n",
    "\n",
    "# 最后保存图像并显示\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/dca_curves_test.pdf\")# 保存DCA图像为PDF文件\n",
    "plt.show() # 显示DCA图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 训练集的PR曲线\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def plot_pr_curve(ax, y_true, y_scores,label):\n",
    "    \"\"\"绘制PR曲线\"\"\"\n",
    "    # 计算Precision-Recall曲线\n",
    "    precision,recall,thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    # 计算AUC值\n",
    "    auc_score = auc(recall, precision)\n",
    "\n",
    "    # 绘制PR曲线\n",
    "    ax.plot(recall,precision, label=f'{label} (AUC = {auc_score:.2f})')\n",
    "\n",
    "    # 图表配置\n",
    "    ax.set_xlabel(\"Recall\", fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.set_ylabel(\"Precision\", fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8)) #设置右边框的颜色\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8)) #设置上边框的颜色\n",
    "    ax.legend(loc='lower left') # 设置图例位置\n",
    "\n",
    "    return ax\n",
    "\n",
    "# 绘制训练集的PR曲线\n",
    "fig_train, ax_train = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# SVM模型\n",
    "svm_preds_train = svm.predict_proba(X_train)[:, 1]\n",
    "ax_train = plot_pr_curve(ax_train,y_train, svm_preds_train, 'SVM')\n",
    "\n",
    "# XGBoost模型\n",
    "xgb_preds_train = xgb_model.predict_proba(X_train)[:, 1]\n",
    "ax_train = plot_pr_curve(ax_train,y_train, xgb_preds_train, 'XGBoost')\n",
    "\n",
    "# GBM模型\n",
    "gbm_preds_train = lgb_model.predict(X_train)\n",
    "ax_train = plot_pr_curve(ax_train, y_train, gbm_preds_train, 'GBM')\n",
    "\n",
    "# 逻辑回归模型\n",
    "log_reg_preds_train = log_reg.predict_proba(X_train)[:, 1]\n",
    "ax_train = plot_pr_curve(ax_train, y_train, log_reg_preds_train, 'Logistic Regression')\n",
    "\n",
    "# 随机森林模型\n",
    "rf_preds_train = rf.predict_proba(X_train)[:, 1]\n",
    "ax_train = plot_pr_curve(ax_train, y_train, rf_preds_train, 'Random Forest')\n",
    "\n",
    "# MLP模型\n",
    "mlp_preds_train = mlp.predict_proba(X_train)[:, 1]\n",
    "ax_train = plot_pr_curve(ax_train,y_train,mlp_preds_train,'MLP')\n",
    "\n",
    "# 设置图标题并保存训练集PR曲线图像\n",
    "plt.title('Precision-Recall Curves(Train)')\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/pr_curves_train.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f033760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 测试集的PR曲线\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def plot_pr_curve(ax, y_true, y_scores,label):\n",
    "    \"\"\"绘制PR曲线\"\"\"\n",
    "    # 计算Precision-Recall曲线\n",
    "    precision,recall,thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    # 计算AUC值\n",
    "    auc_score = auc(recall, precision)\n",
    "\n",
    "    # 绘制PR曲线\n",
    "    ax.plot(recall,precision, label=f'{label} (AUC = {auc_score:.2f})')\n",
    "\n",
    "    # 图表配置\n",
    "    ax.set_xlabel(\"Recall\", fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.set_ylabel(\"Precision\", fontdict={'family': 'Times New Roman', 'fontsize': 15})\n",
    "    ax.spines['right'].set_color((0.8, 0.8, 0.8)) #设置右边框的颜色\n",
    "    ax.spines['top'].set_color((0.8, 0.8, 0.8)) #设置上边框的颜色\n",
    "    ax.legend(loc='lower left') # 设置图例位置\n",
    "\n",
    "    return ax\n",
    "\n",
    "#### 绘制测试集的PR曲线\n",
    "fig_test, ax_test = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# SVM模型\n",
    "svm_preds_test = svm.predict_proba(X_test)[:, 1]\n",
    "ax_test = plot_pr_curve(ax_test,y_test, svm_preds_test, 'SVM')\n",
    "\n",
    "# XGBoost模型\n",
    "xgb_preds_test = xgb_model.predict_proba(X_test)[:, 1]\n",
    "ax_test = plot_pr_curve(ax_test,y_test, xgb_preds_test, 'XGBoost')\n",
    "\n",
    "# GBM模型\n",
    "gbm_preds_test = lgb_model.predict(X_test)\n",
    "ax_test = plot_pr_curve(ax_test, y_test, gbm_preds_test, 'GBM')\n",
    "\n",
    "# 逻辑回归模型\n",
    "log_reg_preds_test = log_reg.predict_proba(X_test)[:, 1]\n",
    "ax_test = plot_pr_curve(ax_test, y_test, log_reg_preds_test, 'Logistic Regression')\n",
    "\n",
    "# 随机森林模型\n",
    "rf_preds_test = rf.predict_proba(X_test)[:, 1]\n",
    "ax_test = plot_pr_curve(ax_test, y_test, rf_preds_test, 'Random Forest')\n",
    "\n",
    "# MLP模型\n",
    "mlp_preds_test = mlp.predict_proba(X_test)[:, 1]\n",
    "ax_test = plot_pr_curve(ax_test,y_test,mlp_preds_test,'MLP')\n",
    "\n",
    "# 设置图标题并保存测试集PR曲线图像\n",
    "plt.title('Precision-Recall Curves(Test)')\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/pr_curves_test.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdad0d1",
   "metadata": {},
   "source": [
    "## 最优模型MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b53fd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_roc_curve(mlp, X_train, y_train, 'MLP')\n",
    "plt.plot([0, 1], [0, 1], color='r', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (Train)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(\"/home/phl/PHL/Car-T/model_v1/output/bestmodel_roc_curves_train.pdf\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
