"""
éªŒè¯é›†æ€§èƒ½è¯„ä¼°è„šæœ¬
Evaluate trained B-NHL CRS model on independent test set
-------------------------------------------------------
- Loads LightGBM pipeline (.pkl)
- Loads test_static.csv + test_dynamic/
- Aggregates dynamic features (Day -15~+2)
- Applies model pipeline
- Outputs performance metrics + curves
-------------------------------------------------------
âœ… åŠŸèƒ½æ¦‚è¿°ï¼š
	â€¢	è‡ªåŠ¨åŠ è½½æ¨¡åž‹æ–‡ä»¶ï¼šBNHL_CRS_model_output/BNHL_CRS_LGBM_pipeline.pkl
	â€¢	è‡ªåŠ¨åŠ è½½æµ‹è¯•é›†ï¼šBNHL_CRS_split_70_30/test_static.csv + test_dynamic/
	â€¢	ä½¿ç”¨ä¸Žè®­ç»ƒä¸€è‡´çš„èšåˆé€»è¾‘ï¼ˆDay -15 ~ +2ï¼‰æå–åŠ¨æ€ç‰¹å¾
	â€¢	è¿›è¡ŒæŽ¨ç†å¹¶è¾“å‡ºï¼š
	â€¢	ROC-AUCã€PR-AUCã€F1ã€Precisionã€Recallã€Brier
	â€¢	æ ¡å‡†æ›²çº¿ã€ROC æ›²çº¿ã€PR æ›²çº¿ï¼ˆä¿å­˜ PNGï¼‰
	â€¢	predictions_test.csvï¼ˆå«é¢„æµ‹æ¦‚çŽ‡ä¸Žæ ‡ç­¾ï¼‰
-------------------------------------------------------
âœ… ç”Ÿæˆç»“æžœç›®å½•
BNHL_CRS_evaluation/
â”œâ”€â”€ predictions_test.csv
â”œâ”€â”€ test_metrics.csv
â”œâ”€â”€ ROC_curve.png
â””â”€â”€ PR_curve.png
-------------------------------------------------------
ðŸ“Š è¾“å‡ºç¤ºä¾‹
ðŸ“Š Test set metrics:
  AUC: 0.8124
  AUPRC: 0.4317
  Precision: 0.6000
  Recall: 0.4211
  F1: 0.4941
  Brier: 0.1328
-------------------------------------------------------
ðŸ§  éªŒè¯åŽæŽ¨è

âœ… è‹¥æŒ‡æ ‡æŽ¥è¿‘äº¤å‰éªŒè¯å¹³å‡å€¼ï¼ˆÂ±0.05 ä»¥å†…ï¼‰â†’ æ¨¡åž‹æ³›åŒ–è‰¯å¥½
âš ï¸ è‹¥å·®è· >0.1 â†’ éœ€æ£€æŸ¥æ—¶é—´æ³„æ¼ã€æ ·æœ¬é‡ã€åˆ†å¸ƒæ¼‚ç§»ç­‰ã€‚
"""

import os
import joblib
import numpy as np
import pandas as pd
from sklearn.metrics import (
    roc_auc_score, average_precision_score, precision_score, recall_score,
    f1_score, brier_score_loss, roc_curve, precision_recall_curve
)
import matplotlib.pyplot as plt
from scipy import stats
from scipy.integrate import trapezoid

# ======================================================
# 1. PATHS
# ======================================================
SPLIT_DIR = "./BNHL_CRS_split_70_30"
MODEL_PATH = "./BNHL_CRS_model_output/BNHL_CRS_LGBM_pipeline.pkl"
STATIC_TEST = os.path.join(SPLIT_DIR, "test_static.csv")
DYNAMIC_TEST_DIR = os.path.join(SPLIT_DIR, "test_dynamic")
OUTPUT_DIR = "./BNHL_CRS_evaluation"

PATIENT_ID_COL = "patient_id"
LABEL_COL = "label"
OBS_START, OBS_END = -15, 2

os.makedirs(OUTPUT_DIR, exist_ok=True)

# ======================================================
# 2. LOAD MODEL
# ======================================================
print("ðŸ”¹ Loading trained pipeline...")
pipeline_data = joblib.load(MODEL_PATH)
preprocessor = pipeline_data["preprocessor"]
model = pipeline_data["model"]

# ======================================================
# 3. LOAD TEST STATIC + DYNAMIC
# ======================================================
df_static = pd.read_csv(STATIC_TEST)
print(f"Loaded {len(df_static)} patients in test set")

# --------------------
# Dynamic aggregation
# --------------------
def aggregate_time_series(df_ts: pd.DataFrame, obs_start=-15, obs_end=2):
    if 'Day' not in df_ts.columns:
        df_ts = df_ts.rename(columns={df_ts.columns[0]: 'Day'})
    df = df_ts[(df_ts['Day'] >= obs_start) & (df_ts['Day'] <= obs_end)]
    if len(df) == 0:
        return {}
    features = {}
    ts_cols = [c for c in df.columns if c != 'Day']
    for col in ts_cols:
        sub = df[['Day', col]].dropna()
        vals = sub[col].values
        ds = sub['Day'].values
        prefix = f"{col}"
        if len(vals) == 0:
            features[f"{prefix}_mean"] = np.nan
            continue
        features[f"{prefix}_mean"] = np.nanmean(vals)
        features[f"{prefix}_std"] = np.nanstd(vals)
        features[f"{prefix}_min"] = np.nanmin(vals)
        features[f"{prefix}_max"] = np.nanmax(vals)
        if len(vals) >= 2:
            slope, *_ = stats.linregress(ds, vals)
            features[f"{prefix}_slope"] = slope
            features[f"{prefix}_auc"] = trapezoid(vals, ds)
    return features

records = []
missing = []
for _, row in df_static.iterrows():
    pid = int(row[PATIENT_ID_COL])
    rec = {PATIENT_ID_COL: pid, LABEL_COL: row[LABEL_COL]}
    for c in df_static.columns:
        if c not in [PATIENT_ID_COL, LABEL_COL]:
            rec[f"s_{c}"] = row[c]
    dyn_path = os.path.join(DYNAMIC_TEST_DIR, f"{pid}.csv")
    if os.path.exists(dyn_path):
        try:
            df_dyn = pd.read_csv(dyn_path)
            dyn_feats = aggregate_time_series(df_dyn, OBS_START, OBS_END)
            rec.update(dyn_feats)
        except Exception as e:
            print(f"âš ï¸ Failed to process {pid}: {e}")
            missing.append(pid)
    else:
        missing.append(pid)
    records.append(rec)

df_test = pd.DataFrame(records)
print(f"âœ… Feature table ready: {len(df_test)} rows, {len(df_test.columns)} cols")

# ======================================================
# 4. APPLY PIPELINE
# ======================================================
X_test = df_test.drop(columns=[LABEL_COL, PATIENT_ID_COL])
y_test = df_test[LABEL_COL].values
X_test_t = preprocessor.transform(X_test)

probs = model.predict_proba(X_test_t)[:, 1]
preds = (probs >= 0.5).astype(int)

# ======================================================
# 5. METRICS
# ======================================================
metrics = {
    "AUC": roc_auc_score(y_test, probs),
    "AUPRC": average_precision_score(y_test, probs),
    "Precision": precision_score(y_test, preds, zero_division=0),
    "Recall": recall_score(y_test, preds, zero_division=0),
    "F1": f1_score(y_test, preds, zero_division=0),
    "Brier": brier_score_loss(y_test, probs)
}
print("\nðŸ“Š Test set metrics:")
for k, v in metrics.items():
    print(f"  {k}: {v:.4f}")

# ======================================================
# 6. SAVE PREDICTIONS
# ======================================================
df_out = df_test[[PATIENT_ID_COL, LABEL_COL]].copy()
df_out["probability"] = probs
df_out["prediction"] = preds
df_out.to_csv(os.path.join(OUTPUT_DIR, "predictions_test.csv"), index=False)
print("âœ… Predictions saved: predictions_test.csv")

# ======================================================
# 7. PLOTS
# ======================================================
fpr, tpr, _ = roc_curve(y_test, probs)
prec, rec, _ = precision_recall_curve(y_test, probs)

plt.figure()
plt.plot(fpr, tpr)
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title(f"ROC Curve (AUC={metrics['AUC']:.3f})")
plt.savefig(os.path.join(OUTPUT_DIR, "ROC_curve.png"), dpi=200)

plt.figure()
plt.plot(rec, prec)
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title(f"PR Curve (AUPRC={metrics['AUPRC']:.3f})")
plt.savefig(os.path.join(OUTPUT_DIR, "PR_curve.png"), dpi=200)

print("ðŸ“ˆ ROC & PR curves saved.")

# ======================================================
# 8. SAVE METRICS
# ======================================================
pd.DataFrame([metrics]).to_csv(os.path.join(OUTPUT_DIR, "test_metrics.csv"), index=False)
print("ðŸ“„ Metrics file saved: test_metrics.csv")
print(f"\nðŸŽ‰ Evaluation complete. Results stored in: {OUTPUT_DIR}")